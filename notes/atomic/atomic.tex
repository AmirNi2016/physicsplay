\input{../peeter_prologue.tex}

\usepackage{listings}
\lstset{ %
language=C++,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                   % adds a frame around the code
tabsize=2,              % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%}{)}          % if you want to add a comment within your code
}


\chapter{An attempt to illustrate differences between memory ordering and atomic access.}
\label{chap:atomic}
%\useCCL
\blogpage{http://sites.google.com/site/peeterjoot/math2009/atomic.pdf}
\date{Oct 8, 2009}
\revisionInfo{$RCSfile: atomic.tex,v $ Last $Revision: 1.5 $ $Date: 2009/11/21 17:37:48 $}

\beginArtWithToc
%\beginArtNoToc

\section{Motivation}

Use of atomic instructions for manipulating ``word'' (or smaller) size quantities, avoiding the use of mutual exclusion library functions is becoming increasingly easy for developers.  This hasn't made the using atomic instructions or library methods correctly any less difficult or error prone.

Information on how to use these written for an average joe developer is hard to come by.  What can be easily found is good detailed low level information targetted at or written by operating system kernel extension writers or compiler developers \cite{lyons2002powerpc}, or language implementors and designers \cite{lea2005jsr} \cite{boehmMM} \cite{manson2004jsr} \cite{boehm2007c++} \cite{sutter2006prism} \cite{blainey2007weakmm} \cite{adve1996shared}.  Once the low level details are understood one can move on to interesting exploitation topics such as lock free data structures \cite{harris2002practical} \cite{harris2001pragmatic} \cite{harrisPapers} \cite{benciaWeb} \cite{michael2002safe} \cite{valois-lock}.

The driving reason for most atomic usage is lock avoidance, and a desire for performance drives most use of atomic methods.  When atomic operations are used to replace lock protected updates, this can change the semantics of the program in subtle ways that are difficult to understand or get right.  In particular, additional use of memory barrier instructions or library methods may be required to correctly replace lock protected updates.

Discussion of problems and solutions associated with correct atomic usage has been a recurrent theme of many cubicle chats within DB2 development, and I will attempt to describe here part of what I've learned in some of those chats.
%of these problems over the years.
%.  I am now deluded enough to think that I understand some of this topic a bit, and am going to make an attempt here at 
%  A lecture discussion forum for sharing information on this topic is hard to use effectively, since it is hard to talk to many 
%.  I've given a couple internal talks on the subject, failing miserably at all but putting people to sleep.  In our own atomic library documentation to describe some of the issues in an coherent fashion.
%I've tried and failed previously in internal (DB2) talks, 
%  Part of the problem was that I was myself struggling to figure things out while also being the source for information.  
%of the issues, and their solutions.

My strategy for this discussion is to pick a single common atomic usage pattern that requires memory barriers for correctness, the use of an atomic to flag another memory update is complete.  As this is a lock avoidance pattern, it is helpful to understand some of the mechanics of how such a lock is implemented.  Hopefully my attempt at such a discussion will not be too scary.  After this the incorrect and correct versions of the atomic flagging code will be presented and discussed.

%replace a lock acquision It is often not understood that an atomic manipulation , even of a single variable how an atomic manipulation 
%.  Acquistion of a mutex, even for a short duration activity, can end up 
%Part of what makes atomic instructions hard to use is they are often not that th

\section{Why do I need locking or atomics?}

Why would you want to use an atomic?  If you are reading because you want to know a bit about memory barrier usage, you probably already know.  However, for illustrative purposes, consider the very simplest example of non-thread-safe updates of an integer in shared memory.

\begin{lstlisting}
   pShared->x++ ;
\end{lstlisting}

Any such operation requires three steps.  Read the value, change the value, update the value in memory.  On some platforms this is explicit and obvious since the value will have to be read into a register before making the update.  Here's a powerpc example that illustrates this

% use as -l output
\begin{lstlisting}
    lwz      gr4=(sharedMem)(gr3,0)    ; gr3 == pShared ; (gr3,0) == *pShared
    addi     gr0=gr4,1
    stw      (sharedMem)(gr3,0)=gr0
\end{lstlisting}

The value in the memory address contained in the register gr3 (offset by zero bytes) is loaded into a register (gr4).  One is added to this (into gr0), and the value is stored back into the original memory location.  Running on a CISC system this may all appear as one instruction, but it still requires the same number of steps internally.

Suppose you had a threaded application where this counter variable is updated routinely by many threads as they complete some routine and commonplace action.  What can now happen, interleaved (with time downwards) accross threads may now look like this

\begin{lstlisting}
T0: pShared->x = 0

T1: R1 = pShared->x (R1 = 0)
    R1 = R1 + 1
T1: <timeslice expires>

T2: R2 = pShared->x
    R2 = R2 + 1
    pShared->x = R2

T3: R3 = pShared->x (assume T3 "sees" T2's update, a value of 1)
    R3 = R3 + 1
    pShared->x = R3

T4: R4 = pShared->x (assume T4 "sees" T3's update, a value of 2)
    R4 = R4 + 1
    pShared->x = R4

T1: <new timeslice.  Starts running again.>
    pShared->x = R1 (T2 and T3's updates are now clobbered)
\end{lstlisting}

We had four updates to the counter after the initialization, but the counter value only goes up by one.  Uncontrolled updates like this can oscillate in peculiar fashions, and something that may be expected to be monotonically increase perhaps only trends upwards on average.  If the counter is not heavily contended you may even fluke out, running for a long time before getting unlucky with an inconsistent update of some sort.

%When I came out of school tenish years ago, multicpu machines were not a commodity applicance and it was a bit of a rude suprise that a sequence of increments would not do just that, increment.  Nowadays a garden variety computer from a big box store is a multi-cpu machine, and next year your phone will probably be too.  

This example leads us nicely to requirements for locking or atomic methods to protect such updates.

\subsection{Use of locking to make the read, change update cycle safe.}

Mutual exclusion mechanisms go by many names, locks, mutexes, critical sections, and latches to name a few.  System libraries usually provide implementations, a common example of which are the Unix pthread\_mutex methods.  The unsafe increment above can be made thread safe, protecting it with such a guard.  With error handling omitted for clarity, a modified example could be as follows

\begin{lstlisting}
   pthread_mutex_lock( &m ) ;

   pShared->x++ ;

   pthread_mutex_unlock( &m ) ;
\end{lstlisting}

If all threads only ever updates this value we can use a similar bit of code to read and do something with the read value

\begin{lstlisting}
   pthread_mutex_lock( &m ) ;

   v = pShared->x ;

   pthread_mutex_unlock( &m ) ;

   doSomethingWithIt( v ) ;
\end{lstlisting}

No two sequential samplings of the shared variable x would see the value go down was possible in the uncontrolled update case.  

Now, you may ask why a mutex is required for the read.  If that is just a single operation, why would it matter?  That's a good question, but not easy to answer portably.  If your variable is appropriately aligned and of an appropriate size (practically speaking this means less than or equal to the native register size) and you aren't compiling with funky compiler options that turn your accesses into byte accesses (such options do exist), then you may be able to do just that.  However, if that variable, for example, is a 64-bit integer and you are running on a 32-bit platform, then this read may take two instructions and you have a risk of reading the two halves at different points in time.  Similarly, suppose you were doing a 32-bit integer read, but that 32-bit integer was aligned improperly on a 16-bit boundary (on a platform that allows unaligned access), then your apparent single read may internally require multiple memory accesses (perhaps on different cachelines).  This could cause the same sort of split read scenerio.

You probably also need to declare your variable volatile and also have to be prepared to deal with a few other subtlies if lock avoidance on read is going to be attempted.  By the end of these notes some of those subtlies will have been touched on.

To make the story short, it should be assumed that if you want portable correct results you need to take and release the mutex for both read and write.

\subsection{What does correctness cost me?}

Now, having corrected the increment code, what does that cost us.  Timing the following very simple single threaded program with an without the mutex code

\begin{lstlisting}
#include <pthread.h>

int x = 0 ;
pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER ;

int inc() ;

int main()
{
   for ( int i = 0 ; i < 1000000000 ; i++ )
   {
      inc() ;
   }
}

int inc()
{
#if defined USE_THE_MUTEX
   pthread_mutex_lock( &m ) ;
#endif

   x++ ;

   int v = x ;

#if defined USE_THE_MUTEX
   pthread_mutex_unlock( &m ) ;
#endif

   return x ;
}
\end{lstlisting}

I find that this billion sets of call a function, increment a variable and return it without the mutex takes about 2.5 seconds.  With the mutex calls enabled, the total elapsed time required goes up to 8.5 seconds.  It costs over three times the time to do it with a mutex, and that's without any contention on the mutex whatsoever!  With threads all fighting over the mutex and blocking on kernel resources when they cannot get it, this is as good as it gets.  Correctness doesn't come cheaply.

\subsection{How does the lock work?}

Any mutex implementation requires at least one platform specific method of performing a read, update, change cycle as if it is done without interruption.  Your hardware will likely provide a \href{http://en.wikipedia.org/wiki/Compare-and-swap}{Compare and swap} or \href{http://en.wikipedia.org/wiki/Load-Link/Store-Conditional}{Load Link/Store Conditional} (which can be used to construct higher level atomics like compare and swap).  Implementing a mutex is intrinsically unportable territory.  Older \href{http://download.intel.com/design/intarch/manuals/24319101.pdf}{intel} cpus did not provide a compare and swap, but one did have a bus locked exchange available (LOCK XCHG).
%, so if you wanted to use compare and swap you had to know or mandate that the code could only be run on newer cpus.  
\href{http://ftp.parisc-linux.org/docs/arch/pa11_acd.pdf}{HP's PARSIC}, also easily argued to not be a modern architecture, only provides an atomic load and clear word instruction, also requiring 16 BYTE a aligned word.  What can be assumed is that any multiple cpu machine provides some sort of low level atomic instruction suitable for building a mutual exclusion interface.  If one were to imagine what a pthread\_mutex\_lock protected increment expands to, it would likely have the following form

\begin{lstlisting}
   T valueRead ;

   while ( ATOMIC_OP_WAS_SUCCESSFUL != SOME_INSTRUCTIONS_FOR_LOCK_ACQUISION( &m ) )
   {
      // instructions (like PAUSE()) for hyperthreaded cpus.

      // code for wasting a bit of time hoping to get the lock
      //   (when not running on a uniprocessor).

      // sleep or block using an OS primitive
   }

   valueRead = pShared->myLockProtectedData ;
   pShared->myLockProtectedData = valueRead + 1 ;

   SOME_INSTRUCTIONS_FOR_LOCK_RELEASE( &m ) ;
\end{lstlisting}

Even before talking about the hardware, one has to assume that there are mechanisms available to the library writer that prevent the generated code from being reordered.  Any instructions containing accesses of myLockProtectedData must NOT occur BEFORE the ACQUISION instructions, and must NOT occur AFTER the RELEASE instructions.  If the compiler were to generate code that had the following effect

\begin{lstlisting}
   T valueRead = pShared->myLockProtectedData ;
   pShared->myLockProtectedData++ ;

   pthread_mutex_lock( &m ) ;
   pthread_mutex_unlock( &m ) ;
\end{lstlisting}

Things would obviously be busted.  In the case of the pthread functions we have non-inline method with unknown side effects requiring a call to an external shared library.  This appears to be enough that the lock protected data does not have to be declared volatile and can be used while the mutex is held as if in a serial programming context.  Understanding the murky language rules that give us the desired thread safe behaviour is difficult (for an average joe programmer like me).  I came to the conclusion this is not a completely well defined area, motivating a lot of the recent C++ standards work into memory consistency and threaded behaviour.

Presuming one assume that the compiler is laying down the instructions for this code in exactly program order, or that there are mechanism available to ensure this, is this enough to guarentee that we have correct program behaviour?

It may come as a rude suprise that, depending on the instructions used to acquire and release the mutex, having the all the instructions scheduled in program order is NOT actually sufficient.  Not all hardware executes instructions in the order the compiler specifies, and we also need mechanisms to prevent the hardware from starting a memory access too early or letting a memory access complete too late.

This requirement for memory ordering instructions is really the whole point of this discussion.  We must have additional mechanism on top of the raw atomic instructions that only ensure read,change,update accesses on an isolated piece of memory (the lock word) can be made as if uninterruptable.

Suppose that one was using the \href{http://gcc.gnu.org/onlinedocs/gcc-4.1.2/gcc/Atomic-Builtins.html}{gcc Atomic Builtins} to attempt to implement a mutex.  One (generally) wrong way to use these would be

\begin{lstlisting}
   // the mutex data
   volatile int m = 0 ;
   #define HELD_BIT 1
   #define WAITER_BIT 2

   // Get the mutex.
   while ( TRUE )
   {
      int prev = __sync_fetch_and_or( &m, HELD_BIT ) ;

      if ( 0 == ( prev & HELD_BIT ) )
      {
         break ;
      }
      // sleep and other stuff, possibly blocking and setting WAITER_BIT.
   }

   // Got the mutex, access the protected data.
   pShared->myLockProtectedData++ ;

   // Release the mutex.
   int preLockState = __sync_fetch_and_and( &m, ~HELD_BIT ) ;
   assert( preLockState & HELD_BIT ) ;
   if ( preLockState & WAITER_BIT )
   {
      WakeUpAnyWaiters() ;
   }
\end{lstlisting} 

Reading the GCC docs one sees that this has desired compiler behaviour ``these builtins are considered a full barrier. That is, no memory operand will be moved across the operation, either forward or backward'', so the compiler will generate the instructions in the desired order.

Will the hardware retain that order?  NO, not neccessarily.  These compiler builtins are partially patterned after ones provided by intel, and intel designed them to match functionality provided by their \href{http://www.intel.com/design/itanium/manuals/245319.htm}{ia64 architecture}, an weakly ordered instruction set.  While ia64 was effectively killed by AMD64 and Windows (now living on only in the guise of HP's IPF systems), there are other weakly ordered instruction sets predating ia64.  Some powerpc chips that you will find on big AIX systems, or in older macs, or in your ps3 will be weakly ordered.  The \href{http://developers.sun.com/solaris/articles/sparcv9.pdf}{sparc architecture} allows for weak ordering, but the chips you find in sun machines implement their TSO (total store order) model which is mostly ordered.  However, this allowance for unordered memory is why, way down at the end of the GCC page, one finds a couple special \_lock methods.  The point of these is to ensure that the hardware does not reorder the relative order of memory accesses of the lock word and the lock data, even when the compiler lays down the code in ``program order''.

We can do a mutex implementation less wrongly using a memory barrier for release

\begin{lstlisting}
   // the mutex data
   volatile int m = 0 ;
   #define HELD_BIT 1

   // Get the mutex.
   while ( TRUE )
   {
      int prev = __sync_fetch_and_or( &m, HELD_BIT ) ;

      if ( 0 == ( prev & HELD_BIT ) )
      {
         break ;
      }
      // sleep and other stuff if you didn't get it
   }

   // Got the mutex, access the protected data.
   pShared->myLockProtectedData++ ;

   __sync_syncronize() ; 

   m = 0 ;
\end{lstlisting} 

Observe that the use of the word barrier is overloaded on the GCC page, with some usage associated with compiler instruction scheduling, and other usage referring to memory ordering.  Their syncronize function appears to be both.  Looking at the generated assembly for a weakly ordered system would verify this interpretation, and one would likely find an lwsync or sync instruction on powerpc for example.  This modification of the locking code ensures the compiler lays down the code with the clearing of the lock word after the protected data access, and the hardware memory barrier instruction should ensure that reads or writes to the protected data are complete before the store to the lock word is allowed to occur.  Depending on the type of instruction that is emitted for the syncronize builtin, this may not actually prevent loads and stores that occur after the lock release from being started before the lock is released.  Generally if that must be prevented a full barrier is required, and it would not suprise me if most implementations of library methods like pthread\_mutex\_unlock do not prevent memory accesses that occur after the lock release from being initiated before the lock release occurs.  This sort of subtlety is not likely to be found in documentation for the mutex library functions.

With the sample code above we are still left with the possibility that the hardware will execute the memory access instructions for the lock protected data before the lock is acquired, making the lock useless.  We can fix this with the \_lock test and set and release builtins, and also remove the likely overzelous full memory barrier that syncronize provides (allowing post lock release memory access into the critical section).  Sample code for this would may look like

\begin{lstlisting}
   // the mutex data
   volatile int m = 0 ;
   #define HELD_VALUE 1

   // Get the mutex.
   while ( TRUE )
   {
      int wasHeld = __sync_lock_test_and_set( &m, HELD_VALUE ) ;

      if ( !wasHeld )
      {
         break ;
      }
      // sleep and other stuff if you didn't get it
   }

   // Got the mutex, access the protected data.
   pShared->myLockProtectedData++ ;

   __sync_lock_release( &m ) ; 
\end{lstlisting} 

This now implements functional mutual exclusion code, even on weakly ordered systems.  The compiler has provided methods that ensure it does not move the lock protected data accesses out of the critical section, we are using atomic instructions that guarentee other threads cannot also be modifying the data (presuming they all use the same mutex), and also have instructions that ensure the hardware does not inappropriately reorder the memory accesses.  Inappropriate reordering means that accesses to the lock protected data remain after the lock acquision and before the lock release.  It may not mean that memory accesses from before the lock acquision are done by the time the lock is acquired nor that memory accesses that occur after the lock is released occur after the lock is released (the compiler says it does that, but the hardware may be a sneaky bastard behind your back).

So, that's a mutex.  We can use something like this to avoid some of the excessive cost observed when using the pthread library mutex functions, and would expect that the uncontended cost of the back to back increment code could be lessened.

Presuming you do not mind the unportability of such code, and manage to get it right, and want to live with maintaining your own mutex implementation, if you do a poor job dealing with contention, you may very well get worse performance than the system provided methods in the contended codepath.

You really really have to want the performance to go down the path of implementing your own mutual exclusion code.  DB2 is such a product.  DB2 is also a product where first born sacrifices in the name of performance are not considered abhorent.  In our defense our mutex implementation predates the availability of widely available library methods.  We had SMP exploitation before pthreads existed using mutiple processes and shared memory.  Our own implementation also has the advantage of providing framework to build problem determination functionality that cannot be found in generic library implementations.  Performance driven coding is not the only reason we do it ourself even in this more modern age where library methods do exist.

\subsection{What is an atomic, and how does an atomic method work?}

At the core of every atomic library method or compiler intrinsic is one or more special purpose instructions, such as a 
Use of these almost invariably requires a retry loop at one level or another.

\section{Guts}

The usual pattern for memory barrier usage matches what you would put in the implementation of a critical section, but split into pairs for the producer and consumer. As an example your critical section implementation would typically be of the form:

\begin{lstlisting}
while (!pShared->lock.testAndSet_Acquire()) ;
// (this loop should include all the normal critical section stuff like
// spin, waste, 
// pause() instructions, and last-resort-give-up-and-blocking on a resource 
// until the lock is made available.)

// Access to shared memory.

pShared->foo = 1 
v = pShared-> goo

pShared->lock.clear_Release()
\end{lstlisting}

Acquire memory barrier above makes sure that any loads (pShared->goo) that may have been started before the successful lock modification are tossed, to be restarted if necessary.

The release memory barrier ensures that the load from goo into the (local say) variable v is complete before the lock word protecting the shared memory is cleared.

You have a similar pattern in the typical producer and consumer atomic flag scenario (it is difficult to tell by your sample if that is what you are doing but should illustrate the idea).

Suppose your producer used an atomic variable to indicate that some other state is ready to use. You'll want something like this:

\begin{lstlisting}
pShared->goo = 14

pShared->atomic.setBit_Release()
\end{lstlisting}

Without a "write" barrier here in the producer you have no guarantee that the hardware isn't going to get to the atomic store before the goo store has made it through the cpu store queues, and up through the memory hierarchy where it is visible (even if you have a mechanism that ensures the compiler orders things the way you want).

In the consumer

\begin{lstlisting}
if ( pShared->atomic.compareAndSwap_Acquire(1,1) )
{
   v = pShared->goo 
}
\end{lstlisting}

Without a "read" barrier here you won't know that the hardware hasn't gone and fetched goo for you before the atomic access is complete. The atomic (ie: memory manipulated with the Interlocked functions doing stuff like lock cmpxchg), is only "atomic" with respect to itself, not other memory.

Now, the remaining thing that has to be mentioned is that the barrier constructs are highly unportable. Your compiler probably provides \_acquire and \_release variations for most of the atomic manipulation methods, and these are the sorts of ways you would use them. Depending on the platform you are using (ie: ia32), these may very well be exactly what you would get without the \_acquire() or \_release() suffixes. Platforms where this matters are ia64 (effectively dead except on HP where its still twitching slightly), and powerpc. ia64 had .acq and .rel instruction modifiers on most load and store instructions (including the atomic ones like cmpxchg). powerpc has separate instructions for this (isync and lwsync give you the read and write barriers respectively).

Now. Having said all this. Do you really have a good reason for going down this path? Doing all this correctly can be very difficult. Be prepared for a lot of self doubt and insecurity in code reviews and make sure you have a lot of high concurrency testing with all sorts of random timing scenarios. Use a critical section unless you have a very very good reason to avoid it, and don't write that critical section yourself.


\EndArticle
%\EndNoBibArticle
