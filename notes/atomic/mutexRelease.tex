%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\input{../peeter_prologue_print.tex}

\usepackage{listings}
\lstset{ %
language=C++,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                   % adds a frame around the code
tabsize=2,              % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
%escapeinside={\%}{)}          % if you want to add a comment within your code
}

\chapter{Timings and correctness issues for mutex release operations}
\label{chap:mutex}
%\useCCL
\blogpage{http://sites.google.com/site/peeterjoot/math2010/mutexRelease.pdf}
\date{March 6, 2009}

%\beginArtWithToc
\beginArtNoToc

\section*{}
{\small{\textbf{Motivation.}

I have made some notes previously about Intel memory barrier instructions in the posts XXX, and YYY.  What was the driving reason for these?

Our product (DB2) breaks what probably ought to be the first rule for the use of spinlocks.  Do not implement these yourself.  Much of this is historical, because our product predated any library implementations.  Another part of this is portability.  These types of interfaces are not available in any sort of library that spans all the platforms we require.  Another part of this is performance.  Our implementation works very well, and is well tuned for the specific purposes we use it for.  The last part is purely an issue of code momentum.  Given that we have exposed an interface to our callers changes made to the spinlock code must avoid having a side effect of changing the interfaces exposed to higher level DB2 code, and even internal changes can be tricky due to the massive platform and portability matrix we are constrained to support.

Given that we are stuck with our own implementation, we are also stuck with maintenance.  Our maintenance story has improved and compared to when I have started we now have a much better abstraction of this code, with less redundancy and platform specifics.  Of all the parts of our operating system portability library layer that is exposed to higher level DB2 code and DB2 developers, this is by far one of the touchiest and scariest parts of our code.  Nobody really wants to touch it if they do not have to.

These posts were in response to understanding if a change to some of this code was required.  In particular, we have some inconsistencies in what barrier instructions and mechanisms are used to implement the unlock portion of one of our types of spinlocks.  It does not really give away any trade secrets to describe some of this externally.  One of our spinlock implementations is a simple and dumb interface that can be used without restriction.  The other sort of spinlock that we implement has additional state, including waiter information used in conjunction with operating system specific functions for wait and wakeup.  This last spinlock type in unambiguously correct release semantics on Intel due to necessary use of locked compare and exchange instructions for unlock.  This is or what should be the mutex type preferred for most of our code.  The more primitive variation is supplied to DB2 code for use in special situations, and in that code the platform inconsistencies in the barrier instructions was noticed.

\section{Lock release barriers}

Here are three variations of unambiguously correct possible primitive lock release instructions for Intel targeted code (assuming the spinlock does not have additional state that must be retained)

\begin{lstlisting}

lock movb [mem], 0

xchg [mem],0	; NOTE: lock implied for xchg with memory operand.

mfence ; movb [mem],0

\end{lstlisting}

It is worth understanding the differences in cost and behavior to a plain old store zero, such as that achieved with the following code sequence

\begin{lstlisting}
// possible memory access associated with the critical section that is implemented by this spinlock
pSharedMem->modifiedField = someValue ; // store something under protection of the critical section.

readValue = pSharedMem->someField ; // read something under protection of the critical section.

__asm__ __volatile__("" : : : "memory" ) ;  // compiler fence to avoid load and store instruction motion past this point.

pMutex->lockWord = 0 ;
\end{lstlisting}

The "memory" constraint in this GCC style inline assembly is a mechanism used to ensure that the compiler does not introduce inappropriate code reordering (especially of instructions with memory accesses).  This works for both the GNU and Intel compilers, and other mechanisms (such as an explicit out of line function call) are required for different compilers.

Lets examine the memory accesses in the listing above.  We have no LFENCE, SFENCE, nor MFENCE instructions, and no implicit or explicit LOCK prefix associated with this store.  What guarantees do we have that things will have the desired order.  We want the load of someField to be complete before the mutex can be acquired by any other process.  If we get a value that is assigned after a subsequent acquisition of the mutex by another thread or process then this mutex has not done its job.  Similarly the store to modifiedField must be complete before something else can acquire the mutex.

Now, if we look at the \href{http://download.intel.com/design/intarch/manuals/24319101.pdf}{Intel system programming guide}, we find in 8.2.2 ``Writes are not reordered with older reads''.  The older read here is the access of someField, storing to the local variable readValue.  Good so far.  How about the store?  Also in 8.2.2 we find ``Writes are not reordered with other writes, with the exception of...''.  The exceptions include streaming stores executed with non-temporal move instructions (MOVNTI, ...), and string operations.

So, it appears that if these two sets of memory operations are the only accesses protected by the mutex we are covered.  Does real code have to worry about non- temporal streaming instructions and these string operation instructions?  What are these beasties, and can we trust our compilers and system libraries (such as the C runtime libraries) to avoid the use of such instructions?  Details like these are not likely documented or guaranteed in any form, so to be safe it appears that something more would be required generally.  For DB2 use where we cannot possible audit all the code generated within all such mutex use, we would need a guarantee that none of the Intel, GCC, Sun(AMD) or Microsoft compilers, generated any such instructions or use these in their compiler runtimes.  We would also require that the C runtime libraries on Linux, Solaris, and Windows used no such instructions.  That sounds like a bit much to ask for, although we could possibly get ``lucky'' and avoid seeing any problem due to the missing  barrier instructions.  If there was a runtime failure to enforce the required ordering chances are that it would be pretty much impossible to debug or solve it!

A little bit of digging quickly answers this question.  Creating a program that includes some non-inline calls to memcpy, disassembly shows that the Intel runtime library functions associated with such a call can include such instructions.  I found for example:

\begin{lstlisting}
  404990:   49 83 e8 40             sub    $0x40,%r8
  404994:   0f 18 82 80 01 00 00    prefetchnta 0x180(%rdx)
  40499b:   4c 8b 0a                mov    (%rdx),%r9
  40499e:   4c 0f c3 09             movnti %r9,(%rcx)
  4049a2:   4c 8b 52 08             mov    0x8(%rdx),%r10
  4049a6:   4c 0f c3 51 08          movnti %r10,0x8(%rcx)
  4049ab:   4c 8b 5a 10             mov    0x10(%rdx),%r11
  4049af:   4c 0f c3 59 10          movnti %r11,0x10(%rcx)
  4049b4:   4c 8b 4a 18             mov    0x18(%rdx),%r9
  4049b8:   4c 0f c3 49 18          movnti %r9,0x18(%rcx)
  4049bd:   4c 8b 52 20             mov    0x20(%rdx),%r10
  4049c1:   4c 0f c3 51 20          movnti %r10,0x20(%rcx)
  4049c6:   4c 8b 5a 28             mov    0x28(%rdx),%r11
  4049ca:   4c 0f c3 59 28          movnti %r11,0x28(%rcx)
  4049cf:   4c 8b 4a 30             mov    0x30(%rdx),%r9
  4049d3:   4c 0f c3 49 30          movnti %r9,0x30(%rcx)
  4049d8:   4c 8b 52 38             mov    0x38(%rdx),%r10
  4049dc:   4c 0f c3 51 38          movnti %r10,0x38(%rcx)
  4049e1:   48 8d 52 40             lea    0x40(%rdx),%rdx
  4049e5:   48 8d 49 40             lea    0x40(%rcx),%rcx
  4049e9:   49 83 f8 40             cmp    $0x40,%r8
  4049ed:   7d a1                   jge    404990 <__intel_new_memcpy+0x16a0>
  4049ef:   e9 7c ff ff ff          jmpq   404970 <__intel_new_memcpy+0x1680>
  4049f4:   66                      data16
\end{lstlisting}

Here we have piles of the non-temporal movnti instructions.  This compiler runtime helper function is a massive one, and looking in other places one finds that some of the non-temporal stores are in fact associated with a SFENCE instruction, such as:

\begin{lstlisting}
  406190:   49 81 e8 80 00 00 00    sub    $0x80,%r8
  406197:   66 0f e7 07             movntdq %xmm0,(%rdi)
  40619b:   66 0f e7 47 10          movntdq %xmm0,0x10(%rdi)
  4061a0:   66 0f e7 47 20          movntdq %xmm0,0x20(%rdi)
  4061a5:   66 0f e7 47 30          movntdq %xmm0,0x30(%rdi)
  4061aa:   66 0f e7 47 40          movntdq %xmm0,0x40(%rdi)
  4061af:   66 0f e7 47 50          movntdq %xmm0,0x50(%rdi)
  4061b4:   66 0f e7 47 60          movntdq %xmm0,0x60(%rdi)
  4061b9:   66 0f e7 47 70          movntdq %xmm0,0x70(%rdi)
  4061be:   48 81 c7 80 00 00 00    add    $0x80,%rdi
  4061c5:   49 81 f8 80 00 00 00    cmp    $0x80,%r8
  4061cc:   7d c2                   jge    406190 <__intel_new_memset+0xac0>
  4061ce:   0f ae f8                sfence
\end{lstlisting}

Here we have a tight loop of MOVNTDQ instructions until done, and when done things terminate with an SFENCE instruction.  It is unambiguous in this second listing that the non-temporal stores will be ordered in a fashion equivalent to a regular plain old store instruction.  If all the non-temporal stores in this runtime library code terminate with an SFENCE instruction before this function exits then we are covered for this one method, but looking at the listing it is not obvious whether or not that is the case.  Perhaps so, but if we are looking for an answer to the question, ``is it possible to encounter non- temporal stores for code that could be executed during a critical section'', we have an answer in the affirmative.  Perhaps the Intel runtime does do an SFENCE instruction before it returns in all codepaths, but a sophisticated symbolic disassembler, or the source code itself is probably required to try to answer that question.  I was not about to try such a disassembly myself (the reverse engineering implied in such an attempt would also likely violates license agreements).

How about the string operation exception to the store ordering guarantee of a pair of stores?  One can also find MOVS instructions in the very same runtime library function, so concern about these is also required.  For us we need to understand if the store zero to the lock word of the mutex that releases this lock is necessarily ordered after the MOVS store instruction that may be associated with the data that this mutex protects.  The answer can be found in 8.2.4.1 bullet 3, where it states ``String operations are not reordered with other stores''.  Provided the memory is WB (write back), whatever that means, we are covered.  We do not have a guarantee that string operations have any intrinsic ordering between themselves, but only need to keep the animal in the cage, making sure that any stores, including these are complete before the lock release is visible to other cpus.

So, given this, it appears that the lock release code fragment must be modified like so for correctness if there is a possibility of non-temporal store instructions

\begin{lstlisting}
// possible memory access associated with the critical section that is implemented by this spinlock
pSharedMem->modifiedField = someValue ; // store something under protection of the critical section.

readValue = pSharedMem->someField ; // read something under protection of the critical section.

__asm__ __volatile__("sfence" : : : "memory" ) ;  // compiler fence to avoid load and store instruction motion past this point.

pMutex->lockWord = 0 ;
\end{lstlisting}

The SFENCE is not required for the possible MOVS instructions that may be executed, but may be required, at least pathologically, if we have a memcpy while the mutex is
held.

Next, let us move on to consider the load fencing.  Can any of the loads get out of the cage?  We now go back to the manual.  It states that ``Reads are not reordered with other reads''.  So if we released the mutex with xchg, we would be covered.  That is not the case in this first or second listing for this potential simple lock release code.  Ah, here we go ``Writes are not reordered with older reads''.  We are good.  It appears that the SFENCE plus store zero is sufficient and correct for the lock release, even if the code in question contains non-temporal stores.  The next question is whether or not it is better to use an SFENCE or to use an XCHG (with implied LOCK prefix).

\section{Some timings for FENCE and LOCK XCHG instructions}

\section{Conclusions}

%\EndArticle
\EndNoBibArticle
