\documentclass{article}

\input{../peeters_macros.tex}
\input{../peeters_macros2.tex}

\newcommand{\ket}[1]{\lvert {#1} \rangle}
\newcommand{\bra}[1]{\langle {#1} \rvert}
\newcommand{\braket}[2]{\langle{#1} \vert {#2}\rangle}
\newcommand{\ketbra}[2]{\ket{#1}\bra{#2}}
\newcommand{\BraOpKet}[3]{\bra{#1} \hat{#2} \ket{#3} }

\usepackage[bookmarks=true]{hyperref}

\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package. 
\usepackage{latexsym,amssymb,epsf} % don't remember if these are
   % needed, but their inclusion can't do any damage


\title{ Dirac delta function in terms of orthogonal functions. }
\author{Peeter Joot \quad peeter.joot@gmail.com }
\date{ March 8, 2009.  Last Revision: $Date: 2009/03/09 12:58:43 $ }

\begin{document}

\maketitle{}
%\tableofcontents

\section{ Motivation. }

Chapter II of \cite{pauli2000wm} expresses the delta function in terms of othonormal basis functions, but the treatment is slightly
hard to follow.  Try this the slow and dumb way.

\section{ Fourier coefficients. }

Given an othonormal basis 

\begin{align*}
\int u_m^\conj(x) u_n(x) = \delta_{mn}
\end{align*}

For a function that can be expressed entirely in this basis, such as

\begin{align*}
f(x) = \sum_k a_k u_k(x)
\end{align*}

We can then compute the Fourier coefficients $a_k$ in the normal fashion

\begin{align*}
\int u_k^\conj(x) f(x) dx 
&= \sum_n a_n \int u_k^\conj(x) u_n(x) dx \\
&= \sum_n a_n \delta_{kn} \\
&= a_k \\
\end{align*}

So we have 
\begin{align*}
f(x) = \sum_k a_k u_k(x)  = \sum_k u_k(x) \int u_k^\conj(x') f(x') dx'
\end{align*}

\subsection{ Mean square convergence. }

How good of a match is a subset of such a sum?  Pauli considers a mean convergence.

\begin{align*}
0 &= \lim_{N \rightarrow \infty}\int 
{\Abs{f(x') -\sum_{k=1}^N a_k u_k(x') }}^2 dx'  \\
&=
\int \left(f^\conj(x') -\sum_{k=1}^N a_k^\conj u_k^\conj(x') \right) \left(f(x') - \sum_{m=1}^N a_m u_m(x') \right) 
dx' \\
&=
\int 
\left( f^\conj(x') f(x') 
-f^\conj(x') \sum_{m=1}^N a_m u_m(x') 
- \sum_{k=1}^N a_k^\conj u_k^\conj(x') f(x') 
+ \sum_{m=1}^N a_m u_m(x') \sum_{k=1}^N a_k^\conj u_k^\conj(x')  \right)
dx' \\
&=
\int f^\conj(x') f(x') dx'
- \sum_{m=1}^N a_m a_m^\conj
- \sum_{k=1}^N a_k^\conj a_k
+ \sum_{m=1}^N \sum_{k=1}^N a_m a_k^\conj \delta_{km} \\
%\int u_m(x') u_k^\conj(x') dx' \\
&= \int \Abs{f(x')}^2 dx' - \sum_{m=1}^N \Abs{a_m}^2 \\
\end{align*}

So if we have mean square equality in the limit as $N \rightarrow \infty$, then it must also be true that

\begin{align*}
\int \Abs{f(x')}^2 dx' = \sum_{m=1}^\infty \Abs{a_m}^2 \\
\end{align*}

He calls this the completeness relation.  If the othonormal basis is sufficient to express the set of desired functions, then
the squared absolute value of such functions can be expressed entirely in terms of the fourier coefficients.  The mean square
equality is weaker in the sense that a function can be mismatched to its fourier representation at a set (of ``measure zero'') points,
and still meet the mean square equality statement.

\subsection{ Generalizing the inner product. }

Pauli next introduces the an inner product on functions (without calling it that) 
in a somewhat indirect
fashion (ie: in terms of fourier components instead of by definition).

Supposing that one has two functions built up by Fourier components

\begin{align*}
f(x) &= \sum_k a_k u_k(x) \\
g(x) &= \sum_k b_k u_k(x) \\
\end{align*}

Then we have 
\begin{align*}
\int f^\conj(x) g(x) &= \sum_{k,m} a_k^\conj b_m \int u_k^\conj(x) u_m(x) = \sum_k a_k^\conj b_k \\
\int g^\conj(x) f(x) &= \sum_{k,m} a_k b_m^\conj \int u_m^\conj(x) u_k(x) = \sum_k b_k^\conj a_k \\
\end{align*}

This is something that is familar to anybody who has taken a linear
algebra course, but perhaps had to be motivated when he wrote the book?

\subsection{ Delta function as a sum. }

Perhaps Pauli wrote this general function inner product that way to show a natural way that a sum of the
form

\begin{align*}
\sum u_m^\conj(x) u_k(x)
\end{align*}

arises in use, because he now writes the completeness relation using a sum similar to that above

\begin{align}
\sum_{k} u_k^\conj(x) u_k(x') \equiv \delta(x-x') 
\end{align}

I'd seen this in bra ket notation, in Susskind's lectures as noted in \cite{PJQmSusskind}, and also in \cite{mcmahon2005qmd} as the
identity operator

\begin{align}
\sum_{k} \ketbra{k}{k} \equiv \delta(x-x') 
\end{align}

From neither of those two sources did I understand where it came from (in Susskind's lectures it appeared to be
related to Fourier transforms).
As Pauli did, let's verify that this works, and try to relate this to a few specific choices of inner products (covering at
least classical Fourier series and the Fourier transform).

FIXME: finish writing this up in a way that I like it.  Having done the above I now follow the remainder too.  After "transcribing" Pauli's
notes in a way that I like then relate this to two specific cases.

first is the fourier series (bounded integral bounds).  second is fourier transform (infinite bounds).  Both with exponential basis.

What is the delta function representation in each of these basis inner-product representations?

Once done this, relate to the bra-ket notation.

Getting a grasp on this will clarify what Susskind did in his lectures with the identity representation (also done in \cite{mcmahon2005qmd}).

\bibliographystyle{plainnat}
\bibliography{myrefs}

\end{document}
