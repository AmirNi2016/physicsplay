\input{../peeter_prologue.tex}

\chapter{Linearizing a set of regular differential equations.}
%\label{chap:template}
%\useCCL
%\blogpage{http://sites.google.com/site/peeterjoot/math2009/template.pdf}
%\date{Sept XX, 2009}
\revisionInfo{$RCSfile: linearizeDE.tex,v $ Last $Revision: 1.2 $ $Date: 2009/11/13 05:11:38 $}

%\beginArtWithToc
\beginArtNoToc

%\section{Motivation}
%\section{Guts}

%Hi Lut,
%
%Sorry for the over the head remarks.  Here's an elaboration.  
All of these discrete multiple particle systems appear to generate coupled differential equations of the following form

\begin{align}
A z' = b(z)
\end{align}

Where $A = A(z)$ is a matrix, and $b(z)$ a column vector valued non-linear function.  Consider the nicely behaved case where $A(z)$ is invertable for all $z$.  Then we can write

\begin{align}
z' = A^{-1} b(z)
\end{align}

Now with a non-linear function $b$ (like the sines that we have in the pendulum problem from $-\grad \cos\phi$), we can't solve this thing easily, but in some small-enough neighbourhood of some point (i.e. a point in phase space containing $z$) we can make a linear approximation.  Suppose our initial phase space point is $z_0$, and we wish to solve for differential displacement from that point $x$, namely $z = z_0 + x$.  Then we have for our system

\begin{align}
x' = \evalbar{A^{-1} b(z)}{z=z_0} + 
\evalnobar{
\begin{bmatrix}
\PD{z_1}{[A^{-1}b(z)]_1} & \PD{z_2}{[A^{-1}b(z)]_1} & \hdots & \PD{z_N}{[A^{-1}b(z)]_1} \\
\PD{z_1}{[A^{-1}b(z)]_2} & \PD{z_2}{[A^{-1}b(z)]_2} & \hdots & \PD{z_N}{[A^{-1}b(z)]_2} \\
\vdots \\
\PD{z_1}{[A^{-1}b(z)]_N} & \PD{z_2}{[A^{-1}b(z)]_N} & \hdots & \PD{z_N}{[A^{-1}b(z)]_N} \\
\end{bmatrix}}{z = z_0} x
\end{align}

Now we have a linear matrix, corresponding roughly to a first order Taylor expansion of the original system of equations.  Mathematically the problem is now reduced to a column vector linear system of the form

\begin{align}\label{eqn:zoo1}
x' = B x + a
\end{align}

When $a = 0$ the solution is just

\begin{align}
x = e^{B t} x(0)
\end{align}

(where you can evaluate $e^{Bt}$ the usual way using an eigenvalue similarity transformation where the exponential of the inner diagonal term is then simple).

Assuming a non-homogeneous solution of the same form $x = e^{B t} f(t)$ one finds the specific solution 

\begin{align}
x = e^{B t} \int_0^t e^{-B\tau} a d\tau
\end{align}

So the complete solution (a specific solution plus the homogeneous solution) to the system \ref{eqn:zoo1} is found to be

\begin{align}
x = e^{B t} \left( x(0) + \int_0^t e^{-B\tau} a d\tau \right)
\end{align}

Now, this works out all nice and pleasantly when $A$ is invertable.  What do we do when it is not?  Each zero eigenvalue of $A$ looks like it corresponds to a conserved quantity.  Manually removing those zeros from the system can reduce things to the form dealt with here.  What is even trickier seeming is what happens if the matrix is almost invertable.  Example

\begin{align}
\begin{bmatrix}
1 & 0 \\
0 & 0.0000000001
\end{bmatrix} z' = b(z)
\end{align}

This is perfectly invertable mathematically, but numerically you really don't want to go there, since you'll end up with garbage.  What I think would work for dealing with this sort of system is using the SVD (symmetric value decomposition) techniques to determine a orthonormal basis for all the ``non-zero'' eigenvalues according to a reasonable numerical threshold for these zeros.  I can't fully spell out the details quickly for this since I'd have to work through them first.  SVD wasn't covered in our linear algebra course when I was in school but I've read of it since and was very impressed with its power and utility (but need more study of it to fully grasp it and its applications).  Prof Gilbert Strang covers this in his online lectures (I saw them on iTunesU), and I'd recommend them highly.

%\EndArticle
\EndNoBibArticle
