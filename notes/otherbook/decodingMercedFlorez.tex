%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

% 
% 
%\input{../peeter_prologue.tex}

\chapter{Decoding the Merced Florez article}
\label{chap:decodingMercedFlorez}
%\useCCL
\blogpage{http://sites.google.com/site/peeterjoot/math2009/decodingMercedFlorez.pdf}
\date{Sept 10, 2009}
\revisionInfo{$RCSfile: decodingMercedFlorez.tex,v $ Last $Revision: 1.2 $ $Date: 2009/12/03 03:24:40 $}

%\beginArtWithToc
\beginArtNoToc

\section{Motivation}

Still trying to understand section II of \citep{montesinos2006sem}.  Lets do this one bit at a time and identify the parts that are understood or not.

\section{Infinitesimal Poincare transformation}

Infinitesimal Poincare transformation is given (11) as 

\begin{align*}
{x'}^\mu &= x^\mu + \delta x^\mu \\
\delta x^\mu &= {\epsilon^\mu}_\nu x^\nu + \epsilon^\nu
\end{align*}

Figured this one out in \citep{gabook:poincareTx} and \citep{gabook:rotationGenerator}.  In non-coordinate form this is really translation plus incremental Lorentz transform

\begin{align}\label{eqn:decodingMercedFlorez:hoo1}
x' = x + a + (i \cdot x)
\end{align}

where $i$ is a spacetime bivector for a boost an spatial bivector for a rotation.  Writing $i = u^\alpha v^\beta \gamma_\alpha \wedge \gamma_\beta$ for the wedge of two four vectors $u = u^\alpha \gamma_\alpha$ and $v = v^\alpha \gamma_\alpha$, we have

\begin{align*}
(i \cdot x)^\mu = (i \cdot x) \cdot \gamma^\mu = (u^\mu v^\nu - u^\nu v^\mu) x_\nu 
\end{align*}

so we identify

\begin{align*}
\epsilon^{\mu\nu} &= (u^\mu v^\nu - u^\nu v^\mu) \\
{\epsilon^{\mu}}_\nu &= (u^\mu v_\nu - u_\nu v^\mu)
\end{align*}

\section{Inverting the infinitesimal Poincare transformation}

The infinitesimal shift and rotate of \ref{eqn:decodingMercedFlorez:hoo1} should be invertible, and intuition says this would be just the opposite shift and inverse rotation

\begin{align}\label{eqn:decodingMercedFlorez:hoo2}
x = x' - a - (i \cdot x')
\end{align}

In the tensor formulation of the article can one show this?  We can write

\begin{align*}
{x'}^\mu 
&= x^\mu + {\epsilon^\mu}_\nu x^\nu + \epsilon^\mu \\
&= ({\delta^\mu}_\nu + {\epsilon^\mu}_\nu) x^\nu + \epsilon^\mu \\
&= (\eta^{\mu\nu} + \epsilon^{\mu\nu}) x_\nu + \epsilon^\mu \\
\end{align*}

With the $-,+,+,+$ metric used in the article this tensor factor of $x_\nu$ this can make the index form a bit less abstract, putting it in explicit matrix form.  Two examples

\begin{align*}
\begin{bmatrix}
{x'}^0 \\
{x'}^1 \\
{x'}^2 \\
{x'}^3
\end{bmatrix}
=
\begin{bmatrix}
-1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 \\ 
0 & 0 & 1 & 1 \\ 
0 & 0 & -1 & 1 \\ 
\end{bmatrix}
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
x_3
\end{bmatrix}
+
\begin{bmatrix}
\epsilon^0 \\
\epsilon^1 \\
\epsilon^2 \\
\epsilon^3
\end{bmatrix}
\end{align*}

\begin{align*}
\begin{bmatrix}
{x'}^0 \\
{x'}^1 \\
{x'}^2 \\
{x'}^3
\end{bmatrix}
=
\begin{bmatrix}
-1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 1 \\ 
0 & 0 & 1 & 1 \\ 
0 & -1 & -1 & 1 \\ 
\end{bmatrix}
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
x_3
\end{bmatrix}
+
\begin{bmatrix}
\epsilon^0 \\
\epsilon^1 \\
\epsilon^2 \\
\epsilon^3
\end{bmatrix}
\end{align*}

These are only examples, and in general we will have the metric signature along the diagonal, and a completely antisymmetric matrix.  With the matrix $U$ upper diagonal, the general form of the matrix will be

\begin{align*}
\begin{bmatrix}
\eta^{\mu\nu}
\end{bmatrix}
+ U - U^\T
\end{align*}

The second example shows that the inverse of this matrix is not necessarily directly proportional to the transpose as is the case for the first example.  What can we say in general about matrices of this form?  What are the eigenvalues and vectors?  This matrix is not symmetric, so the spectral theorem doesn't necessarily apply.

If one assumes that it is invertible then one can form an inverse of the form

\begin{align*}
x^\alpha =
(\eta_{\mu\alpha} )^{-1} (\eta^{\mu\nu} + \epsilon^{\mu\nu})^{-1} 
({x'}^\mu - \epsilon^\mu )
\end{align*}

My initial intuition on the inverse appears to be wrong.  Instead perhaps just

\begin{align*}
x 
&= x' - i \cdot (x' - a) \\
&= x' + i \cdot a - i \cdot x' 
\end{align*}

If that were the case we should have

\begin{align*}
x' \questionEquals
x' + i \cdot a - i \cdot x' 
+ i \cdot (x' + i \cdot a - i \cdot x' ) \\
\implies \\
0 &=
+ i \cdot a 
+ i \cdot (i \cdot a - i \cdot x' ) \\
\end{align*}

That can only be true if both $x'$ and $a$ are perpendicular to $i$ (not in the span of the vectors $u$ and $v$ that lie in the plane described by the bivector $i = u \wedge v$).

\section{Eigenvalues and vectors for antisymmetric and diagonal plus antisymmetric}

Answering the idle question above calculated the similarity transformation for a couple examples

\begin{align*}
A &= 
\begin{bmatrix}
0 & -1 \\
1 & 0  \\
\end{bmatrix} 
= E^\dagger 
\begin{bmatrix}
i & 0 \\
0 & -i \\
\end{bmatrix} 
E \\
E &=
\inv{\sqrt{2}}
\begin{bmatrix}
1 & 1 \\
i & -i \\
\end{bmatrix} 
\end{align*}

\begin{align*}
A &= 
\begin{bmatrix}
1 & -1 \\
1 & 1  \\
\end{bmatrix} 
= E^\dagger 
\begin{bmatrix}
1+i & 0 \\
0 & 1-i \\
\end{bmatrix} 
E \\
E &=
\inv{\sqrt{2}}
\begin{bmatrix}
1 & 1 \\
-i & i \\
\end{bmatrix} 
\end{align*}

For 

\begin{align*}
A &= 
\begin{bmatrix}
1 & 0 & 1 \\
0 & 1 & -1 \\
-1 & 1 & 1  \\
\end{bmatrix} 
\end{align*}

There's one real eigenvalue, and two complex.  Namely $\lambda = 1, 1 \pm \sqrt{2} i$.  For that I didn't bother computing the eigenvectors.

And
\begin{align*}
A &= 
\begin{bmatrix}
1 & 1 & 1 \\
-1 & 1 & -1 \\
-1 & 1 & 1  \\
\end{bmatrix} 
\end{align*}

Again one real eigenvalue, and two complex.  Namely $\lambda = 1, 1 \pm \sqrt{3} i$.  I'd guess this is the pattern in general, one complex pair of conjugate eigenvalues from the antisymmetric part, and one real eigenvalue from the diagonal contribution.

\EndArticle
%\EndNoBibArticle
