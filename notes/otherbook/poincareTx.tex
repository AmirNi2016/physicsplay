%\input{../peeter_prologue.tex}

\chapter{Poincare transformations.}
\label{chap:poincareTx}
\blogpage{http://sites.google.com/site/peeterjoot/math2009/poincareTx.pdf}
%\author{Peeter Joot \quad peeter.joot@gmail.com }
\date{ June 1, 2009.  $RCSfile: poincareTx.tex,v $ Last $Revision: 1.2 $ $Date: 2009/12/03 03:24:40 $ }

\beginArtWithToc

\section{Motivation}

In (\cite{montesinos2006sem}) a Poincare transformation is used to develop the symmetric stress energy tensor directly, in contrast to the non-symmetric canonical stress energy tensor that results from spacetime translation.

Attempt to decode one part of this article, the use of a Poincare transformation.

\section{Incremental transformation in GA form.}

Equation (11) in the article, is labeled an infinitesimal Poincare transformation

\begin{align}\label{eqn:txComponents}
{x'}^\mu
&=
{x'}^\mu
+ {{\epsilon}^\mu}_\nu x^\nu
+ {\epsilon}^\mu
\end{align}

It is stated that an antisymmetrization condition $\epsilon_{\mu\nu} = -\epsilon_{\nu\mu}$.  This is somewhat confusing since the infinitesimal transformation is given by a mixed upper and lower index tensor.   Due to the antisymmetry perhaps this all a coordinate statement of the following vector to vector linear transformation 

\begin{align}\label{eqn:linearTxGuess}
x' = x + \epsilon + A \cdot x
\end{align}

This transformation is less restricted than a plain old spacetime transformation, as it also contains a projective term, where $x$ is projected onto the spacetime (or spatial) plane $A$ (a bivector), plus a rotation in that plane.

Writing as usual

\begin{align*}
x = \gamma_\mu x^\mu
\end{align*}

So that components are recovered by taking dot products, as in
\begin{align*}
x^\mu = x \cdot \gamma^\mu
\end{align*}

For the bivector term, write

\begin{align*}
A = c \wedge d = c^\alpha d^\beta (\gamma_\alpha \wedge \gamma_\beta)
\end{align*}

For
\begin{align*}
(A \cdot x ) \cdot \gamma^\mu 
&=
c^\alpha d^\beta x_\sigma ((\gamma_\alpha \wedge \gamma_\beta) \cdot \gamma^\sigma) \cdot \gamma^\mu \\
&=
c^\alpha d^\beta x_\sigma ( {\delta_\alpha}^\mu {\delta_\beta}^\sigma -{\delta_\beta}^\mu {\delta_\alpha}^\sigma ) \\
&=
(c^\mu d^\sigma -c^\sigma d^\mu ) x_\sigma 
\end{align*}

This allows for an identification $\epsilon^{\mu\sigma} = c^\mu d^\sigma -c^\sigma d^\mu$ which is antisymmetric as required.  With that identification we can write (\ref{eqn:txComponents}) via the equivalent vector relation (\ref{eqn:linearTxGuess}) if we write

\begin{align*}
{\epsilon^\mu}_\sigma x^\sigma = (c^\mu d_\sigma -c_\sigma d^\mu ) x^\sigma 
\end{align*}

Where ${\epsilon^\mu}_\sigma$ is defined implicitly in terms of components of the bivector $A = c \wedge d$.

Is this what a Poincare transformation is?  The \href{http://mathworld.wolfram.com/PoincareTransformation.html}{Poincare Transformation} article suggests not.  This article suggests that the Poincare transformation is a spacetime translation plus a Lorentz transformation (composition of boosts and rotations).  That Lorentz transformation will not be antisymmetric however, so how can these be reconciled?  The key is probably the fact that this was an infinitesimal Poincare transformation so lets consider a Taylor expansion of the Lorentz boost or rotation rotor, considering instead a transformation of the following form

\begin{align}\label{eqn:spaceTimeTxAndLor}
x' &= x + \epsilon + R x \tilde{R} \\
R \tilde{R} &= 1
\end{align}

In particular, let's look at the Lorentz transformation in terms of the exponential form
\begin{align*}
R = e^{I \theta/2}
\end{align*}

Here $\theta$ is either the angle of rotation (when the bivector is a unit spatial plane such as $I = \gamma_k \wedge \gamma_m$), or a rapidity angle (when the bivector is a unit spacetime plane such as $I = \gamma_k \wedge \gamma_0$).

Ignoring the translation in (\ref{eqn:spaceTimeTxAndLor}) for now, to calculate the first order term in Taylor series we need

\begin{align*}
\frac{dx'}{d\theta} 
&= 
\frac{dR}{d\theta}  x \tilde{R} 
+
{R} x \frac{d\tilde{R}}{d\theta}  
\\
&= 
\frac{dR}{d\theta} \tilde{R} R x \tilde{R} 
+
{R} x \tilde{R} R \frac{d\tilde{R}}{d\theta}  
\\
&=
\inv{2} ( \Omega x' + x' \tilde{\Omega} ) \\
\end{align*}

where 
\begin{align*}
\inv{2}\Omega = \frac{dR}{d\theta} \tilde{R} 
\end{align*}

Now, what is the grade of the product $\Omega$?  We have both $dR/d\theta$ and $R$ in $\{\bigwedge^0 \oplus \bigwedge^2\}$ so the product can only have even grades $\Omega \in \{\bigwedge^0 \oplus \bigwedge^2 \oplus \bigwedge^4\}$, but the unitary constraint on $R$ restricts this

Since $R \tilde{R} = 1$ the derivative of this is zero

\begin{align*}
\frac{dR}{d\theta} \tilde{R} + {R} \frac{d\tilde{R}}{d\theta}  = 0
\end{align*}

Or
\begin{align*}
\frac{dR}{d\theta} \tilde{R} = - \left( \frac{dR}{d\theta} \tilde{R} \right)^{\tilde{}}
\end{align*}

Antisymmetry rules out grade zero and four terms, leaving only the possibility of grade 2.  That leaves

\begin{align*}
\frac{dx'}{d\theta} = \inv{2}(\Omega x' - x' \Omega) = \Omega \cdot x'
\end{align*}

And the first order Taylor expansion around $\theta =0$ is

\begin{align*}
x'(d\theta) 
&\approx x'(\theta = 0) + ( \Omega d\theta ) \cdot x' \\
&= x + ( \Omega d\theta ) \cdot x'
\end{align*}

This has close to the postulated form in (\ref{eqn:linearTxGuess}), but differs in one notable way.  The dot product with the antisymmetric form $A = \inv{2} \frac{dR}{d\theta} \tilde{R} d\theta$ is a dot product with $x'$ and not $x$!  One can however invert the identity writing $x$ in terms of $x'$ (to first order)

\begin{align*}
x = x' - ( \Omega d\theta ) \cdot x'
\end{align*}

Replaying this argument in fast forward for the inverse transformation should give us a relation for $x'$ in terms of $x$ and the incremental Lorentz transform

\begin{align*}
x' &= R x \tilde{R} \\
\implies \\
x &= \tilde{R} x' {R} \\
\end{align*}

\begin{align*}
\frac{dx}{d\theta}
&= \frac{d \tilde{R}}{d\theta} R \tilde{R} x' R + \tilde{R} x' R \tilde{R} \frac{d {R}}{d\theta}  \\
&= \left(2 \frac{d \tilde{R}}{d\theta} R \right) \cdot x
\end{align*}

So we have our incremental transformation given by

% red box ... ugly.
%\colorbox{red}{\parbox{0.5\textwidth}{
\begin{align}\label{eqn:gotcha}
x'= x - \left(2 \frac{d \tilde{R}}{d\theta} R d\theta \right) \cdot x
\end{align}
%}}

% plain box.  also ugly
%\boxedeqn{
%x'= x - \left(2 \frac{d \tilde{R}}{d\theta} R d\theta \right) \cdot x
%}\label{eqn:gotcha}

\section{Consider a specific infinitesimal spatial rotation.}

The signs and primes involved in arriving at (\ref{eqn:gotcha}) were a bit confusing.  To firm things up a bit considering a specific example is called for.

For a rotation in the $x,y$ plane, we have

\begin{align}
R &= e^{\gamma_1 \gamma_2 \theta/2} \\
x' &= R x \tilde{R}
\end{align}

Here also it is easy to get the signs wrong, and it is worth pointing out the sign convention picked here for the Dirac basis is ${\gamma_0}^2 = -{\gamma_k}^2 = 1$.  To verify that $R$ does the desired job, we have

\begin{align*}
R \gamma_1 \tilde{R}
&=
\gamma_1 \tilde{R^2} \\
&=
\gamma_1 e^{\gamma_2 \gamma_1 \theta} \\
&=
\gamma_1 (\cos\theta + \gamma_2 \gamma_1 \sin\theta) \\
&=
\gamma_1 (\cos\theta - \gamma_1 \gamma_2 \sin\theta) \\
&=
\gamma_1 \cos\theta + \gamma_2 \sin\theta \\
\end{align*}

and 

\begin{align*}
R \gamma_2 \tilde{R}
&=
\gamma_2 \tilde{R^2} \\
&=
\gamma_2 e^{\gamma_2 \gamma_1 \theta} \\
&=
\gamma_2 (\cos\theta + \gamma_2 \gamma_1 \sin\theta) \\
&=
\gamma_2 \cos\theta - \gamma_1 \sin\theta \\
\end{align*}

For $\gamma_3$ or $\gamma_0$, the quaternion $R$ commutes, so we have

\begin{align*}
R \gamma_3 \tilde{R} &= R \tilde{R} \gamma_3 = \gamma_3 \\
R \gamma_0 \tilde{R} &= R \tilde{R} \gamma_0 = \gamma_0 \\
\end{align*}

(leaving the perpendicular basis directions unchanged).

Summarizing the action on the basis vectors in matrix form this is
%1: \gamma_1 \cos\theta + \gamma_2 \sin\theta \\
%2: \gamma_2 \cos\theta - \gamma_1 \sin\theta \\
\begin{align*}
\begin{bmatrix}
\gamma_0 \\
\gamma_1 \\
\gamma_2 \\
\gamma_3 \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos\theta & \sin\theta & 0 \\
0 & -\sin\theta & \cos\theta & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
\gamma_0 \\
\gamma_1 \\
\gamma_2 \\
\gamma_3 \\
\end{bmatrix}
\end{align*}

Observe that the basis vectors transform with the transposed matrix to the coordinates, and we have

\begin{align*}
\gamma_0 x^0
+ \gamma_1 x^1
+ \gamma_2 x^2
+ \gamma_3 x^3 
\rightarrow
\gamma_0 x^0
+x^1 (\gamma_1 \cos\theta + \gamma_2 \sin\theta)
+x^2 (\gamma_2 \cos\theta - \gamma_1 \sin\theta)
+\gamma_3 x^3
\end{align*}

Dotting ${x'}^\mu = x' \cdot \gamma^\mu$ we have

\begin{align*}
x^0 &\rightarrow x^0 \\
x^1 &\rightarrow x^1 \cos\theta - x^2 \sin\theta \\
x^2 &\rightarrow x^1 \sin\theta +x^2 \cos\theta  \\
x^3 &\rightarrow x^3
\end{align*}

In matrix form this is the expected and familiar rotation matrix in coordinate form
\begin{align*}
\begin{bmatrix}
x^0 \\
x^1 \\
x^2 \\
x^3 \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos\theta & -\sin\theta & 0 \\
0 & \sin\theta & \cos\theta & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
x^0 \\
x^1 \\
x^2 \\
x^3 \\
\end{bmatrix}
\end{align*}

Moving on to the initial verification we have

\begin{align*}
2 \frac{d\tilde{R}}{d\theta} 
&= 2\frac{d}{d\theta} e^{\gamma_2\gamma_1 \theta/2} \\
&= \gamma_1 \gamma_2 e^{\gamma_2\gamma_1 \theta/2} \\
\end{align*}

So we have

\begin{align*}
2 \frac{d\tilde{R}}{d\theta} R 
&= \gamma_2 \gamma_1 e^{\gamma_2\gamma_1 \theta/2} e^{\gamma_1\gamma_2 \theta/2} \\
&= \gamma_2 \gamma_1 
\end{align*}

The antisymmetric form $\epsilon_{\mu\nu}$ in this case therefore appears to be nothing more than the unit bivector for the plane of rotation!  We should now be able to verify the incremental transformation result from (\ref{eqn:gotcha}), which is in this specific case now calculated to be

\begin{align}\label{eqn:xyRot}
x'= x + d\theta (\gamma_1 \gamma_2) \cdot x
\end{align}

As a final check let's look at the action of rotation part of the transformation (\ref{eqn:xyRot}) on the coordinates $x^\mu$.  Only the $x^1$ and $x^2$ coordinates need be considered since there is no projection of $\gamma_0$ or $\gamma_3$ components onto the plane $\gamma_1 \gamma_2$.

\begin{align*}
d\theta (\gamma_1 \gamma_2) \cdot (x^1 \gamma_1 + x^2 \gamma_2)
&= 
d\theta \gpgradeone{ \gamma_1 \gamma_2 (x^1 \gamma_1 + x^2 \gamma_2) } \\
&= 
d\theta (\gamma_2 x^1 - \gamma_1 x^2)  \\
\end{align*}

Now compare to the incremental transformation on the coordinates in matrix form.  That is

\begin{align*}
\delta R
&=
d\theta \frac{d}{d\theta}
{
\left.
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos\theta & -\sin\theta & 0 \\
0 & \sin\theta & \cos\theta & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\right\vert}_{\theta=0} \\
&=
d\theta
{
\left.
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & -\sin\theta & -\cos\theta & 0 \\
0 & \cos\theta & -\sin\theta & 0 \\
0 & 0 & 0 & 0 \\
\end{bmatrix}
\right\vert}_{\theta=0} \\
&=
d\theta
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
\end{bmatrix} \\
\end{align*}

So acting on the coordinate vector

\begin{align*}
\delta R &= d\theta
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
\end{bmatrix} 
\begin{bmatrix}
x^0 \\
x^1 \\
x^2 \\
x^3
\end{bmatrix} \\
&=
d\theta
\begin{bmatrix}
0 \\
-x^2 \\
x^1 \\
0
\end{bmatrix} \\
\end{align*}

This is exactly what we got above with the bivector dot product.  Good.

\section{Consider a specific infinitesimal boost.}

For a boost along the $x$ axis we have

\begin{align}
R &= e^{\gamma_0\gamma_1 \alpha/2} \\
x' &= R x \tilde{R}
\end{align}

Verifying, we have

\begin{align*}
x^0 \gamma_0 
&\rightarrow x^0 ( \cosh\alpha + \gamma_0 \gamma_1 \sinh\alpha ) \gamma_0 \\
&= x^0 ( \gamma_0 \cosh\alpha - \gamma_1 \sinh\alpha ) \\
\end{align*}

\begin{align*}
x^1 \gamma_1 
&\rightarrow x^1 ( \cosh\alpha + \gamma_0 \gamma_1 \sinh\alpha ) \gamma_1 \\
&= x^1 ( \gamma_1 \cosh\alpha - \gamma_0 \sinh\alpha ) \\
\end{align*}

Dot products recover the familiar boost matrix
\begin{align*}
\begin{bmatrix}
x^0 \\
x^1 \\
x^2 \\
x^3 \\
\end{bmatrix}'
&=
\begin{bmatrix}
\cosh\alpha & -\sinh\alpha & 0 & 0 \\
-\sinh\alpha & \cosh\alpha & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
x^0 \\
x^1 \\
x^2 \\
x^3 \\
\end{bmatrix}
\end{align*}

Now, how about the incremental transformation given by (\ref{eqn:gotcha}).  A quick calculation shows that we have

\begin{align}\label{eqn:xLor}
x' = x + d\alpha (\gamma_0 \gamma_1) \cdot x
\end{align}

Just like the (\ref{eqn:xyRot}) case for a rotation in the $x y$ plane, the antisymmetric form is again the unit bivector of the rotation plane (this time the unit bivector in the spacetime plane of the boost.)

This completes the examination of two specific incremental Lorentz transformations.  It is clear that the result will be the same for an arbitrarily oriented bivector, and the original guess (\ref{eqn:linearTxGuess}) of a geometric equivalent of tensor relation (\ref{eqn:txComponents}) was correct, provided that $A$ is a unit bivector scaled by the magnitude of the incremental transformation.

The specific case not treated however are those transformations where the orientation of the bivector is allowed to change.  Parameterizing that by angle is not such an obvious procedure.

\section{In tensor form.}

For an arbitrary bivector $A = a \wedge b$, we can calculate ${\epsilon^\sigma}_\alpha$.  That is

\begin{align*}
\epsilon^{\sigma\alpha} x_\alpha
&=
d\theta
\frac
{
((a^\mu \gamma_\mu \wedge b^\nu \gamma_\nu) \cdot ( x_\alpha \gamma^\alpha)) \cdot \gamma^\sigma 
}
{\Abs{((a^\mu \gamma_\mu) \wedge (b^\nu \gamma_\nu)) \cdot ((a_\alpha \gamma^\alpha) \wedge (b_\beta \gamma^\beta))}^{1/2}} 
\\
&=
\frac{ a^\sigma b^\alpha - a^\alpha b^\sigma }{\Abs{a^\mu b^\nu( a_\nu b_\mu - a_\mu b_\nu)}^{1/2}} x_\alpha \\
\end{align*}

So we have

\begin{align*}
{\epsilon^\sigma}_\alpha
&=
d\theta
\frac{ a^\sigma b_\alpha - a_\alpha b^\sigma }{\Abs{a^\mu b^\nu( a_\nu b_\mu - a_\mu b_\nu)}^{1/2}} 
\end{align*}

The denominator can be subsumed into $d\theta$, so the important factor is just the numerator, which encodes an incremental boost or rotational in some arbitrary spacetime or spatial plane (respectively).  The associated antisymmetry can be viewed as a consequence of the bivector nature of the rotor derivative rotor product.

\EndArticle
