%\input{../peeter_prologue_print.tex}
%\input{../peeter_prologue_widescreen.tex}

\chapter{Dirac Notation Ponderings.}
\label{chap:desaiDiracNotes}
%\useCCL
\blogpage{http://sites.google.com/site/peeterjoot/math2010/desaiDiracNotes.pdf}
\date{July 23, 2010}
\revisionInfo{desaiDiracNotes.tex}

%\beginArtWithToc
\beginArtNoToc

\section{Motivation.}

I've got the textbook \cite{desai2009quantum} now for the QM I course I'll be taking in the fall, and have started some light perusing.  Starting things off is the Dirac bra ket notation.  Some aspects of that notation, or the explanation in the text, are not quite obvious to me so here I try to make sense of things.

\section{Dirac Adjoint notes.}

There are a pair of relations given to define the Dirac Adjoint.  These are 1.26 and 1.27 respectively:

\begin{align*}
\left( A \ket{\alpha} \right)^\conj &= \bra{\alpha} A^\dagger \\
{\bra{\beta} A \ket{\alpha}}^\conj &= \bra{\alpha} A^\dagger \ket{\beta}
\end{align*}

Is there some redundancy to these definitions.  Namely is 1.27 a consequence of 1.26?

Since the ket was defined as the conjugate of the bra, we can probably rewrite 1.26 as

\begin{align*}
\bra{\alpha} A^\conj &= \bra{\alpha} A^\dagger 
\end{align*}

Left ``multiplication'', by the ket $\ket{\beta}$ gives

\begin{align*}
(\bra{\alpha} A^\conj) \ket{\beta} &= (\bra{\alpha} A^\dagger) \ket{\beta} \\
{\bra{\beta} (A \ket{\alpha})}^\conj &= (\bra{\alpha} A^\dagger) \ket{\beta} \\
\end{align*}

I've added and retained parenthesis to retain the operational direction.  Is that operational direction not important?  For example, given an operator like $p = -i \hbar \partial_x$, it makes a big difference whether the operator operates to the left or to the right.  In the text, this last relation is equation 1.27 once the parens are dropped, so it does appear that 1.27 is a consequence of 1.26.  This also then seems to imply that in a bra operator ket sandwich, the operator implicitly operates on the ket (to the right), while an adjoint operator implicitly operates on the bra (to the left).

Let's compare this to the simpler and more pedestrian notation found in an old fashioned book like Bohm's \cite{bohm1989qt}.  His expectation values explicitly use an integral definition, and his adjoint definition is very explicit about order of operations.  Namely

\begin{align}\label{eqn:desaiDiracNotes:1}
\int \phi^\conj (A \psi) 
&\equiv \int \psi (A^\dagger \phi^\conj) 
\end{align}

Starting with a concrete definition like this seems a bit easier.  Suppose we also define the bra ket sandwich based on the integral as follows

\begin{align*}
\bra{\phi} A \ket{\psi} 
&\equiv \bra{\phi} (A \ket{\psi}) \\
&\equiv \int \phi^\conj (A \psi) \\
\end{align*}

Now, we can rewrite \ref{eqn:desaiDiracNotes:1}, as 

\begin{align*}
\int \phi^\conj (A \psi)   &\equiv \int \psi (A^\dagger \phi^\conj) \\
&\implies \\
\bra{\phi} (A \ket{\psi})  &= \bra{\psi^\conj} ( A^\dagger \ket{\phi^\conj} ) \\
&\implies \\
\left(\bra{\phi} (A \ket{\psi}) \right)^\conj  &= ( \bra{\phi} A^\dagger ) \ket{\psi}
\end{align*}

When starting off with the integral we see the notational requirement for non-adjoint operators to operate implicitly to the right, and the adjoint operators to operate implicitly to the left.  With that notation requirement we can drop the parens and recover 1.27.

A couple clarification goals are now complete.  The first is seeing how equation 1.26 in the text implies 1.27.  We also have reconciled the Dirac notation with the familiar integral inner product notation, and seen two different ways that clarify the implicit operator directionality in the bra operator ket sandwiches.

\section{Chapter 1 problems.}

\subsection{P1.}

With
\begin{align*}
\ket{\alpha_1} &\equiv 
\begin{bmatrix}
1 \\
0
\end{bmatrix} \\
\ket{\alpha_2} &\equiv 
\begin{bmatrix}
0 \\
1
\end{bmatrix} \\
\bra{\alpha_1} &\equiv 
\begin{bmatrix}
1 & 0
\end{bmatrix} \\
\bra{\alpha_2} &\equiv 
\begin{bmatrix}
0 & 1
\end{bmatrix}
\end{align*}

\subsubsection{P1.i Orthonormal.}

Straight multiplication is sufficient to show this and we get

\begin{align*}
\braket{\alpha_1}{\alpha_1} &= 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
\begin{bmatrix}
1 \\
0
\end{bmatrix} =
\begin{bmatrix}
1
\end{bmatrix} \\
\braket{\alpha_2}{\alpha_2} &= 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
\begin{bmatrix}
0 \\
1
\end{bmatrix} =
\begin{bmatrix}
1
\end{bmatrix} \\
\braket{\alpha_1}{\alpha_2} &= 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
\begin{bmatrix}
0 \\
1
\end{bmatrix} =
\begin{bmatrix}
0
\end{bmatrix} \\
\braket{\alpha_2}{\alpha_1} &= 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
\begin{bmatrix}
1 \\
0
\end{bmatrix} =
\begin{bmatrix}
0
\end{bmatrix} \\
\end{align*}

\subsubsection{P1.ii Linear combinations for state vectors}

\begin{align*}
\begin{bmatrix}
a \\
b
\end{bmatrix} &= 
a \ket{\alpha_1}
+b \ket{\alpha_2}
\end{align*}

\subsubsection{P1.iii Outer products.}

We have

\begin{align*}
\ket{\alpha_1}\bra{\alpha_2} &=
\begin{bmatrix}
1 \\
0
\end{bmatrix} 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
=
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix} \\
\ket{\alpha_2}\bra{\alpha_1} &=
\begin{bmatrix}
0 \\
1
\end{bmatrix} 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
=
\begin{bmatrix}
0 & 0 \\
1 & 0
\end{bmatrix} \\
\ket{\alpha_1}\bra{\alpha_1} &=
\begin{bmatrix}
1 \\
0
\end{bmatrix} 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
=
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix} \\
\ket{\alpha_2}\bra{\alpha_2} &=
\begin{bmatrix}
0 \\
1
\end{bmatrix} 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
=
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} \\
\end{align*}

\subsubsection{P1.iv Completeness relation.}

From the above outer products, summation over just the diagonal terms we have

\begin{align*}
\ket{\alpha_1}\bra{\alpha_1} + \ket{\alpha_2}\bra{\alpha_2} &=
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix} +
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} 
= I
\end{align*}

\subsubsection{P1.v Arbitrary matrix as sum of outer products.}

By inspection

\begin{align*}
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} 
&=
a \ket{\alpha_1}\bra{\alpha_1} 
+b \ket{\alpha_1}\bra{\alpha_2} 
+c \ket{\alpha_2}\bra{\alpha_1} 
+d \ket{\alpha_2}\bra{\alpha_2}
\end{align*}

\subsubsection{P1.vi Spin matrix}

Given

\begin{align*}
A \ket{\alpha_1} &= + \ket{\alpha_1} \\
A \ket{\alpha_2} &= - \ket{\alpha_1}
\end{align*}

Our matrix elements are
\begin{align*}
\bra{\alpha_1} A \ket{\alpha_1} &= 1 \\
\bra{\alpha_2} A \ket{\alpha_1} &= 0 \\
\bra{\alpha_1} A \ket{\alpha_2} &= 0 \\
\bra{\alpha_2} A \ket{\alpha_2} &= -1
\end{align*}

Thus the matrix representation of the operator $A$ with respect to basis $\{\alpha_1, \alpha_2\}$ is

\begin{align*}
\{A\} =
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix} 
\end{align*}

\subsection{P2. Derivative of inverse operator}

We take derivatives of the identity operator, giving

\begin{align*}
0 
&= \frac{dI}{d\lambda} \\
&= \frac{d (A A^{-1})}{d\lambda} \\
&= \frac{d A }{d\lambda} A^{-1} + A \frac{d A^{-1}}{d\lambda} \\
\end{align*}

left multiplication by $A^{-1}$ and rearranging we have

\begin{align*}
\frac{d A^{-1}}{d\lambda} 
&= -A^{-1} \frac{d A }{d\lambda} A^{-1} 
\end{align*}

as desired.

\subsection{P3. Unitary representations.}

Show that a unitary operator $U$ can be written

\begin{align*}
U = \frac{1 + iK}{1-iK},
\end{align*}

where $K$ is a Hermitian operator.

\subsubsection{A commutation assumption for the numerator and denominator.}

Before tackling the problem, note that with the fraction written this way, and not as

\begin{align}\label{eqn:desaiDiracNotes:2}
U = (1 + iK)\frac{1}{1-iK},
\end{align}

or
\begin{align*}
U = \frac{1}{1-iK}(1 + iK),
\end{align*}

there appears to be an implicit assumption that the numerator and denominator commute.  How can that be justified?

Suppose that the denominator can be expanded in Taylor series

\begin{align*}
\frac{1}{1-iK} = 1 + iK + (iK)^2 + (iK)^3 + \cdots
\end{align*}

If this converges, this series does in fact commute with the numerator since both are polynomials in $K$.  Another way of looking at this would be to apply a spectral decomposition to the operators (assumed to be matrices now) where using $K = V \Sigma V^\dagger$ for a unitary $V$ and diagonal $\Sigma$, we can write

\begin{align*}
U = \frac{1}{1-iK}(1 + iK) = \frac{1}{1-i \Sigma}(1 + i \Sigma)
\end{align*}

Both the numerator and denominator are now diagonal and thus commute.  Generalizing either of these commutation justifications to infinite dimensional Hilbert operators or where that inverse power series in $K$ does not converge would take further thought.

\subsubsection{That this representation is unitary.}

From \ref{eqn:desaiDiracNotes:2} we have

\begin{align*}
U U^\dagger 
&= (1 + iK)\frac{1}{1-iK} \frac{1}{1+iK} (1 - iK) \\
&= \frac{1 + K^2}{1+K^2} \\
&= 1
\end{align*}

So this operator is unitary for all Hermitian $K$.  However, is there a $K$ for any unitary $U$ that is Hermitian and for which this identity holds true?  We can rearrange for $K$ to get

\begin{align}\label{eqn:desaiDiracNotes:3}
K = i \frac{ U - 1 }{U + 1}
\end{align}

Is this Hermitian?  If so then $K - K^\dagger = 0$, so let's evaluate that.

\begin{align*}
K - K^\dagger = i \frac{ U - 1 }{U + 1} + i \frac{ U^\dagger - 1 }{U^\dagger + 1}
\end{align*}

Multiplying by $-i(U+1)(U^\dagger+1)$ we have

\begin{align*}
-i(U+1)(U^\dagger+1)(K - K^\dagger) 
&= ( U - 1 )(U^\dagger + 1) + ( U^\dagger - 1 )(U + 1) \\
&= U U^\dagger - 1 - U^\dagger + U     + U^\dagger U - U + U^\dagger - 1 \\
&= 0.
\end{align*}

Therefore, provided $2 + U + U^\dagger \ne 0$ (if it does we only showed that $0 = 0$), the operator $K$ is Hermitian.  The expression \ref{eqn:desaiDiracNotes:3} then allows any unitary operator to be expressed as the fraction \ref{eqn:desaiDiracNotes:2}.

\subsubsection{An exponential representation.}

Show that one can also write

\begin{align*}
U = e^{i C},
\end{align*}

where $C$ is Hermitian.  Utilizing the power series we have

\begin{align*}
(e^{iC})^\dagger
&= 
\sum_{k=0}^\infty \inv{k!} ((iC)^k)^\dagger \\
&= 
\sum_{k=0}^\infty \inv{k!} ((-iC)^k) \\
&= 
e^{-iC}.
\end{align*}

The operators $i C$ and $-i C$ commute, so we can write

\begin{align*}
(e^{iC})^\dagger e^{iC} = e^{ -iC + iC } = 1,
\end{align*}

which shows that this exponential construction is in fact unitary for any Hermitian $C$.  The remainder of the exercise requires a demonstration that we can find such an operator $C$ for any given unitary operator $U$.  Rearranging, we have

\begin{align}\label{eqn:desaiDiracNotes:10}
C = -i \ln ( U ).
\end{align}

How can we give this some meaning?  One way, with the presumption that working with the matrix representation of the operator is allowable, is to utilize the spectral theorem for normal matrices.  Normal here means that the matrix and its Hermitian conjugate commute, which is implied by $U U^\dagger = 1 = U^\dagger U$.  So we can write, for a diagonal matrix $\Sigma$, and a unitary matrix $V$, 
\begin{align*}
U = V \Sigma V^\dagger,
\end{align*}

so the logarithm of \ref{eqn:desaiDiracNotes:10} can be reduced, and we are left with

\begin{align}\label{eqn:desaiDiracNotes:11}
C = -i V \ln ( \Sigma ) V^\dagger.
\end{align}

Here the logarithm of the diagonal matrix is nothing more than the diagonal matrix of the eigenvalues.

We still have to show that $C$ as defined in \ref{eqn:desaiDiracNotes:11} is Hermitian.  At a glance it looks like this may be anti Hermitian $C^\dagger = -C$, but we really need a characterization of the eigenvalues to say.  That conjugate is

\begin{align}\label{eqn:desaiDiracNotes:11b}
C^\dagger = i V (\ln ( \Sigma ))^\dagger V^\dagger.
\end{align}

It seems worthwhile to work an example to see if we are even on the right track.  Let's pick the 2 dimensional rotation matrix, and express it using its eigenvalue decomposition.  That is

\begin{align}\label{eqn:desaiDiracNotes:12}
U &= 
\begin{bmatrix}
\cos\theta & \sin\theta \\
-\sin\theta & \cos\theta
\end{bmatrix},
\end{align}

with decomposition

\begin{align}\label{eqn:desaiDiracNotes:12b}
V &= \inv{\sqrt{2}} 
\begin{bmatrix}
1 & 1 \\
i & -i \\
\end{bmatrix} \\
\Sigma &= 
\begin{bmatrix}
e^{i\theta} & 0 \\
0 & e^{-i\theta} 
\end{bmatrix} \\
U &= V \Sigma V^\dagger.
\end{align}

We have

\begin{align*}
\ln \Sigma &=
\begin{bmatrix}
i\theta & 0 \\
0 & -i\theta 
\end{bmatrix}.
\end{align*}

Ah.  This is purely imaginary, and accounts for the Hermiticity of $C$ in this specific example.  Are the logs of the eigenvalues of unitary matrices all purely imaginary?  That seems like a lot to ask for.

Incidentally, for this example, $C = -i V \ln \Sigma V^\dagger$ gives us

\begin{align*}
C &= i \theta 
\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix} \\
&= \theta \sigma_2,
\end{align*}

So, in a rather neat way, we have an matrix exponential expression for the standard planar rotation matrix, in terms of one of the Pauli matrices.  It is straight forward to verify that 

\begin{align}\label{eqn:desaiDiracNotes:13}
U = e^{i \sigma_2 \theta},
\end{align}

does in fact recover \ref{eqn:desaiDiracNotes:12}.  This follows directly from $(i \sigma_2)^2 = -I$, allowing us to write

\begin{align}\label{eqn:desaiDiracNotes:14}
U = \cos\theta I + i \sigma_2 \sin\theta.
\end{align}

Okay.  With that example worked out, we come to the conclusion that the operator specified in \ref{eqn:desaiDiracNotes:11}, can be Hermitian.

Having worked an example, we are left to prove the more general case.  To do this we have only to note that the eigenvalues of a Unitary matrix have unit norm, so they must all be of the form $e^{i\alpha}$.  Suppose we write for the diagonal matrix 

\begin{align}\label{eqn:desaiDiracNotes:11c}
\Sigma = 
{\begin{bmatrix}
e^{i\alpha_k} \delta_{kj}
\end{bmatrix}}_k.
\end{align}

The logarithm and it's conjugate are then

\begin{align}\label{eqn:desaiDiracNotes:11d}
\ln \Sigma &= 
{\begin{bmatrix}
i\alpha_k \delta_{kj}
\end{bmatrix}}_k \\
(\ln \Sigma)^\dagger &= -\ln \Sigma.
\end{align}

This completes the required proof, showing that the matrix $C$ is Hermitian

\begin{align}\label{eqn:desaiDiracNotes:11e}
C = -i V \ln ( \Sigma ) V^\dagger = C^\dagger.
\end{align}

I initially relied on wikipedia \cite{wiki:unitary} for the hint that Unitary matrices have unit norm eigenvalues (and the wiki article references Shankar, which I don't have).  However, this is straightforward to show.  Suppose that $x$ is an eigenvector for $U$ with eigenvalue $\lambda$, then we have

\begin{align*}
\braket{U x}{U x}
&= \braket{U^\dagger U x}{x} \\
&= \braket{x}{x},
\end{align*}

but we also have

\begin{align*}
\braket{U x}{U x}
&= \braket{\lambda x}{ \lambda x} \\
&= \Abs{\lambda}^2 \braket{x}{x}.
\end{align*}

We must then have $\Abs{\lambda}^2 = 1$, or $\lambda = e^{i\alpha}$ for some real $\alpha$.

\subsubsection{Commuting real and imaginary parts.}

If 
\begin{align}\label{eqn:desaiDiracNotes:4}
U = A + iB,
\end{align}

show that $A$ and $B$ commute.

We can form the matrices $A$, and $B$ with the usual real and imaginary decomposition, but using Hermitian conjugation.  That is
\begin{align}\label{eqn:desaiDiracNotes:5}
A &=
\inv{2} ( U + U^\dagger ) \\
B &=
\inv{2i} ( U - U^\dagger ).
\end{align}

Then the commutation question essentially just requires that we show the commutator is zero

\begin{align*}
A B - B A 
&=
\inv{4i}\left( 
(U + U^\dagger) (U - U^\dagger)
- (U - U^\dagger) (U + U^\dagger)
\right) \\
&=
\inv{4i}\left( 
U^2 + (U^\dagger)^2 + 1 - 1
- (U^2 + (U^\dagger)^2 - 1 + 1 )
\right) \\
&= 0.\qquad \square
\end{align*}

Now, if $U = e^{iC} = A + iB$, we can expand $U$ trigonometrically, with the typical power series expansions, and can also write

\begin{align*}
A &= \cos C \\
B &= \sin C 
\end{align*}

We can also use the spectral decomposition of $U$ and $C$ above in \ref{eqn:desaiDiracNotes:11}, to write

\begin{align*}
A &= V \cosh(\ln \Sigma) V^\dagger \\
B &= -V \sinh(\ln \Sigma) V^\dagger,
\end{align*}

and again here the functions of matrices are nothing more than diagonal evaluation of the respective functions to each of the eigenvalues of $\Sigma$.

\subsection{P4. Determinant of exponential in terms of trace.}

Show 

\begin{align*}
\det (e^A) = e^{\tr A}.
\end{align*}

The problem doesn't put constraints (ie: no statement that $A$ is Hermitian), so we can't assume a Unitary diagonalization is possible.  We can however assume an upper triangular similarity transformation of the form

\begin{align*}
A = W J W^{-1},
\end{align*}

where $W$ is invertible, but not necessarily unitary, and $J$ is in Jordan Canonical form.  That form is upper triangular with the eigenvalues on the diagonal, and only ones or zeros above the diagonal (however, for the purposes of this problem we only need to know that it is upper triangular).

The determinant of $e^A$ is then

\begin{align*}
\det(e^A) 
&=
\det(W) \det(e^J) \det(W^{-1}) \\
&=
\det(e^J).
\end{align*}

Note that the exponential of a triangular matrix has the exponentials of the eigenvalues along the diagonal.  We can see this by computing the square of an upper triangular matrix in block form.  A general proof of this is straightforward, but one gets the idea by considering the two by two case

\begin{align*}
\begin{bmatrix}
a & c \\
0 & b
\end{bmatrix}
\begin{bmatrix}
a & c \\
0 & b
\end{bmatrix}
&=
\begin{bmatrix}
a^2 & (a + b)c \\
0 & b^2
\end{bmatrix}.
\end{align*}

Forming the exponential series, one is left with exponentials of the eigenvalues along the diagonal.  So we have for our determinant

\begin{align*}
\det(e^A) 
&=
\det(e^J) \\
&=
\Pi_k e^{\lambda_k} \\
&=
e^{\sum_k \lambda_k} \\
&=
e^{\tr(A)} \qquad \square.
\end{align*}

\subsection{P5. Trace of an outer product operator.}

Show that 

\begin{align*}
\tr( \ket{\alpha}\bra{\beta} ) = \braket{\beta}{\alpha}.
\end{align*}

Let $A = \ket{\alpha}\bra{\beta}$, and introduce a complete basis $\ket{e_k}$.  The trace with respect to this basis (or any) is thus

\begin{align*}
\tr(A) 
&= \sum_k \bra{e_k} A \ket{e_k} \\
&= \sum_k \bra{e_k} \left( \ket{\alpha} \bra{\beta} \right) \ket{e_k} \\
&= \sum_k \braket{\beta} {e_k} \braket{e_k} {\alpha} \\
&= \bra{\beta} \left( \sum_k \ket{e_k} \bra{e_k} \right) \ket{\alpha} \\
&= \bra{\beta} I \ket{\alpha} \\
&= \braket{\beta} {\alpha} \qquad \square.
\end{align*}

\subsection{P6. eigen calculation.}

For operator(s)

\begin{align}\label{eqn:desaiDiracNotes:60}
A = 
\ket{\alpha}\bra{\alpha}
+ \lambda \ket{\beta}\bra{\alpha}
+ \lambda^\conj \ket{\alpha}\bra{\beta}
\pm \ket{\beta}\bra{\beta},
\end{align}

where $\braket{\alpha}{\beta} = 0$, and $\braket{\alpha}{\alpha} = \braket{\beta}{\beta} = 1$, find the eigenvalues and vectors for (i) $\lambda = 1$, and (ii) $\lambda = i$.

\subsubsection{Without using matrix representation.}
Our eigenvector must be some linear combination of the two kets, so lets look for one of the form $\ket{e} = \ket{\alpha} + a\ket{\beta}$, and use this to find eigenvalues for

\begin{align}\label{eqn:desaiDiracNotes:61}
A \ket{e} = b \ket{e}.
\end{align}

This means we seek solutions to

\begin{align}\label{eqn:desaiDiracNotes:62}
\ket{\alpha}
+ \lambda \ket{\beta}
+ a \lambda^\conj \ket{\alpha}
\pm a \ket{\beta}
= b ( \ket{\alpha} + a \ket{\beta} ).
\end{align}

This supplies a pair of simultaneous equations
\begin{align}\label{eqn:desaiDiracNotes:63}
1 + a \lambda^\conj &= b \\
\lambda \pm a &= b a.
\end{align}

We have our eigenvalue $b$ in terms of the constant $a$ immediately, so for $a$ we wish to solve the quadratic

\begin{align}\label{eqn:desaiDiracNotes:64}
\lambda \pm a &= (1 + a \lambda^\conj ) a
\end{align}

Let's treat these four cases separately, starting the two $\lambda = 1$ operators.  Those quadratics are

\begin{align}\label{eqn:desaiDiracNotes:65}
1 + a &= (1 + a ) a \\
1 - a &= (1 + a ) a
\end{align}

with respective solutions

\begin{align}\label{eqn:desaiDiracNotes:66}
a &= \pm 1 \\
a &= \pm \sqrt{2} - 1
\end{align}

Summarizing the operator, eigenvalue, and eigenvector triplets for this $\lambda = 1$ case we have

\begin{subequations}
\label{eqn:desaiDiracNotes:67}
\begin{align}
A &=
\ket{\alpha}\bra{\alpha}
+ \ket{\beta}\bra{\alpha}
+ \ket{\alpha}\bra{\beta}
+ \ket{\beta}\bra{\beta} \\
\ket{e}_{\pm} &= \ket{\alpha} \pm \ket{\beta} \\
\lambda_{\pm} &= 1 \pm 1 
\end{align}
\end{subequations}

and

\begin{subequations}
\label{eqn:desaiDiracNotes:68}
\begin{align}
A &=
\ket{\alpha}\bra{\alpha}
+ \ket{\beta}\bra{\alpha}
+ \ket{\alpha}\bra{\beta}
- \ket{\beta}\bra{\beta} \\
\ket{e}_{\pm} &= \ket{\alpha} + (\pm \sqrt{2} - 1) \ket{\beta} \\
\lambda_{\pm} &= \pm \sqrt{2}
\end{align}
\end{subequations}

Now for the pair of $\lambda = i$ operators, our quadratic is

\begin{align}\label{eqn:desaiDiracNotes:64i}
i \pm a &= (1 - i a) a,
\end{align}

or separately
\begin{align}\label{eqn:desaiDiracNotes:64ii}
a^2 + 1 &= 0 \\
%a^2 + 2 a i + 1 &= 0 
(a + i)^2 + 2 &= 0
\end{align}

The respective solutions are

\begin{align}\label{eqn:desaiDiracNotes:66i}
a &= \pm i \\
a &= i (-1 \pm \sqrt{2} ),
\end{align}

with eigenvalues $b = 1 - i a$, which are respectively

\begin{align}\label{eqn:desaiDiracNotes:66j}
b &= 1 \pm 1 \\
b &= \pm \sqrt{2}.
\end{align}

Summarizing the results, we have

\begin{subequations}
\label{eqn:desaiDiracNotes:67ip}
\begin{align}
A &=
\ket{\alpha}\bra{\alpha}
+ i \ket{\beta}\bra{\alpha}
- i \ket{\alpha}\bra{\beta}
+ \ket{\beta}\bra{\beta} \\
\ket{e}_{\pm} &= \ket{\alpha} \pm i \ket{\beta} \\
\lambda_{\pm} &= 2, 0
\end{align}
\end{subequations}

and
\begin{subequations}
\label{eqn:desaiDiracNotes:67in}
\begin{align}
A &=
\ket{\alpha}\bra{\alpha}
+ i \ket{\beta}\bra{\alpha}
- i \ket{\alpha}\bra{\beta}
+ \ket{\beta}\bra{\beta} \\
\ket{e}_{\pm} &= \ket{\alpha} + i (-1 \pm \sqrt{2} ) \ket{\beta} \\
\lambda_{\pm} &= \pm \sqrt{2}
\end{align}
\end{subequations}

So it appears we got the same eigenvalues and vectors for both $\lambda = 1$ and $\lambda = i$.  Is there a higher order principle that this follows from?  Perhaps the fact that both terms with $\lambda$ coefficients were conjugate pairs?  That's something perhaps worth thinking about.

\subsubsection{Using matrix representation.}

In the matrix notation with basis $\{\sigma_1, \sigma_2\} = \{(1,0), (0,1)\}$, and $A_{mn} = \bra{\sigma_m} A \ket{\sigma_n}$, we have

\begin{align*}%\label{eqn:desaiDiracNotes:70}
A_{11} &= \bra{\sigma_1} A \ket{\sigma_1} = \braket{\alpha}{\alpha} = 1 \\
A_{22} &= \bra{\sigma_2} A \ket{\sigma_2} = \mu \braket{\beta}{\beta} = \mu \\
A_{12} &= \bra{\sigma_1} A \ket{\sigma_2} = \lambda^\conj \\
A_{21} &= \bra{\sigma_2} A \ket{\sigma_1} = \lambda
\end{align*}

Or in whole matrix notation, we have

\begin{align}\label{eqn:desaiDiracNotes:71}
\{ A \} = 
\begin{bmatrix}
1 & \lambda^\conj \\
\lambda & \mu
\end{bmatrix}.
\end{align}

Finding the eigenvalues and vectors becomes a straightforward, albeit somewhat tedious, algebraic job, solving for $\Abs{ A - \sigma I } = 0$, for eigenvalues $\sigma$.  Doing this, I get

\begin{itemize}
\item $\lambda = 1$, $\mu = 1$

\begin{align}\label{eqn:desaiDiracNotes:72}
\sigma &= 2,0 \\
\ket{\sigma_2} &= \inv{\sqrt{2}} (1,1) \\
\ket{\sigma_0} &= \inv{\sqrt{2}} (1,-1) 
\end{align}

Alternatively, for the $\sigma=2$ case we have

\begin{align}\label{eqn:desaiDiracNotes:72i}
\ket{\sigma_2} = \inv{\sqrt{2}}\left( \ket{\alpha} + \ket{\beta} \right),
\end{align}

and for the $\sigma=0$ case we have

\begin{align}\label{eqn:desaiDiracNotes:72ii}
\ket{\sigma_0} = \inv{\sqrt{2}}\left( \ket{\alpha} - \ket{\beta} \right).
\end{align}

Ignoring the normalization constant used here, this is consistent with \ref{eqn:desaiDiracNotes:67} as it should be.

\item $\lambda = 1$, $\mu = -1$

\begin{align}\label{eqn:desaiDiracNotes:73}
\sigma &= \pm \sqrt{2} \\
\ket{\sigma_{\pm}} &\propto (1,-1 \pm \sqrt{2}) 
\end{align}

Normalization wasn't bothered with this time due to pesky $\sqrt{2}$ terms.  The eigenstates expressed in terms of the original basis vectors are 

\begin{align}\label{eqn:desaiDiracNotes:73i}
\ket{\sigma_{\pm}} = \ket{\alpha} + (-1 \pm \sqrt{2}) \ket{\beta}
\end{align}

This is consistent with \ref{eqn:desaiDiracNotes:68} as expected.

\item $\lambda = i$, $\mu = 1$

\begin{align}\label{eqn:desaiDiracNotes:74}
\sigma &= 2,0 \\
\ket{\sigma_0} &= \inv{\sqrt{2}} (i,1) \\
\ket{\sigma_2} &= \inv{\sqrt{2}} (-i,1) 
\end{align}

In terms of the original basis vectors this is 
\begin{align}\label{eqn:desaiDiracNotes:74i}
\ket{\sigma_0} = \inv{\sqrt{2}}\left( i\ket{\alpha} + \ket{\beta} \right)
\end{align}

\begin{align}\label{eqn:desaiDiracNotes:74ii}
\ket{\sigma_2} = \inv{\sqrt{2}}\left( -i\ket{\alpha} + \ket{\beta} \right)
\end{align}

Checking against \ref{eqn:desaiDiracNotes:67ip} shows that $\ket{\sigma_2}$ above only differs by a constant as expected.

\item $\lambda = i$, $\mu = -1$

\begin{align}\label{eqn:desaiDiracNotes:75}
\sigma &= \pm \sqrt{2} \\
\ket{\sigma_{\pm}} &\propto (1, -i(1 \mp \sqrt{2}))
\end{align}

Or, in terms of the original basis,

\begin{align}\label{eqn:desaiDiracNotes:75i}
\ket{\sigma_{\pm}} = \ket{\alpha} + i (-1 \pm \sqrt{2}) \ket{\beta}.
\end{align}

This matches the previous calculation summarized by \ref{eqn:desaiDiracNotes:67in}.

\end{itemize}

\EndArticle
