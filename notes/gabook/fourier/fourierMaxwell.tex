%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

% 
% 
\chapter{Fourier transform solutions to Maxwell's equation}\label{chap:PJfourierMaxwellSecondOrder}
%\date{Jan 29, 2009.  fourierMaxwell.tex}

\section{Motivation}

In \chapcite{PJwaveFourier} a Green's function solution to the homogeneous wave equation

\begin{align}\label{eqn:fourier_maxwell:wave}
\left(\inv{v^2} \partial_{tt} -\partial_{xx} -\partial_{yy} -\partial_{zz} \right)\psi = 0
\end{align}

was found to be

\begin{align}\label{eqn:fourier_maxwell:greensSolution3d}
{\psi}(x,y,z,t) &= \IIinf \phi(u,w,r) G( x-u, y-w, z-r, t) du d\tau dr \\
G(x,y,z,t) &= \inv{({2\pi})^3} \IIinf \exp\left( i k x + i m y + i n z \pm i \sqrt{k^2 + m^2 + n^2} v t \right) dk dm dn
\end{align}

The aim of this set of notes is to explore the same ideas to the forced wave equations for the four vector potentials of the Lorentz gauge Maxwell equation.

Such solutions can be used to find the Faraday bivector or its associated tensor components.

Note that the specific form of the Fourier transform used in these notes continues to be

\begin{align}
%\hat{f}(\Bk) &= \inv{(\sqrt{2\pi})^n} \IIinf f(\Bx) \exp\left( -i k_j x^j \right) d^n x \\
%{f}(\Bx) &= \inv{(\sqrt{2\pi})^n} \IIinf \hat{f}(\Bk) \exp\left( i k_j x^j \right) d^n k
\hat{f}(\Bk) &= \inv{(\sqrt{2\pi})^n} \IIinf f(\Bx) \exp\left( -i \Bk \cdot \Bx \right) d^n x \\
{f}(\Bx) &= \inv{(\sqrt{2\pi})^n} \IIinf \hat{f}(\Bk) \exp\left( i \Bk \cdot \Bx \right) d^n k
\end{align}

\section{Forced wave equation}

\subsection{One dimensional case}

A good starting point is the reduced complexity one dimensional forced wave equation

\begin{align}
\left( \inv{v^2} \partial_{tt} -\partial_{xx} \right)\psi = g
\end{align}

Fourier transforming to to the wave number domain, with application of integration by parts twice (each toggling the sign of the spatial derivative term) we have

\begin{align}
\inv{v^2}\hat{\psi}_{tt} - (-i k)^2 \hat{\psi} = \hat{g}
\end{align}

This leaves us with a linear differential equation of the following form to solve

\begin{align}\label{eqn:fourier_maxwell:waveNumEquationToSolve}
f'' + \alpha^2 f = h
\end{align}

Out of line solution of this can be found below in equation \ref{eqn:fourier_maxwell:solutionToWaveNumberDomainEquation}, where we have $f = \hat{\psi}$, $\alpha = k v$, and $h = \hat{g} v^2$.  Our solution for the wave function in the wave number domain is now completely
specified

\begin{align*}
\hat{\psi}(k, t) = \Abs{\frac{v}{k}} \int_{u=t_0(k)}^t \hat{g}(u) \sin( \Abs{k v} (t-u) ) du
\end{align*}

Here because of the partial differentiation we have the flexibility to make the initial time a function of the wave number $k$, but it is probably more natural to just set $t_0 = -\infty$.  Also let us explicitly pick $v > 0$ so that absolutes are only required on the factors of $k$

\begin{align*}
\hat{\psi}(k, t) = \frac{v}{\Abs{k}} \int_{u = -\infty}^t \hat{g}(k, u) \sin( \Abs{k} v (t-u) ) du
\end{align*}

But seeing the integral in this form suggests a change of variables $\tau = t-u$, which gives us our final wave function in the wave number domain with all the time
dependency removed from the integration limits

\begin{align*}
\hat{\psi}(k, t) = \frac{v}{\Abs{k}} \int_{\tau = 0}^\infty \hat{g}(k, t-\tau) \sin( \Abs{k} v \tau ) d\tau
\end{align*}

With this our wave function is

\begin{align*}
{\psi}(x, t)
&=
\inv{\sqrt{2\pi}} \IIinf
\left(
\frac{v}{k} \int_{\tau = 0}^\infty \hat{g}(k, t-\tau) \sin( \Abs{k} v \tau ) d\tau
\right) \exp( i k x ) dk \\
\end{align*}


But we also have

\begin{align*}
\hat{g}(k,t) &= \inv{\sqrt{2\pi}} \int_{-\infty}^\infty {g}(x, t) \exp( -i k x ) dx
\end{align*}

Reassembling we have

\begin{align*}
{\psi}(x, t)
&= 
\int_{k = -\infty}^\infty
\int_{\tau = 0}^\infty
\int_{y=-\infty}^\infty
\frac{v}{ 2 \pi \Abs{k}}
{g}(y, t-\tau) 
\sin( \Abs{k} v \tau ) 
\exp( i k (x-y) ) 
dy
d\tau
dk
\end{align*}

Rearranging a bit, and noting that $\sinc(\Abs{k}x) = \sinc(kx)$ we have

\begin{align}\label{eqn:fourier_maxwell:oneDimResult}
{\psi}(x, t)
&=
\int_{x'=-\infty}^\infty
\int_{t' = 0}^\infty {g}(x-x', t-t') G(x', t') dx' dt' \\
G(x, t) &=
\int_{k = -\infty}^\infty
\frac{v}{2\pi {k}}
\sin( {k} v t )
\exp( i k x )
dk
\end{align}

We see that our charge density summed over all space contributes to the wave function, but it is the charge density at that spatial location as it existed at a specific previous time.

The Green's function that we convolve with in \ref{eqn:fourier_maxwell:oneDimResult} is a rather complex looking function.  As seen later in \chapcite{PJpoisson} it was possible to evaluate a 3D variant of such an integral in ad-hoc methods to produce a form in terms of retarded time and advanced time delta functions.  A similar reduction, also in \chapcite{PJpoisson}, of the Green's function above yields a unit step function identification

\begin{align}
G(x, t) &= \frac{v}{2} \left(\theta(x + v t )  - \theta(x - v t) \right) 
\end{align}

(This has to be verified more closely to see if it works).

\subsection{Three dimensional case}

Now, lets move on to the 3D case that is of particular interest for electrodynamics.  Our wave equation is now of the form

\begin{align}
\left( \inv{v^2} \PDSq{t}{} -\sum_j \PDSq{x^j}{} \right)\psi = g
\end{align}

and our Fourier transformation produces almost the same result, but we have a wave number contribution from each of the three dimensions

\begin{align}
\inv{v^2}\hat{\psi}_{tt} + \Bk^2 \hat{\psi} = \hat{g}
\end{align}

Our wave number domain solution is therefore
\begin{align}\label{eqn:fourier_maxwell:waveNumDomainSolution}
\hat{\psi}(\Bk, t) = \frac{v}{\Abs{\Bk}} \int_{\tau = 0}^\infty \hat{g}(\Bk, t-\tau) \sin( \Abs{\Bk} v \tau ) d\tau
\end{align}

But our wave number domain charge density is

\begin{align*}
\hat{g}(\Bk, t) &= \inv{(\sqrt{2\pi})^3} \IIinf g(\Bx, t) \exp\left( -i \Bk \cdot \Bx \right) d^3 x \\
\end{align*}

Our wave number domain result in terms of the charge density is therefore

%\hat{g}(\Bk, t-\tau) &= \inv{(\sqrt{2\pi})^3} \IIinf g(\Br, t-\tau) \exp\left( -i \Bk \cdot \Br \right) d^3 r \\
\begin{align*}
\hat{\psi}(\Bk, t) =
\frac{v}{\Abs{\Bk}} \int_{\tau = 0}^\infty
%\hat{g}(\Bk, t-\tau) &=
\left(
\inv{(\sqrt{2\pi})^3} \IIinf g(\Br, t-\tau) \exp\left( -i \Bk \cdot \Br \right) d^3 r
\right)
\sin( \Abs{\Bk} v \tau ) d\tau
\end{align*}

And finally inverse transforming back to the spatial domain we have a complete solution for the inhomogeneous wave equation in terms of the spatial and temporal charge density distribution

\begin{align}
{\psi}(\Bx, t)
%&= \IIinf \int_{t' = 0}^\infty g(\Br, t-t') G(\Bx -\Br) d^3 r dt' \\
% x'_j = x_j - r_j
% r_j = x_j - x'_j
% dr_j = -dx'_j
% III dr_1 dr_2 dr_3 = (-1)^3 III d^3 x'
&= \IIinf \int_{t' = 0}^\infty g(\Bx -\Bx', t-t') G(\Bx', t') d^3 x' dt' \\
G(\Bx, t)
&= \IIinf
\frac{v}{(2\pi)^3 \Abs{\Bk}}
\sin( \Abs{\Bk} v t )
\exp\left( i \Bk \cdot \Bx \right)
d^3 k
\end{align}

For computational purposes we are probably much better off using
equation \ref{eqn:fourier_maxwell:waveNumDomainSolution}, however,
from an abstract point of form this expression is much prettier.

One can also see the elements of the traditional retarded time expressions for the potential hiding in there.  See
\chapcite{PJpoisson} for an evaluation of this integral (in an ad-hoc
non-rigorous fashion) eventually producing the retarded time solution.

\subsubsection{Tweak this a bit to put into proper Green's function form}

Now, it makes sense to redefine $G(\Bx,t)$ above so that we can integrate uniformly over all space and time.  To do so we can add a unit step function into the definition, so that $G(\Bx,t<0) = 0$.  Additionally, if we express this convolution it is slightly tidier (and consistent with the normal Green's function notation) to put the parameter differences in the kernel term.  Such a change of variables will alter the sign of the integral limits by a factor of $(-1)^4$, but we also have a $(-1)^4$ term from the differentials.  After making these final adjustments we have a final variation of our integral solution

%
% y_j = x_j - x'_j
% dy_j = x_j - dx'_j
% t' = t - \tau
% dt' = t - d\tau
% 
%&= \int_x'1 \int_x'2 \int_x'3 \int_tau g(\Bx -\Bx', t-\tau) G(\Bx', \tau) d^3 x' d\tau \\
%
% =>
%
%&= (-1)^4 \int_y1 \int_y2 \int_y3 \int_t' g(\By, t') G(\Bx - \By, t - t') (-1)^4 d^3 y dt' \\
% 
% 
\begin{align}
{\psi}(\Bx, t)
&= \IIinf g(\Bx', t') G(\Bx - \Bx', t - t') d^3 x' dt' \\
G(\Bx, t)
&= \theta(t) \IIinf
\frac{v}{(2\pi)^3 \Abs{\Bk}}
\sin( \Abs{\Bk} v t )
\exp\left( i \Bk \cdot \Bx \right)
d^3 k
\end{align}

Now our inhomogeneous solution is expressed nicely as the convolution of our current density over all space and time with an integral kernel.  That integral kernel is precisely the Green's function for this forced wave equation.

This solution comes with a large number of assumptions.  Along the way we have the assumption that both our wave function and the charge density was Fourier transformable, and that the wave number domain products were inverse transformable.  We also had an assumption that the wave function is sufficiently small at the limits of integration that the intermediate contributions from the integration by parts vanished, and finally the big assumption that we were perfectly free to interchange integration order in an extremely ad-hoc and non-rigorous fashion!

\section{Maxwell equation solution}

Having now found Green's function form for the forced wave equation, we can now move to Maxwell's equation

\begin{align*}
\grad F = J/\epsilon_0 c
\end{align*}

In terms of potentials we have $F = \grad \wedge A$, and may also impose the Lorentz gauge $\grad \cdot A = 0$, to give us our four charge/current forced wave equations

\begin{align*}
\grad^2 A = J/\epsilon_0 c
\end{align*}

As scalar equations these are

\begin{align}
\left( \inv{c^2} \PDSq{t}{} -\sum_j \PDSq{x^j}{} \right) A^\mu = \frac{J^\mu}{\epsilon_0 c}
\end{align}

So, from above, also writing $x^0 = ct$, we have

\begin{align}\label{eqn:fourier_maxwell:fourVectorPotentials}
{A^\mu}(x)
&= \inv{\epsilon_0 c} \int J^\mu(x') G(x - x') d^4 x' \\
G(x)
&= \theta(x^0) \int
\frac{1}{(2\pi)^3 \Abs{\Bk}}
\sin( \Abs{\Bk} x^0 )
\exp\left( i \Bk \cdot \Bx \right)
d^3 k
\end{align}

\subsection{Four vector form for the Green's function}

Can we put the sine and exponential product in a more pleasing form?  It would be nice to merge the $\Bx$ and $ct$ terms into a 
single four vector form.  One possibility is merging the two

\begin{align*}
\sin( \Abs{\Bk} x^0 ) &\exp\left( i \Bk \cdot \Bx \right) \\
&=
\inv{2i} \left(
\exp\left( i \left( \Bk \cdot \Bx +\Abs{\Bk} x^0 \right) \right)
-\exp\left( i \left( \Bk \cdot \Bx -\Abs{\Bk} x^0 \right) \right)
\right) \\
&=
\inv{2i} \left(
\exp\left( i \Abs{\Bk} \left( \kcap \cdot \Bx + x^0 \right) \right)
-\exp\left( i \Abs{\Bk} \left( \kcap \cdot \Bx - x^0 \right) \right)
\right)
\end{align*}

Here we have a sort of sine like conjugation in the two exponentials.  Can we tidy this up?  Let us write the unit wave number
vector in terms of direction cosines

\begin{align*}
\kcap 
&= \sum_m \sigma_m \alpha_m \\
&= \sum_m \gamma_m \gamma_0 \alpha_m \\
\end{align*}

Allowing us to write

\begin{align*}
\sum_m \gamma^m \alpha_m &= -\kcap \gamma_0
\end{align*}

This gives us
\begin{align*}
\kcap \cdot \Bx + x^0 
&= \alpha_m x^m + x^0 \\
&= (\alpha_m \gamma^m) \cdot (\gamma_j x^j) + \gamma^0 \cdot \gamma_0 x^0 \\
&= (-\kcap \gamma_0 + \gamma_0) \cdot \gamma_\mu x^\mu \\
&= (-\kcap \gamma_0 + \gamma_0) \cdot x
\end{align*}

Similarly we have

\begin{align*}
\kcap \cdot \Bx - x^0  &= (-\kcap \gamma_0 - \gamma_0) \cdot x
\end{align*}

and can now put $G$ in explicit four vector form

\begin{align*}
G(x)
&= 
%\theta(x \cdot \gamma_0) \int
%\frac{1}{(2\pi)^3 2 i \Abs{\Bk}}
%\left(
%\exp\left( i ((\Abs{\Bk} -\Bk)\gamma_0) \cdot x \right)
%-\exp\left( -i ((\Abs{\Bk} +\Bk)\gamma_0) \cdot x \right)
%\right)
%d^3 k
\frac{\theta(x \cdot \gamma_0)}{
(2\pi)^3 2 i 
} \int
\left(
\exp\left( i ((\Abs{\Bk} -\Bk)\gamma_0) \cdot x \right)
-\exp\left( -i ((\Abs{\Bk} +\Bk)\gamma_0) \cdot x \right)
\right)
\frac{d^3 k}{ \Abs{\Bk} }
\end{align*}

Hmm, is that really any better?  Intuition says that this whole thing 
can be written as sine with some sort of geometric product conjugate 
terms.

I get as far as writing 

\begin{align*}
i ( \Bk \cdot \Bx \pm \Abs{\Bk} x^0 ) 
&=
(i \gamma_0) \wedge ( \Bk \pm \Abs{\Bk} ) \cdot x
\end{align*}

But that does not quite have the conjugate form I was looking for (or does it)?  Have to go back and look at Hestenes's multivector conjugation operation.  Think it had something to do with reversion, but do not recall.

Failing that tidy up the following

\begin{align}
G(x)
&= 
\frac{\theta(x \cdot \gamma_0)}{ (2\pi)^3 }
\int
\sin( \Abs{\Bk} x \cdot \gamma_0 )
\exp\left( -i (\Bk \gamma_0) \cdot x \right)
\frac{d^3 k}{ \Abs{\Bk} }
\end{align}

is probably about as good as it gets for now.  Note the interesting feature
that we end up essentially integrating over a unit ball in our wave number
space.  This suggests the possibility of simplification using the 
divergence theorem.

\subsection{Faraday tensor}

Attempting to find a tidy four vector form for the four vector potentials was in preparation for taking derivatives.
Specifically, applied to \ref{eqn:fourier_maxwell:fourVectorPotentials} we have

\begin{align*}
F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu
\end{align*}

subject to the Lorentz gauge constraint
\begin{align*}
0 = \partial_\mu A^\mu
\end{align*}

If we switch the convolution indexes for our potentials

\begin{align*}
{A^\mu}(\Bx, t) &= \inv{\epsilon_0 c} \int J^\mu(x - x') G(x') d^4 x' \\
\end{align*}

Then the Lorentz gauge condition, after differentiation under the integral sign, is

\begin{align*}
0 = \partial_\mu A^\mu &= \inv{\epsilon_0 c} \int \left(\partial_\mu J^\mu(x - x') \right) G(x') d^4 x' \\
\end{align*}

So we see that the Lorentz gauge seems to actually imply the continuity equation

\begin{align*}
\partial_\mu J^\mu(x) = 0
\end{align*}

Similarly, it appears that we can write our tensor components in terms of current density derivatives

\begin{align}
F^{\mu\nu} 
%&= \partial^\mu A^\nu - \partial^\nu A^\mu \\
&= \inv{\epsilon_0 c} \int \left(\partial^\mu J^\nu(x - x') - \partial^\nu J^\mu(x - x') \right) G(x') d^4 x'
\end{align}

Logically, I suppose that one can consider the entire problem solved here, pending the completion of this calculus exercise.

In terms of tidiness, it would be nicer seeming use the original convolution, and take derivative differences of the Green's function.  However, how to do this is not clear to me since this function has no defined derivative at the $t=0$ points due to the unit step.

\section{Appendix.  Mechanical details}

\subsection{Solving the wave number domain differential equation}

We wish to solve equation the inhomogeneous equation \ref{eqn:fourier_maxwell:waveNumEquationToSolve}.  Writing this in terms of a linear operator equation this is

\begin{align*}
L(y) &= y'' + \alpha^2 y \\
L(y) &= h
\end{align*}

The solutions of this equation will be formed from linear combinations of the homogeneous problem plus a specific solution of the inhomogeneous problem

By inspection the homogeneous problem has solutions in $\Span \{ e^{ i \alpha x }, e^{ -i \alpha x }\}$.
We can find a solution to the inhomogeneous problem using the variation of parameters method, assuming a solution of the form

\begin{align*}
y  = u e^{ i \alpha x } + v e^{ -i \alpha x }
\end{align*}

Taking derivatives we have
\begin{align*}
y' = u' e^{ i \alpha x } + v' e^{ -i \alpha x } + i \alpha (u e^{ i \alpha x } - v e^{ -i \alpha x })
\end{align*}

The trick to solving this is to employ the freedom to set the $u'$, and $v'$ terms above to zero

\begin{align}\label{eqn:fourier_maxwell:firstConstraint}
u' e^{ i \alpha x } + v' e^{ -i \alpha x } = 0
\end{align}

Given this choice we then have
\begin{align*}
y' &= i \alpha (u e^{ i \alpha x } - v e^{ -i \alpha x }) \\
y'' &=
(i \alpha)^2 (u e^{ i \alpha x } + v e^{ -i \alpha x })
i \alpha (u' e^{ i \alpha x } - v' e^{ -i \alpha x })
\end{align*}

So we have
\begin{align*}
L(y)
&=
(i \alpha)^2 (u e^{ i \alpha x } + v e^{ -i \alpha x })  \\
&+i \alpha (u' e^{ i \alpha x } - v' e^{ -i \alpha x })
+ (\alpha)^2 (u e^{ i \alpha x } + v e^{ -i \alpha x })  \\
&=
i \alpha (u' e^{ i \alpha x } - v' e^{ -i \alpha x })
\end{align*}

With this and \ref{eqn:fourier_maxwell:firstConstraint} we have a set of simultaneous first order linear differential equations to solve

\begin{align*}
\begin{bmatrix}
u' \\
v' \\
\end{bmatrix}
&=
{\begin{bmatrix}
 e^{ i \alpha x } &- e^{ -i \alpha x } \\
 e^{ i \alpha x } &  e^{ -i \alpha x } \\
\end{bmatrix}}^{-1}
\begin{bmatrix}
{h}/{i \alpha} \\
0 \\
\end{bmatrix} \\
&=
\inv{2}
{\begin{bmatrix}
 e^{ -i \alpha x } & e^{ -i \alpha x } \\
 -e^{ i \alpha x } &  e^{ i \alpha x } \\
\end{bmatrix}}
\begin{bmatrix}
{h}/{i \alpha} \\
0 \\
\end{bmatrix} \\
&=
\frac{h}{2 i \alpha}
{\begin{bmatrix}
 e^{ -i \alpha x } \\
 -e^{ i \alpha x } \\
\end{bmatrix}}
\end{align*}

Substituting back into the assumed solution we have
\begin{align*}
y
&= \frac{1}{2 i \alpha} \left(
  e^{ i \alpha x } \int h e^{ -i \alpha x }
- e^{ -i \alpha x } \int h e^{ i \alpha x }
\right) \\
&= \frac{1}{2 i \alpha} \int_{u=x_0}^x h(u) \left( e^{ -i \alpha (u-x) } -e^{ i \alpha (u-x) } \right) du \\
\end{align*}

So our solution appears to be

\begin{align}\label{eqn:fourier_maxwell:solutionToWaveNumberDomainEquation}
y &= \frac{1}{\alpha} \int_{u=x_0}^x h(u) \sin( \alpha(x-u) ) du
\end{align}

A check to see if this is correct is in order to verify this.  Differentiating using \ref{eqn:fourier_maxwell:diffInt} we have

\begin{align*}
y'
&=
{\left.
\frac{1}{\alpha}
h(u) \sin( \alpha(x-u) ) \right\vert}_{u=x}
+\frac{1}{\alpha} \int_{u=x_0}^x \PD{x}{} h(u) \sin( \alpha(x-u) ) du \\
&= \int_{u=x_0}^x h(u) \cos( \alpha(x-u) ) du \\
\end{align*}

and for the second derivative we have

\begin{align*}
y''
&=
{\left. h(u) \cos( \alpha(x-u) ) \right\vert}_{u=x}
- \alpha \int_{u=x_0}^x h(u) \sin( \alpha(x-u) ) du \\
&= h(x) - \alpha^2 y(x)
\end{align*}

Excellent, we have $y'' + \alpha^2 y = h$ as desired.

\subsection{Differentiation under the integral sign}

Given an function that is both a function of the integral limits and the integrals kernel

\begin{align*}
f(x) = \int_{u = a(x)}^{b(x)} G(x,u) du,
\end{align*}

lets recall how to differentiate the beastie.  First let $G(x,u) = \PDi{u}{F(x,u)}$ so we have

\begin{align*}
f(x) = F(x,b(x)) - F(x,a(x))
\end{align*}

and our derivative is
\begin{align*}
f'(x)
&=
\PD{x}{F}(x,b(x))
\PD{u}{F}(x,b(x)) b'
-\PD{x}{F}(x,a(x))
-\PD{u}{F}(x,a(x)) a' \\
&=
G(x,b(x)) b'
-G(x,a(x)) a'
+\PD{x}{F}(x,b(x))
-\PD{x}{F}(x,a(x))
\\
\end{align*}

Now, we want $\PDi{x}{F}$ in terms of $G$, and to get there, assuming sufficient continuity, we have from the definition

\begin{align*}
\PD{x}{} G(x,u) 
&= \PD{x}{} \PD{u}{F(x,u)} \\
&= \PD{u}{} \PD{x}{F(x,u)} \\
\end{align*}

Integrating both sides with respect to $u$ we have

\begin{align*}
\int \PD{x}{G} du 
&= \int \PD{u}{} \PD{x}{F(x,u)} du \\
%&= \int \PD{u}{} \left( \PD{x}{F(x,u)} + A(x) \right) du \\
&= \PD{x}{F(x,u)} 
\end{align*}

This allows us to write 

\begin{align*}
\PD{x}{F}(x,b(x))
-\PD{x}{F}(x,a(x))
&=
\int_{a}^b \PD{x}{G}(x,u) du
\end{align*}

and finally

\begin{align}\label{eqn:fourier_maxwell:diffInt}
\frac{d}{dx} \int_{u = a(x)}^{b(x)} G(x,u) du
&=
G(x,b(x)) b'
-G(x,a(x)) a'
+ \int_{a(x)}^{b(x)} \PD{x}{G}(x,u) du
\end{align}

\subsubsection{Argument logic error above to understand}

Is the following not also true

\begin{align*}
\int \PD{x}{G} du 
&= \int \PD{u}{} \PD{x}{F(x,u)} du \\
&= \int \PD{u}{} \left( \PD{x}{F(x,u)} + A(x) \right) du \\
&= \PD{x}{F(x,u)} + A(x)u + B
\end{align*}

In this case we have
\begin{align*}
\PD{x}{F}(x,b(x)) -\PD{x}{F}(x,a(x)) &= \int_{a}^b \PD{x}{G}(x,u) du - A(x) ( b(x) - a(x))
\end{align*}

How to reconcile this with the answer I expect (and having gotten it, I believe matches my recollection)?
