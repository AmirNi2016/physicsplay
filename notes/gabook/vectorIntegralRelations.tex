\documentclass{article}

\input{../peeters_macros.tex}
\newcommand{\grad}[0]{\nabla}
\newcommand{\PD}[2]{\frac{\partial {#2}}{\partial {#1}}}
\newcommand{\Abs}[1]{\left\lvert{#1}\right\rvert}

\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package. 
\usepackage{latexsym,amssymb,epsf} % don't remember if these are
   % needed, but their inclusion can't do any damage



\usepackage[bookmarks=true]{hyperref}

\title{ Reconciling vector integral relations. }
\author{Peeter Joot}
\date{ Sept. 18, 2008.  Last Revision: $Date: 2008/09/19 04:32:37 $ }

\begin{document}

\maketitle{}

\tableofcontents

\section{ A hodge podge of relations. }

%I was never satisfied with how vector integral relationships were presented
%to me in my Calculus classes.  Some of these were presented as equations to
%memorize instead of with proof.  

The aim of these notes is to work through proofs of the following 
integral equations

\begin{itemize}

\item Gradient line integral. 

\begin{equation}\label{eqn:lineintegral}
\int_C (\grad f) \cdot d\Br = f \vert_{\partial C}
\end{equation}

\item Jacobian area determinants. 

Change of variables for a double integral

\begin{equation}
dA = dx dy =
\begin{vmatrix}
\PD{u}{x} & \PD{u}{y} \\
\PD{v}{x} & \PD{v}{y} \\
\end{vmatrix}
du dv
= \Abs{ \PD{(u,v)}{(x,y)} } du dv
\end{equation}

In Salus and Hille this is proved using Green's theorem, despite it 
being seeming like the more basic operation.  The greater than two
dimensional cases are not proved at all.

\item Green's theorem. 

\begin{equation}\label{eqn:greens}
\int\int \left(\PD{y}{Q} - \PD{x}{P}\right) dx dy = \oint P dx + Q dy
\end{equation}

\item Divergence theorem. 

\begin{equation}\label{eqn:divergenceplane}
\int\int \grad \cdot \Bv\, dx dy = \oint \Bv \cdot \ncap\, ds
\end{equation}

\begin{equation}\label{eqn:divergencevolume}
\int\int\int_V \grad \cdot \Bv\, dx dy dz = \int\int_S \Bv \cdot \ncap\, dA
\end{equation}

\begin{equation}\label{eqn:divergencegrad}
\int\int\int_V \grad \phi\, dV \int\int_S \ncap \phi\, dA
\end{equation}

\begin{equation}\label{eqn:divergencegradcross}
\int\int\int_V \grad \cross \Bv\, dV \int\int_S \Bv \cross \ncap\, dA
\end{equation}

\item Stokes theorem. 

\begin{equation}\label{eqn:stokes}
\int\int (\grad \cross \Bv) \cdot \ncap\, dx dy = \oint \Bv \cdot d\Br
\end{equation}

\end{itemize}

In particular I'd like to relate these to the geometrical concepts
of Clifford algebra now that I know how to work with that in a 
differential and algebraic fashion for many sorts of problems.  I am hoping
that working through proofs of these basic identities 
will be enough that I can go on to the more general approaches in 
differential forms and the geometric calculus of Hestenes.

John Denker's 
\href{ http://www.av8n.com/physics/straight-wire.pdf }{ article on the
magnetic field of a straight wire }
gives a simple looking high level description of vector form of Stokes'
theorem in it's Clifford formulation

\begin{equation}\label{eqn:stokesGA}
\int_S \grad \wedge F = \int_{\partial S} F
\end{equation}

This is simple enough looking, but there are some important details left
out.  In particular the grades do not match, so there must be some sort of
implied projection or dot product operations too.

I'd say this suffers from some of the things that I had trouble with in
attempting to study differential forms.

The basic ideas of how to formulate
the curve, surface, volume, ... of integration is not specified.  How to do
that in greater than three dimensions is not trivial seeming to me since
none of the traditional methods of dotting with a normal will not work.

Knowing now about how subspaces can be expressed using blades is likely the
key.  The Clifford algebra ideas seem particularly suited to this as many
of these ideas can be formulated independent of the calculus applications.
One can learn the geometric and algebraic concepts first and then move on
to the Calculus.

\section{ Gradient line integral. }

This is the easiest of the identities to prove.  Introduction of a reciprocal frame $\gamma^{\mu} \cdot \gamma_{\nu} = {\delta^{\mu}}_{\nu}$
also means that we can do in full generalitity with a possibly
non-orthonormal basis of any dimension, and an arbitrary metric.

Write the gradient as normal

\begin{equation*}
\grad = \sum \gamma^{\mu} \PD{x^{\mu}}{} = \gamma^{\mu} \partial_{\mu}
\end{equation*}

Here summation convention with implied sum over mixed upper and lower indexes is employed.

Express the position vector along the curve as
a parameterized path $\Br = \Br(\lambda) = \gamma_{\mu} x^{\mu}$, and use
this to form the element of vector length along the path
%This can be used to form the line integral element that we also need
%the differential element of vector length.  

\begin{equation*}
d\Br = \gamma_{\mu} \frac{d x^{\mu}}{d\lambda} d\lambda
\end{equation*}

Dotting the gradient and the path element we have
\begin{align*}
\grad f \cdot d\Br 
&= \left(\gamma^{\mu} \partial_{\mu} f\right) \cdot \left(\gamma_{\nu} \frac{d x^{\nu}}{d\lambda} \right) d\lambda \\
&= {\delta^{\mu}}_{\nu} \PD{x^{\mu}}{f} \frac{d x^{\nu}}{d\lambda} d\lambda \\
&= \sum \PD{x^{\mu}}{f} \frac{d x^{\mu}}{d\lambda} d\lambda \\
&= \frac{d f}{d \lambda} d\lambda
\end{align*}

Equation \ref{eqn:lineintegral} follows immediately, which we see to be really not much more than the chain rule.  One kind of nice feature
about this derivation is that if you do it backwards, it provides a nice motivation for the general form of the gradient (ie: in terms
of a non-orthonormal basis).  That's a lot more obvious a way to get at this result than my previous way of observing that the Euler-Lagrange
equations when summed in vector form imply that this is the required form of the gradient.

\section{ Jacobian area determinants. }

Next in ease of proof is the Jacobian determinant.  This actually comes largely for free since we can utilize the wedge product to
express areas.

\begin{figure}[htp]
\centering
\includegraphics[totalheight=0.4\textheight]{planeParameterization}
\caption{Plane parameterization}\label{fig:planeParameterization}
\end{figure}

Introduce a two vector parameterization of the area as in figure \ref{fig:planeParameterization}

\begin{equation*}
\Br = \gamma^{i} \phi_i(u,v)
\end{equation*}

Provided that the partials are not colinear at the point of interest, we can compute the area of the parallogram spanned by these

\begin{align*}
d\BA 
&= \left(\PD{u}{\Br} du\right) \wedge \left(\PD{v}{\Br} dv\right) \\
&= \left(\gamma^{i} \PD{u}{\phi_i}\right) \wedge \left(\gamma^{j} \PD{v}{\phi_j} \right) du dv \\
&= \gamma^{i} \wedge \gamma^{j} \PD{u}{\phi_i} \PD{v}{\phi_j} du dv \\
&= \sum_{i<j} \gamma^{i} \wedge \gamma^{j} \left( \PD{u}{\phi_i} \PD{v}{\phi_j} - \PD{u}{\phi_i} \PD{v}{\phi_i} \right) du dv \\
&= \sum_{i<j} \gamma^{i} \wedge \gamma^{j} \PD{(u,v)}{(\phi_i,\phi_j)} du dv \\
\end{align*}

Here $d\BA$ is a bivector area element, so in the purely two dimensional case, where this is constrained to a plane, the scalar area element
is recovered by dividing by the plane unit pseudoscalar having the same orientation as this bivector.

One can also see how the same idea will be of use later in the Stokes' generalization of Green's theorem (considering a surface element small enough to be considered planar).

For now, considering just the 2D case we have, to divide through by the plane unit pseudoscalar $i = \Be_1\Be_2$ produced by the product of two orthonormal vectors we want to calculate the product:

\begin{align*}
\inv{i} \gamma^{1} \wedge \gamma^{2} 
&= (\Be_2 \wedge \Be_1) \cdot (\gamma^{1} \wedge \gamma^{2}) \\
&= \Be_2 \cdot (\Be_1 \cdot (\gamma^{1} \wedge \gamma^{2})) \\
&= \Be_2 \cdot ( (\Be_1 \cdot \gamma^{1}) \gamma^{2} -(\Be_1 \cdot \gamma^{2}) \gamma^{1} ) \\
&= (\Be_1 \cdot \gamma^{1}) (\Be_2 \cdot \gamma^{2}) -(\Be_1 \cdot \gamma^{2}) (\Be_2 \cdot \gamma^{1}) \\
&=
\begin{vmatrix}
\Be_1 \cdot \gamma^{1} & \Be_1 \cdot \gamma^{2} \\
\Be_2 \cdot \gamma^{1} & \Be_2 \cdot \gamma^{2}
\end{vmatrix}
\end{align*}

Thus the (scalar) area element is

\begin{align}\label{eqn:jacobianframe}
dA =
\begin{vmatrix}
\Be_1 \cdot \gamma^{1} & \Be_1 \cdot \gamma^{2} \\
\Be_2 \cdot \gamma^{1} & \Be_2 \cdot \gamma^{2}
\end{vmatrix}
\PD{(u,v)}{(\phi_1,\phi_2)} du dv
\end{align}

This is a slightly more general form than we are used to seeing since the position vector parameterization was allowed to be expresed in terms
of an arbitrary (possibly non-orthonormal) basis.  For that special case of an orthonormal parameterization

\begin{align*}
\Br = x(u,v) \Be_1 + y(u,v) \Be_2
\end{align*}

the product of determinants in \ref{eqn:jacobianframe} takes the usual form

\begin{align}\label{eqn:jacobianarea}
dx dy = \PD{(u,v)}{(x,y)} du dv.
\end{align}

Now the danger of an expression like \ref{eqn:jacobianarea} is that it almost
seems obvious.  Now, if you understand the wedge product origin you can state
that obviousness after a little bit of algebra.  However, in a book like Salus and Hille where
they can't derive it without waiting til close to the end of the book so that Green's is
available, it's not really so obvious.  The geometrical background just isn't there.

Now, it's not too hard to see that this idea will also work for change of variables for volume and higher
dimensional volume elements.  We just have to divide by the spatial (or higher dimensional) pseudoscalar of the
same orientation as the parameterization.  Because there are sign variations in the pseudoscalar, this also
illustrates why the absolute value of the Jacobian determinant is used in some circumstances.

\section{ Green's theorem. }

\section{ Divergence theorem. }

\section{ Stokes theorem. }

\end{document}               % End of document.
