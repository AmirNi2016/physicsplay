\documentclass{article}      % Specifies the document class

\input{../peeters_macros.tex}
%\DeclareMathOperator{\TextTranspose}{T}
\DeclareMathOperator{\rank}{rank}
%\newcommand{\transpose}[1]{{{#1}^{\TextTranspose}}}
\newcommand{\transpose}[1]{{{#1}^{\text{T}}}}

\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package. 
\usepackage{latexsym,amssymb,epsf} % don't remember if these are
   % needed, but their inclusion can't do any damage


%
% The real thing:
%

                             % The preamble begins here.
\title{ Matrix review. } % Declares the document's title.
\author{Peeter Joot}         % Declares the author's name.
\date{ April 11, 2008 }        % Deleting this command produces today's date.

\begin{document}             % End of preamble and beginning of text.

\maketitle{}

\section{ Motivation. }

My initial intention for these notes was to 
get a feel for the similarities and differences between GA and matrix approaches to solution of projection.  Attempting to
write up that comparison
I found gaps in my understanding of the matrix algebra.  In particular the topic of projection as well as the related
ideas of pseudoinverses and SVD were not adequately covered in my university courses, nor my texts from those courses.
Here is my attempt to write up what I understand of these subjects and explore the gaps in my knowledge.

Particularily helpful was 
Gilbert Strang's excellent MIT lecture on subspace projection (available on the MIT opencourse website).
Much of the notes below are probably detailed in his course textbook, which I don't have.  However, 
if I can't explain the ideas to myself, or write them up in a way that I feel would explain
to others, then I obviously don't understand them sufficiently.

\section{ Subspace projection in matrix notation. }

\subsection{ Projection onto line. }

\begin{figure}[htp]
\centering
\includegraphics[totalheight=0.4\textheight]{Projection_line}
\caption{Projection onto line.}\label{fig:Projection_line}
\end{figure}

The simplest sort of projection to compute is projection onto a line.  Given a direction vector $b$, and a line with direction vector $u$
as in figure \ref{fig:Projection_line}.

The projection onto $u$ is some value:

\[
p = \alpha u
\]

and we can write

\[
b = p + e
\]

where e is the component perpendicular to the line $u$.

Expressed in terms of the dot product this relationship is described by:

\[
(b - p) \cdot a = 0
\]

Or, 

\[
b \cdot a = \alpha a \cdot a
\]

and solving for $\alpha$ and substituting we have:

\begin{equation}
p = a \frac{a \cdot b}{a \cdot a}
\end{equation}

In matrix notation that is:

\begin{equation}
p = a \left(\frac{\transpose{a} b}{\transpose{a} a} \right)
\end{equation}

Following Gilbert Strang's MIT lecture on subspace projection, the parenthesis can be moved to directly express this as a projection matrix operating on b.

\begin{equation}\label{eqn:projectmatrixline}
p = \left(\frac{a \transpose{a}}{\transpose{a} a}\right) b = P b
\end{equation}

\subsection{ Projection onto plane (or subspace). }

\begin{figure}[htp]
\centering
\includegraphics[totalheight=0.4\textheight]{Projection_plane}
\caption{Projection onto plane.}\label{fig:Projection_plane}
\end{figure}

Calculation of the projection matrix to project onto a plane is similar.  The variables to solve for are $p$, and $e$ in as figure
figure \ref{fig:Projection_plane}.

For projection onto a plane (or hyperplane) the idea is the same, splitting the vector into a component in the plane and an perpendicular component.
Since the
idea is the same for any dimensional subspace, explicit specification of the summation range is omitted here so the result is good for higher dimensional
subspaces as well as the plane:

\[
b - p = e
\]
\[
p = \sum \alpha_i u_i
\]

however, we get a set of equations, one for each direction vector in the plane

\[
(b - p) \cdot u_i = 0
\]

Expanding $p$ explicitly and rearranging we have the following set of equations:

\[
b \cdot u_i = (\sum_s \alpha_s u_s) \cdot u_i
\]

putting this in matrix form

\begin{align*}
[b \cdot u_i]_i 
&= 
{
\begin{bmatrix}
(\sum_s \alpha u_s) \cdot u_i
\end{bmatrix}
}_i \\
\end{align*}

Writing $U = 
\begin{bmatrix}
u_1 & u_2 & \cdots 
\end{bmatrix}$

\begin{align*}
\begin{bmatrix}
\transpose{u_1} \\
\transpose{u_2} \\
\vdots 
\end{bmatrix}
b
&= 
\begin{bmatrix}
(\sum_s \alpha_s u_s) \cdot u_i
\end{bmatrix} \\
&= 
{
\begin{bmatrix}
u_i \cdot u_j
\end{bmatrix}
}_{ij}
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\vdots \\
\end{bmatrix} \\
\end{align*}

Solving for the vector of unknown coefficients $\alpha = [\alpha_i]_i$ we have

\[
\alpha
=
{{
\begin{bmatrix}
u_i \cdot u_j
\end{bmatrix}
}_{ij}}^{-1}
\transpose{U} b
\]

And 

\begin{equation}
p = U \alpha = U
{{
\begin{bmatrix}
u_i \cdot u_j
\end{bmatrix}
}_{ij}}^{-1}
\transpose{U} b
\end{equation}

However, this matrix in the middle is just $\transpose{U} U$:

\begin{align*}
\begin{bmatrix}
\transpose{u_1} \\
\transpose{u_2} \\
\vdots \\
\end{bmatrix}
\begin{bmatrix}
{u_1} & {u_2} & \hdots \\
\end{bmatrix}
&=
\begin{bmatrix}
\transpose{u_1} {u_1} & \transpose{u_1} {u_2} & \hdots \\
\transpose{u_2} {u_1} & \transpose{u_2} {u_2} & \hdots \\
\vdots & & \\
\end{bmatrix} \\
&=
{
\begin{bmatrix}
\transpose{u_i} {u_j}
\end{bmatrix}
}_{ij} \\
&=
{
\begin{bmatrix}
{u_i} \cdot {u_j}
\end{bmatrix}
}_{ij} \\
\end{align*}

This provides the final result:

\begin{equation}\label{eqn:projectiongeneralmatrix}
\Proj_{U}\left(b\right) = U (\transpose{U} U)^{-1} \transpose{U} b
\end{equation}

\subsection{ Application of projection as left pseudoinverse (ie: linear fitting). }

Equation \ref{eqn:projectiongeneralmatrix} provides us a way to find best solutions to general equations of the form:

\[
A x = b
\]

Here $A$ is the matrix of a linear transformation, $A : \mathbb{R}^k \rightarrow \mathbb{R}^n$, for some $k<n$.
By ``best solutions'' here, we give this the geometrical meaning, namely, the solution matching the projection of $b$ onto the space.

If b is not completely in the column space $C(A)$ of $A$, this can have no solution.  However, writing

\[
b = \Proj_A(b) + b_\perp
\]

as the components of $b$ in $C(A)$ and not in $C(A)$ respectively we can at least solve the reduced equation for $\hat{x}$:

\begin{equation}\label{eqn:reducedinverseproblem}
A \hat{x} = \Proj_A(b)
\end{equation}


This will be possible even in circumstances that the original equation had no solution.  Specifically, the vector b when projected onto the plane can be expressed as some
linear combination of the columns of $A$ (a basis for the subspace).

Substuition of our projection result into equation \ref{eqn:reducedinverseproblem} yields:

\begin{align*}
A \hat{x} 
&= \Proj_{A}\left(b\right) = A (\transpose{A} A)^{-1} \transpose{A} b
\end{align*}

The simplest case here is when $A$ is of full column rank since one can pre-multiply this complete equation by $\transpose{A}$ without any possibility of nulling
$A \hat{x}$.

\begin{align*}
\transpose{A} A \hat{x} 
&= \transpose{A} A (\transpose{A} A)^{-1} \transpose{A} b \\
&= \transpose{A} b \\
\end{align*}

Thus our best fit vector is

\begin{equation}
\hat{x} 
= (\transpose{A} A)^{-1} \transpose{A} b
\end{equation}

Another way to view this is for any vector $x$ that is not in the null space $N(A)$, then the matrix:

\begin{equation}
A^{+}= (\transpose{A} A)^{-1} \transpose{A}
\end{equation}

has the action of a left inverse for any full column rank matrix $A$.  Thus when there is a solution to:

\begin{equation}
A x = b.
\end{equation}

It can be obtained by pre-multiplication using this "left" inverse.

\begin{equation}
A^{+} A x = x = A^{+} b
\end{equation}

\subsection{ SVD connection. }

SVT decomposition is an factoring of $A \in M^{m \times n}$ with orthonormal matrices $U \in M^{m \times m}$
and $V \in M^{n \times n} $ producing the following form:

\[
A = U \Sigma \transpose{V}
\]

Sigma has the form:

\[
\Sigma = 
\begin{bmatrix}
D_{r,r} & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r} \\
\end{bmatrix}
\]

where $r = \rank(A)$, and $D$ is a diagonal matrix with the root of the (positive) eigenvalues of $\transpose{A}A$.

This provides a generalized spectral decomposition and similarity that applies to both non-square matrices and matrices not otherwise diagonalizable
(ie: square matrix with similarity to a Jordon form matrix).  Given this decomposition we can write:

\[
\Sigma = \transpose{U} A V
\]

If one were to ask the question of what is the closest that one could get to inverting such a matrix.  It's pretty clear that the closest one could get to
identity will be with multiplication of a $\Sigma^{+}$ of the following form:

\[
\Sigma^{+} \Sigma
=
\begin{bmatrix}
(D_{r,r})^{-1} & 0_{r,m-r} \\
0_{n-r,r} & 0_{n-r,m-r} \\
\end{bmatrix}
\begin{bmatrix}
D_{r,r} & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r} \\
\end{bmatrix}
=
\begin{bmatrix}
I_{r,r} & 0_{r,n-r} \\
0_{n-r,r} & 0_{n-r,n-r} \\
\end{bmatrix}
\]

For a right pseudoinverse we have a similar result:

\[
\Sigma
\Sigma^{+}
=
\begin{bmatrix}
D_{r,r} & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r} \\
\end{bmatrix}
\begin{bmatrix}
(D_{r,r})^{-1} & 0_{r,m-r} \\
0_{n-r,r} & 0_{n-r,m-r} \\
\end{bmatrix}
=
\begin{bmatrix}
I_{r,r} & 0_{r,m-r} \\
0_{m-r,r} & 0_{m-r,m-r} \\
\end{bmatrix}
\]

With either of these one can define a corresponding pseudoinverse (left or right) as:

\begin{equation}
A^{+} = V \Sigma^{+} \transpose{U}
\end{equation}

This is a logical definition, but how close is it to the projective
left inverse we calculated above in the case where $A$ is not of full column 
rank?

Multiplication gives: 

\begin{align*}
A^{+} A 
&= V \Sigma^{+} \transpose{U} U \Sigma \transpose{V} \\
&= V \Sigma^{+} \Sigma \transpose{V} \\
&=
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r & v_{r+1} & \cdots & v_n \\
\end{bmatrix}
\begin{bmatrix}
(D_{r,r})^{-1} & 0_{r,m-r} \\
0_{n-r,r} & 0_{n-r,m-r} \\
\end{bmatrix}
\begin{bmatrix}
D_{r,r} & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r} \\
\end{bmatrix}
\begin{bmatrix}
\transpose{v_1} \\ \transpose{v_2} \\ \vdots \\ \transpose{v_r} \\ \transpose{v_{r+1}} \\ \vdots \\ \transpose{v_n} \\
\end{bmatrix}
%\begin{bmatrix}
%I_{r,r} & 0_{r,n-r} \\
%0_{n-r,r} & 0_{n-r,n-r} \\
%\end{bmatrix}
%\transpose{V} \\
%&=
%\begin{bmatrix}
%v_1 & v_2 & \cdots & v_r & v_{r+1} & \cdots & v_n \\
%\end{bmatrix}
%\begin{bmatrix}
%I_{r,r} & 0_{r,n-r} \\
%0_{n-r,r} & 0_{n-r,n-r} \\
%\end{bmatrix}
%\transpose{V} \\
%&=
%\begin{bmatrix}
%v_1 & v_2 & \cdots & v_r & 0 & \cdots & 0 \\
%\end{bmatrix}
%\begin{bmatrix}
%\transpose{v_1} \\ \transpose{v_2} \\ \vdots \\ \transpose{v_r} \\ \transpose{v_{r+1}} \vdots \\ \transpose{v_n} \\
%\end{bmatrix}
%\\
\end{align*}
%Embeded in that is the same "as-close-to" identity as calculated above.

Writing $D_{r,r} = [\delta_{ij}\sigma_i]_{ij}$, we have:

\begin{equation}\label{eqn:VIrVt}
V \Sigma^{+} \Sigma \transpose{V} 
=
\begin{bmatrix}
\frac{v_1}{\sigma_1} & \frac{v_2}{\sigma_2} & \cdots & \frac{v_r}{\sigma_2} & 0 & \cdots & 0 
\end{bmatrix}
\begin{bmatrix}
\transpose{v_1}\sigma_1 \\ \transpose{v_2}\sigma_2 \\ \vdots \\ \transpose{v_r}\sigma_r \\ 0 \\ \vdots \\ 0 \\
\end{bmatrix}
\end{equation}

Considering this as the product of block matrices we have a product here of the form

\[
\begin{bmatrix}
A_{n,r} & 0_{n,n-r}
\end{bmatrix}
\begin{bmatrix}
B_{r,n} \\ 0_{n-r,n}
\end{bmatrix}
=
\begin{bmatrix}
A_{n,r} B_{r,n} + 0_{n,n-r} 0_{n-r,n}
\end{bmatrix}
=
\begin{bmatrix}
A_{n,r} B_{r,n} + 0_{n,n}
\end{bmatrix}
=
\begin{bmatrix}
A_{n,r} B_{r,n}
\end{bmatrix}
\]

Thus we can strip the block zero matrices from equation \ref{eqn:VIrVt} and write

\begin{equation}\label{eqn:pseudoinversetimesmatrix}
A^{+} A =
V \Sigma^{+} \Sigma \transpose{V} 
=
\begin{bmatrix}
\frac{v_1}{\sigma_1} & \frac{v_2}{\sigma_2} & \cdots & \frac{v_r}{\sigma_2} 
\end{bmatrix}
\begin{bmatrix}
\transpose{v_1}\sigma_1 \\ \transpose{v_2}\sigma_2 \\ \vdots \\ \transpose{v_r}\sigma_r 
\end{bmatrix}
\end{equation}

Eliminating the $\sigma$ terms we have:

\begin{equation}\label{eqn:pseudoinversetimesmatrixsum}
A^{+} A =
\begin{bmatrix}
\sum_{k=1}^r {v_k}\transpose{v_k}
\end{bmatrix}
=
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r 
\end{bmatrix}
\begin{bmatrix}
\transpose{v_1} \\ \transpose{v_2} \\ \vdots \\ \transpose{v_r} 
\end{bmatrix}
\end{equation}

We previously calculated a left inverse using the projection matrix associated with a full column rank matrix.  For this product to have the properties of a
left acting inverse we also expect it to be a projection.
Let's disgress
slightly before looking at whether equation
\ref{eqn:pseudoinversetimesmatrixsum} satisifies this expectation.

\subsection{ Numerical expansion of left pseudoscalar matrix with matrix.}

Numerically expanding the projection matrix $A (\transpose{A} A)^{-1}\transpose{A}$ isn't something
that we want to do (it will get messy with the matrix inversion required for the center product).
However, in the form of equation \ref{eqn:pseudoinversetimesmatrix} this is not too hard.

Let's do this to get a feel for things.

\subsubsection{ \R{4} plane projection example. }

Take a simple projection onto the plane spanned by the following two orthonormal vectors

\[
v_1 = 
\frac{\sqrt{2}}{4}
\begin{bmatrix}
1 \\
2 \\
\sqrt{3} \\
0 \\
\end{bmatrix}
\]

\[
v_2 = 
\frac{\sqrt{2}}{4}
\begin{bmatrix}
-\sqrt{3} \\
0 \\
1 \\
2 \\
\end{bmatrix}
\]

Now form what we believe to be the projection matrix:

\[
P =
\begin{bmatrix}
v_1 & v_2 \\
\end{bmatrix}
\begin{bmatrix}
\transpose{v_1} \\
\transpose{v_2} \\
\end{bmatrix}
=
\inv{8}
\begin{bmatrix}
1 & -\sqrt{3} \\
2 & 0 \\
\sqrt{3} & 1 \\
0 & 2 \\
\end{bmatrix}
\begin{bmatrix}
1 & 2 & \sqrt{3} & 0 \\
-\sqrt{3} & 0 & 1 & 2 \\
\end{bmatrix}
\]
\[
\implies
P =
\inv{8}
\begin{bmatrix}
4 & 2 & 0 & -2\sqrt{3} \\
2 & 4 & 2\sqrt{3} & 0 \\
0 & 2\sqrt{3} & 4 & 2 \\
-2\sqrt{3} & 0 & 2 & 4 \\
\end{bmatrix}
=
\begin{bmatrix}
1/2 & 1/4 & 0 & -\sqrt{3}/4 \\
1/4 & 1/2 & \sqrt{3}/4 & 0 \\
0 & \sqrt{3}/4 & 1/2 & 1/4 \\
-\sqrt{3}/4 & 0 & 1/4 & 1/2 \\
\end{bmatrix}
\]

What can be said about this just by looking at the matrix itself?

\begin{enumerate}
\item
One can verify by inspection that $P v_1 = v_1$ and $P v_2 = v_2$, so this
is has at least one of the expected properties of a projection matrix.
We also expect that $P x = 0$ for any $x \in N(\transpose{V})$, so if
it has that property too we can call it the projection matrix for the subspace
spanned by $v_1$, and $v_2$.  We also assume that this is the projection
matrix for $A$ (we haven't yet shown any explicit
relationship between this first
$r$ column vectors in the matrix $V$ and the original matrix $A$).

\item
It is symmetric.  For a full column rank matrix $A$ we know to expect this
of the associated projection matrix from the
equation $P = A \inv{\transpose{A}A} \transpose{A}$.  For that we can see by 
inspection that $\transpose{P} = P$.

\item
In this particular case columns 2,4 and columns 1,3 are each pairs of
perpendicular vectors.  Is something like this to be expected in general for
projection matrices?
\end{enumerate}

\section{ Return to analytic treatment. }

\begin{figure}[htp]
\centering
\includegraphics[totalheight=0.4\textheight]{visualize_subspace_projection}
\caption{Visualizing projection onto a subspace.}\label{fig:Projection_subspace}
\end{figure}

We can geometrically visualize the projection problem 
as in figure \ref{fig:Projection_subspace}.  Here
the subspace can be pictured
as a plane containing a set of mutually perpendicular basis vectors, as if
one has visually projected all the higher dimensional vectors onto a plane.

For a vector $x$ that contains some part not in the space we want to find
the component in the space $p$, or characterize the projection operation
that produces this vector, and also find the space of vectors that lie
perpendicular to the space.


\section{ Proof of omitted details and auxiliary stuff. }

\subsection{ That we can remove parenthesis to form projection matrix in line projection equation. }

Remove the parenthesis in some of these expressions may not always be correct, so it is worth demonstrating that this is okay as
done to calculate the projection matrix $P$ in 
equation \ref{eqn:projectmatrixline}.
We only need to look at the numerator since the denominator is a scalar in this case.

\begin{align*}
(a \transpose{a}) b
&= [ a_i a_j ]_{ij} [b_i]_i \\
&= 
{\begin{bmatrix}
\sum_k a_i a_k b_k
\end{bmatrix}
}_i \\
&= 
{\begin{bmatrix}
a_i \sum_k a_k b_k
\end{bmatrix}
}_i \\
&= [ a_i ]_i \transpose{a} b \\
&= a (\transpose{a} b) \\
\end{align*}

\subsection{ Full column rank implies $\transpose{A}A$ invertability. }

Now, if $A = [a_i]_i$, 
the matrix $\transpose{A}A$ is

\[
\begin{bmatrix}
{a_1} \cdot {a_1} & {a_1} \cdot {a_2} & \hdots \\
{a_2} \cdot {a_1} & {a_2} \cdot {a_2} & \hdots \\
\vdots & & \\
\end{bmatrix}.
\]

This is an invertible matrix provided $\{a_i\}_i$ is a linearly independent set of vectors.
I'm not sure how to show that this is true with just matrix algebra, but one can recognize the determinant of this matrix as a GA dot product of a k-blade:

\[
(-1)^{k(k-1)/2} (a_1 \wedge \cdots \wedge a_k) \cdot (a_1 \wedge \cdots \wedge a_k).
\]

Linear independence means that this wedge product is non-zero, and therefore the dot product, and original determinant is also non-zero.

\subsection{ Any invertible scaling of column space basis vectors does not change the projection }

Suppose that one introduces an alternate basis for the column space

\[
v_i = \sum \alpha_{ik} u_k
\]

This can be expressed in matrix form as:

\[
V = U E
\]

or

\[
U E^{-1} = V
\]

We should expect that the projection onto the plane expressed with this alternate basis should be identical to the original.  Verification
is straightforward:

\begin{align*}
\Proj_V 
&= V \left(\transpose{V} V\right)^{-1} \transpose{V} \\
&= \left(U E^{-1}\right) \left(\transpose{\left(U E^{-1}\right)} \left(U E^{-1}\right)\right)^{-1} \transpose{\left(U E^{-1}\right)} \\
&= \left(U E^{-1}\right) \left( \transpose{E^{-1}} \transpose{U} U E^{-1}\right)^{-1} \transpose{\left(U E^{-1}\right)} \\
&= \left(U E^{-1}\right) E \left( \transpose{U}U\right)^{-1} \transpose{E} \transpose{\left(U E^{-1}\right)} \\
&= U \left( \transpose{U}U\right)^{-1} \transpose{E} \transpose{E^{-1}} \transpose{U} \\
&= U \left( \transpose{U}U\right)^{-1} \transpose{U} \\
&= \Proj_U
\end{align*}

\end{document}               % End of document.
