\documentclass{article}      % Specifies the document class


\input{../peeters_macros.tex}
%\DeclareMathOperator{\TextTranspose}{T}
\DeclareMathOperator{\rank}{rank}
%\newcommand{\transpose}[1]{{{#1}^{\TextTranspose}}}
\newcommand{\transpose}[1]{{{#1}^{\text{T}}}}

\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package. 
\usepackage{latexsym,amssymb,epsf} % don't remember if these are
   % needed, but their inclusion can't do any damage


%
% The real thing:
%

                             % The preamble begins here.
\title{ Matrix review. } % Declares the document's title.
\author{Peeter Joot}         % Declares the author's name.
\date{ April 11, 2008 }        % Deleting this command produces today's date.

\begin{document}             % End of preamble and beginning of text.

\maketitle{}


\section{ Motivation. }


My initial intention for these notes was to 

get a feel for the similarities and differences between GA and matrix approaches to solution of projection.  Attempting to
write up that comparison
I found gaps in my understanding of the matrix algebra.  In particular the topic of projection as well as the related
ideas of pseudoinverses and SVD were not adequately covered in my university courses, nor my texts from those courses.
Here is my attempt to write up what I understand of these subjects and explore the gaps in my knowledge.

Particularily helpful was 
Gilbert Strang's excellent MIT lecture on subspace projection (available on the MIT opencourse website).
Much of the notes below are probably detailed in his course textbook, which I don't have.  However, 
if I can't explain the ideas to myself, or write them up in a way that I feel would explain
to others, then I obviously don't understand them sufficiently.


\section{ Subspace projection in matrix notation. }


\subsection{ Projection onto line. }


\begin{figure}[htp]

\centering
\includegraphics[totalheight=0.4\textheight]{Projection_line}
\caption{Projection onto line.}\label{fig:Projection_line}
\end{figure}

The simplest sort of projection to compute is projection onto a line.  Given a direction vector $b$, and a line with direction vector $u$
as in figure \ref{fig:Projection_line}.

The projection onto $u$ is some value:

\[
p = \alpha u
\]

and we can write

\[
b = p + e
\]

where e is the component perpendicular to the line $u$.

Expressed in terms of the dot product this relationship is described by:

\[
(b - p) \cdot a = 0
\]

Or, 

\[
b \cdot a = \alpha a \cdot a
\]

and solving for $\alpha$ and substituting we have:

\begin{equation}
p = a \frac{a \cdot b}{a \cdot a}
\end{equation}

In matrix notation that is:

\begin{equation}
p = a \left(\frac{\transpose{a} b}{\transpose{a} a} \right)
\end{equation}

Following Gilbert Strang's MIT lecture on subspace projection, the parenthesis can be moved to directly express this as a projection matrix operating on b.

\begin{equation}\label{eqn:projectmatrixline}
p = \left(\frac{a \transpose{a}}{\transpose{a} a}\right) b = P b
\end{equation}


\subsection{ Projection onto plane (or subspace). }



\begin{figure}[htp]
\centering
\includegraphics[totalheight=0.4\textheight]{Projection_plane}
\caption{Projection onto plane.}\label{fig:Projection_plane}
\end{figure}

Calculation of the projection matrix to project onto a plane is similar.  The variables to solve for are $p$, and $e$ in as
figure \ref{fig:Projection_plane}.

For projection onto a plane (or hyperplane) the idea is the same, splitting the vector into a component in the plane and an perpendicular component.
Since the
idea is the same for any dimensional subspace, explicit specification of the summation range is omitted here so the result is good for higher dimensional
subspaces as well as the plane:

\[
b - p = e
\]
\[
p = \sum \alpha_i u_i
\]

however, we get a set of equations, one for each direction vector in the plane

\[
(b - p) \cdot u_i = 0
\]

Expanding $p$ explicitly and rearranging we have the following set of equations:

\[
b \cdot u_i = (\sum_s \alpha_s u_s) \cdot u_i
\]

putting this in matrix form

\begin{align*}
[b \cdot u_i]_i 
&= 
{
\begin{bmatrix}
(\sum_s \alpha u_s) \cdot u_i
\end{bmatrix}
}_i \\
\end{align*}

Writing $U = 
\begin{bmatrix}
u_1 & u_2 & \cdots 
\end{bmatrix}$
\begin{align*}
\begin{bmatrix}
\transpose{u_1} \\
\transpose{u_2} \\
\vdots 
\end{bmatrix}
b
&= 
\begin{bmatrix}
(\sum_s \alpha_s u_s) \cdot u_i
\end{bmatrix} \\
&= 
{
\begin{bmatrix}
u_i \cdot u_j
\end{bmatrix}
}_{ij}
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\vdots \\
\end{bmatrix} \\
\end{align*}

Solving for the vector of unknown coefficients $\alpha = [\alpha_i]_i$ we have

\[
\alpha
=
{{
\begin{bmatrix}
u_i \cdot u_j
\end{bmatrix}
}_{ij}}^{-1}
\transpose{U} b
\]

And 

\begin{equation}
p = U \alpha = U
{{
\begin{bmatrix}
u_i \cdot u_j

\end{bmatrix}
}_{ij}}^{-1}
\transpose{U} b
\end{equation}

However, this matrix in the middle is just $\transpose{U} U$:

\begin{align*}
\begin{bmatrix}
\transpose{u_1} \\
\transpose{u_2} \\
\vdots \\
\end{bmatrix}
\begin{bmatrix}
{u_1} & {u_2} & \hdots \\
\end{bmatrix}
&=
\begin{bmatrix}
\transpose{u_1} {u_1} & \transpose{u_1} {u_2} & \hdots \\
\transpose{u_2} {u_1} & \transpose{u_2} {u_2} & \hdots \\
\vdots & & \\
\end{bmatrix} \\
&=
{
\begin{bmatrix}
\transpose{u_i} {u_j}
\end{bmatrix}
}_{ij} \\
&=
{
\begin{bmatrix}
{u_i} \cdot {u_j}
\end{bmatrix}
}_{ij} \\
\end{align*}

This provides the final result:

\begin{equation}\label{eqn:projectiongeneralmatrix}
\Proj_{U}\left(b\right) = U (\transpose{U} U)^{-1} \transpose{U} b
\end{equation}

\subsection{ Simplifying case.  Orthonormal basis for column space. }

To evaluate equation \ref{eqn:projectiongeneralmatrix} we need only full column rank for $U$, but this will be messy in general due to the matrix inversion required for the center product.  That can be avoided by picking an orthonormal basis for the vector space that we are projecting on.  With an orthonormal column basis that 
central product term to invert is:

\[
\transpose{U} U = [ \transpose{u_i} u_j ]_{ij} = [ \delta_{ij} ]_{ij} = I_{r,r}
\]

Therefore, the projection matrix can be expressed using the two exterior terms alone:

\begin{equation}\label{eqn:projOrthonormal}
\Proj_U = U (\transpose{U} U)^{-1} \transpose{U} = U \transpose{U}
\end{equation}

\subsection{ Numerical expansion of left pseudoscalar matrix with matrix.}

Numerically expanding the projection matrix $A (\transpose{A} A)^{-1}\transpose{A}$ isn't something
that we want to do, but the simpler projection matrix of equation
\ref{eqn:projOrthonormal} that we get with an orthonormal basis makes this not so daunting.

Let's do this to get a feel for things.

\subsubsection{ \R{4} plane projection example. }

Take a simple projection onto the plane spanned by the following two orthonormal vectors

\[
u_1 = 
\frac{\sqrt{2}}{4}
\begin{bmatrix}
1 \\
2 \\
\sqrt{3} \\
0 \\
\end{bmatrix}
\]

\[
u_2 = 
\frac{\sqrt{2}}{4}
\begin{bmatrix}
-\sqrt{3} \\
0 \\
1 \\
2 \\
\end{bmatrix}
\]

Thus the projection matrix is:

\[
P =
\begin{bmatrix}
u_1 & u_2 \\
\end{bmatrix}
\begin{bmatrix}
\transpose{u_1} \\
\transpose{u_2} \\
\end{bmatrix}
=
\inv{8}
\begin{bmatrix}
1 & -\sqrt{3} \\
2 & 0 \\
\sqrt{3} & 1 \\
0 & 2 \\
\end{bmatrix}
\begin{bmatrix}
1 & 2 & \sqrt{3} & 0 \\
-\sqrt{3} & 0 & 1 & 2 \\
\end{bmatrix}
\]
\[
\implies
P =
\inv{8}
\begin{bmatrix}
4 & 2 & 0 & -2\sqrt{3} \\
2 & 4 & 2\sqrt{3} & 0 \\
0 & 2\sqrt{3} & 4 & 2 \\
-2\sqrt{3} & 0 & 2 & 4 \\
\end{bmatrix}
=
\begin{bmatrix}
1/2 & 1/4 & 0 & -\sqrt{3}/4 \\
1/4 & 1/2 & \sqrt{3}/4 & 0 \\
0 & \sqrt{3}/4 & 1/2 & 1/4 \\
-\sqrt{3}/4 & 0 & 1/4 & 1/2 \\
\end{bmatrix}
\]

What can be said about this just by looking at the matrix itself?

\begin{enumerate}
\item
One can verify by inspection that $P u_1 = u_1$ and $P u_2 = u_2$.  This is what we expected
so this validates all the math performed so far.  Good!

%, so this
%is has at least one of the expected properties of a projection matrix.
%We also expect that $P x = 0$ for any $x \in N(\transpose{V})$, so if
%it has that property too we can call it the projection matrix for the subspace
%spanned by $u_1$, and $u_2$.  We also assume that this is the projection
%matrix for $A$ (we haven't yet shown any explicit
%relationship between this first
%$r$ column vectors in the matrix $V$ and the original matrix $A$).

\item
It is symmetric.  Analytically, we know to expect this, since for a 
a full column rank matrix $A$ the transpose of the projection matrix is:

\[
\transpose{P} = \transpose{\left(A \inv{\transpose{A}A} \transpose{A}\right)} = P.
\]

\item
In this particular case columns 2,4 and columns 1,3 are each pairs of
perpendicular vectors.  Is something like this to be expected in general for
projection matrices?

\item
We expect this to be a rank two matrix, so the null space has dimension two.  This can be verified.

\end{enumerate}

\subsection{ Return to analytic treatment. }

Let's look at the matrix for projection onto an orthonormal basis in a bit more detail.  This simpler form allows for
some observations that are a bit harder in the general form.

Suppose we have a vector $n$ that is perpendicular to all the orthonormal vectors $u_i$ that span the subspace.  We can then write:

\[
u_i \cdot n = 0
\]

Or,
\[
\transpose{u_i} n = 0
\]

In block matrix form for all $u_i$ that is:

\[
[\transpose{u_i} n]_i = [\transpose{u_i}]_i n = \transpose{U} n = 0
\]

This is all we need to verify that our projection matrix indeed produces a zero for any vector completely outside of the subspace:

\[
\Proj_U(n) = U (\transpose{U} n) = U 0 = 0
\]

Now we have seen numerically that $U \transpose{U}$ is not an identity matrix despite 
operating as one on any vector that lies completely in the subspace.
%Although this matrix may 
%have blocks of identity matrixes along the diagonal in some cases.

Having seen the action of this matrix on vectors in the null space, we can now
directly examine the action of this matrix on any vector that lies in the
span of the set $\{u_i\}$.  By linearity it is sufficient to do this calcuation 
for a particular $u_i$:

\begin{align*}
U \transpose{U} u_i
&=
U
\begin{bmatrix}
\transpose{u_1} u_i \\
\transpose{u_2} u_i \\
\vdots \\
\transpose{u_r} u_i \\
\end{bmatrix}
\\
&=
\begin{bmatrix}
{u_1} & {u_2} & \cdots & {u_r} \\
\end{bmatrix}
{
\begin{bmatrix}
\delta_{si}
\end{bmatrix}
}_s \\
&= \sum_{k=1}^{r} u_k \delta_{ki} \\
&= u_i \\
\end{align*}

This now completes the validation of the properties of this matrix (in it's simpler form with an orthonormal basis for the subspace).




%\begin{figure}[htp]
%\centering
%\includegraphics[totalheight=0.4\textheight]{visualize_subspace_projection}
%\caption{Visualizing projection onto a subspace.}\label{fig:Projection_subspace}
%\end{figure}
%
%We can geometrically visualize the projection problem 
%
%as in figure \ref{fig:Projection_subspace}.  Here
%the subspace can be pictured
%as a plane containing a set of mutually perpendicular basis vectors, as if
%one has visually projected all the higher dimensional vectors onto a plane.
%
%For a vector $x$ that contains some part not in the space we want to find
%the component in the space $p$, or characterize the projection operation
%that produces this vector, and also find the space of vectors that lie
%perpendicular to the space.




%This is why we say that $n$ is in the null space of $\transpose{U}$,
%$N(\transpose{U})$ not $N(U)$.  One perhaps could say this is in the null
%or perpendicular space of the set $\{u_i\}$, but our use of columns as
%vectors coupled with the Euclianian norm requires transposition to express
%the same null space concept for a matrix of column vectors.
%
%Since $\transpose{U} n = 0$ for any $n \in N(U)$ we also have


















%\subsection{ Application of projection as left pseudoinverse (ie: linear fitting). }
%
%%We have shown that the left pseudoinverse product with the matrix can
%%be expressed as a projection matrix (sum of the projection matrices
%%associated with a set of orthonormal vectors)
%
%%\begin{equation}\label{eqn:pseudoprojmatsum}
%%A^{+} A =
%%\sum_{k=1}^r {u_k}\transpose{u_k}
%%\end{equation}
%%
%
%%(note this is a different ``$V$'' than the $V$ in $A = U \Sigma \transpose{V}$ since it only includes the first $r$ columns).
%%This allows us to write the matrix of equation \ref{eqn:pseudoprojmatsum} as
%
%%\begin{equation}
%%A^{+} A = V \transpose{V}
%%\end{equation}
%
%
%Equation \ref{eqn:projectiongeneralmatrix} provides us a way to find best solutions to general equations of the form:
%
%
%\[
%A x = b
%\]
%
%Here $A$ is the matrix of a linear transformation, $A : \mathbb{R}^k \rightarrow \mathbb{R}^n$, for some $k<n$.
%By ``best solutions'' here, we give this the geometrical meaning, namely, the solution matching the projection of $b$ onto the space.
%
%If b is not completely in the column space $C(A)$ of $A$, this can have no solution.  However, writing
%
%\[
%b = \Proj_A(b) + b_\perp
%\]
%
%as the components of $b$ in $C(A)$ and not in $C(A)$ respectively we can at least solve the reduced equation for $\hat{x}$:
%
%
%\begin{equation}\label{eqn:reducedinverseproblem}
%A \hat{x} = \Proj_A(b)
%\end{equation}
%
%
%This will be possible even in circumstances that the original equation had no solution.  Specifically, the vector b when projected onto the plane can be expressed as some
%linear combination of the columns of $A$ (a basis for the subspace).
%
%Substuition of our projection result into equation \ref{eqn:reducedinverseproblem} yields:
%
%\begin{align*}
%A \hat{x} 
%&= \Proj_{A}\left(b\right) = A (\transpose{A} A)^{-1} \transpose{A} b
%\end{align*}
%
%The simplest case here is when $A$ is of full column rank since one can pre-multiply this complete equation by $\transpose{A}$ without any possibility of nulling
%$A \hat{x}$.
%
%\begin{align*}
%\transpose{A} A \hat{x} 
%&= \transpose{A} A (\transpose{A} A)^{-1} \transpose{A} b \\
%&= \transpose{A} b \\
%\end{align*}
%
%Thus our best fit vector is
%
%\begin{equation}
%\hat{x} 
%= (\transpose{A} A)^{-1} \transpose{A} b
%\end{equation}
%
%Another way to view this is for any vector $x$ that is not in the null space $N(A)$, then the matrix:
%
%\begin{equation}
%A^{+}= (\transpose{A} A)^{-1} \transpose{A}
%\end{equation}
%
%has the action of a left inverse for any full column rank matrix $A$.  Thus when there is a solution to:
%
%\begin{equation}
%A x = b.
%\end{equation}
%
%It can be obtained by pre-multiplication using this "left" inverse.
%
%\begin{equation}
%A^{+} A x = x = A^{+} b
%\end{equation}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%\section{ SVD connection. }
%
%
%SVT decomposition is an factoring of $A \in M^{m \times n}$ with orthonormal matrices $U \in M^{m \times m}$
%
%and $V \in M^{n \times n} $ producing the following form:
%
%\[
%A = U \Sigma \transpose{V}
%\]
%
%Sigma has the form:
%
%\[
%\Sigma = 
%\begin{bmatrix}
%D_{r,r} & 0_{r,n-r} \\
%0_{m-r,r} & 0_{m-r,n-r} \\
%\end{bmatrix}
%\]
%
%where $r = \rank(A)$, and $D$ is a diagonal matrix with the root of the (positive) eigenvalues of $\transpose{A}A$.
%
%This provides a generalized spectral decomposition and similarity that applies to both non-square matrices and matrices not otherwise diagonalizable
%(ie: square matrix with similarity to a Jordon form matrix).  Given this decomposition we can write:
%
%\[
%\Sigma = \transpose{U} A V
%\]
%
%If one were to ask the question of what is the closest that one could get to inverting such a matrix.  It's pretty clear that the closest one could get to
%identity will be with multiplication of a $\Sigma^{+}$ of the following form:
%
%\[
%\Sigma^{+} \Sigma
%=
%\begin{bmatrix}
%(D_{r,r})^{-1} & 0_{r,m-r} \\
%0_{n-r,r} & 0_{n-r,m-r} \\
%\end{bmatrix}
%\begin{bmatrix}
%D_{r,r} & 0_{r,n-r} \\
%0_{m-r,r} & 0_{m-r,n-r} \\
%\end{bmatrix}
%=
%\begin{bmatrix}
%I_{r,r} & 0_{r,n-r} \\
%0_{n-r,r} & 0_{n-r,n-r} \\
%\end{bmatrix}
%\]
%
%For a right pseudoinverse we have a similar result:
%
%\[
%\Sigma
%\Sigma^{+}
%=
%\begin{bmatrix}
%D_{r,r} & 0_{r,n-r} \\
%0_{m-r,r} & 0_{m-r,n-r} \\
%\end{bmatrix}
%\begin{bmatrix}
%(D_{r,r})^{-1} & 0_{r,m-r} \\
%0_{n-r,r} & 0_{n-r,m-r} \\
%\end{bmatrix}
%=
%\begin{bmatrix}
%I_{r,r} & 0_{r,m-r} \\
%0_{m-r,r} & 0_{m-r,m-r} \\
%\end{bmatrix}
%\]
%
%With either of these one can define a corresponding pseudoinverse (left or right) as:
%
%\begin{equation}
%A^{+} = V \Sigma^{+} \transpose{U}
%\end{equation}
%
%This is a logical definition, but how close is it to the projective
%left inverse we calculated above in the case where $A$ is not of full column 
%rank?
%
%Multiplication gives: 
%
%\begin{align*}
%A^{+} A 
%&= V \Sigma^{+} \transpose{U} U \Sigma \transpose{V} \\
%&= V \Sigma^{+} \Sigma \transpose{V} \\
%&=
%\begin{bmatrix}
%v_1 & v_2 & \cdots & v_r & v_{r+1} & \cdots & v_n \\
%\end{bmatrix}
%\begin{bmatrix}
%(D_{r,r})^{-1} & 0_{r,m-r} \\
%0_{n-r,r} & 0_{n-r,m-r} \\
%\end{bmatrix}
%\begin{bmatrix}
%D_{r,r} & 0_{r,n-r} \\
%0_{m-r,r} & 0_{m-r,n-r} \\
%\end{bmatrix}
%\begin{bmatrix}
%\transpose{v_1} \\ \transpose{v_2} \\ \vdots \\ \transpose{v_r} \\ \transpose{v_{r+1}} \\ \vdots \\ \transpose{v_n} \\
%\end{bmatrix}
%\end{align*}
%%Embeded in that is the same "as-close-to" identity as calculated above.
%
%Writing $D_{r,r} = [\delta_{ij}\sigma_i]_{ij}$, we have:
%
%\begin{equation}\label{eqn:VIrVt}
%V \Sigma^{+} \Sigma \transpose{V} 
%=
%\begin{bmatrix}
%\frac{v_1}{\sigma_1} & \frac{v_2}{\sigma_2} & \cdots & \frac{v_r}{\sigma_2} & 0 & \cdots & 0 
%\end{bmatrix}
%\begin{bmatrix}
%\transpose{v_1}\sigma_1 \\ \transpose{v_2}\sigma_2 \\ \vdots \\ \transpose{v_r}\sigma_r \\ 0 \\ \vdots \\ 0 \\
%\end{bmatrix}
%\end{equation}
%
%Considering this as the product of block matrices we have a product here of the form
%
%\[
%\begin{bmatrix}
%A_{n,r} & 0_{n,n-r}
%\end{bmatrix}
%\begin{bmatrix}
%B_{r,n} \\ 0_{n-r,n}
%\end{bmatrix}
%=
%\begin{bmatrix}
%A_{n,r} B_{r,n} + 0_{n,n-r} 0_{n-r,n}
%\end{bmatrix}
%=
%\begin{bmatrix}
%A_{n,r} B_{r,n} + 0_{n,n}
%\end{bmatrix}
%=
%\begin{bmatrix}
%A_{n,r} B_{r,n}
%\end{bmatrix}
%\]
%
%Thus we can strip the block zero matrices from equation \ref{eqn:VIrVt} and write
%
%\begin{equation}\label{eqn:pseudoinversetimesmatrix}
%A^{+} A =
%V \Sigma^{+} \Sigma \transpose{V} 
%=
%\begin{bmatrix}
%\frac{v_1}{\sigma_1} & \frac{v_2}{\sigma_2} & \cdots & \frac{v_r}{\sigma_2} 
%\end{bmatrix}
%\begin{bmatrix}
%\transpose{v_1}\sigma_1 \\ \transpose{v_2}\sigma_2 \\ \vdots \\ \transpose{v_r}\sigma_r 
%\end{bmatrix}
%\end{equation}
%
%Eliminating the $\sigma$ terms we have:
%
%\begin{equation}\label{eqn:pseudoinversetimesmatrixsum}
%A^{+} A =
%\begin{bmatrix}
%\sum_{k=1}^r {v_k}\transpose{v_k}
%\end{bmatrix}
%=
%\begin{bmatrix}
%v_1 & v_2 & \cdots & v_r 
%\end{bmatrix}
%\begin{bmatrix}
%\transpose{v_1} \\ \transpose{v_2} \\ \vdots \\ \transpose{v_r} 
%\end{bmatrix}
%\end{equation}
%
%We previously calculated a left inverse using the projection matrix associated with a full column rank matrix.  For this product to have the properties of a
%left acting inverse we also expect it to be a projection.
%Let's disgress
%slightly before looking at whether equation
%\ref{eqn:pseudoinversetimesmatrixsum} satisifies this expectation.
%
%
%
%\subsection{ Correlating the SVD derived projection matrix back to $A$. }
%
%
%We now have to show that this is also the projection matrix associated
%
%with the columns of the 
%original matrix that we have an SVD factorization for
%
%\[
%A = U \Sigma \transpose{V}
%\]
%
%Once we show this, then we have also demonstrated that the first $r$ 
%(orthonormal) column vectors in the matrix $V$ of this decomposition
%are a basis for the column space of $A$ itself.  Note that we are
%switching back to the original definition of $V \in M^{n,n}$ here, and
%not the $V \in M^{n,r}$ of equation \ref{eqn:projOrthonormal}.
%
%






\section{ Proof of omitted details and auxiliary stuff. }

\subsection{ That we can remove parenthesis to form projection matrix in line projection equation. }

Remove the parenthesis in some of these expressions may not always be correct, so it is worth demonstrating that this is okay as
done to calculate the projection matrix $P$ in 
equation \ref{eqn:projectmatrixline}.
We only need to look at the numerator since the denominator is a scalar in this case.

\begin{align*}
(a \transpose{a}) b
&= [ a_i a_j ]_{ij} [b_i]_i \\
&= 
{\begin{bmatrix}
\sum_k a_i a_k b_k
\end{bmatrix}
}_i \\
&= 
{\begin{bmatrix}
a_i \sum_k a_k b_k
\end{bmatrix}
}_i \\
&= [ a_i ]_i \transpose{a} b \\
&= a (\transpose{a} b) \\
\end{align*}

\subsection{ Full column rank implies $\transpose{A}A$ invertability. }


Now, if $A = [a_i]_i$, 
the matrix $\transpose{A}A$ is

\[
\begin{bmatrix}
{a_1} \cdot {a_1} & {a_1} \cdot {a_2} & \hdots \\
{a_2} \cdot {a_1} & {a_2} \cdot {a_2} & \hdots \\
\vdots & & \\
\end{bmatrix}.
\]

This is an invertible matrix provided $\{a_i\}_i$ is a linearly independent set of vectors.
This could be proven if one could show that the 
determinant of this matrix was non-zero.
I'm not sure how to show that this is true with just matrix algebra, however
one can identify, with an adjustment of sign for reversion, as 
the GA dot product of a k-blade:

\[
(-1)^{k(k-1)/2} (a_1 \wedge \cdots \wedge a_k) \cdot (a_1 \wedge \cdots \wedge a_k).
\]

Linear independence means that this wedge product is non-zero, and therefore the dot product, and thus original determinant is also non-zero.


\subsection{ Any invertible scaling of column space basis vectors does not change the projection }


Suppose that one introduces an alternate basis for the column space


\[
v_i = \sum \alpha_{ik} u_k
\]

This can be expressed in matrix form as:

\[
V = U E
\]

or

\[
U E^{-1} = V
\]

We should expect that the projection onto the plane expressed with this alternate basis should be identical to the original.  Verification
is straightforward:

\begin{align*}
\Proj_V 
&= V \left(\transpose{V} V\right)^{-1} \transpose{V} \\
&= \left(U E^{-1}\right) \left(\transpose{\left(U E^{-1}\right)} \left(U E^{-1}\right)\right)^{-1} \transpose{\left(U E^{-1}\right)} \\
&= \left(U E^{-1}\right) \left( \transpose{E^{-1}} \transpose{U} U E^{-1}\right)^{-1} \transpose{\left(U E^{-1}\right)} \\
&= \left(U E^{-1}\right) E \left( \transpose{U}U\right)^{-1} \transpose{E} \transpose{\left(U E^{-1}\right)} \\
&= U \left( \transpose{U}U\right)^{-1} \transpose{E} \transpose{E^{-1}} \transpose{U} \\
&= U \left( \transpose{U}U\right)^{-1} \transpose{U} \\
&= \Proj_U
\end{align*}

\end{document}               % End of document.
