\documentclass{article}

\input{../peeters_macros.tex}
\input{../peeters_macros2.tex}
\newcommand{\PDSq}[2]{\frac{\partial^2 {#2}}{\partial {#1}^2}}
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator{\PV}{PV}
\newcommand{\FF}[0]{\mathcal{F}}
\newcommand{\IIinf}[0]{ \int_{-\infty}^\infty }

\usepackage[bookmarks=true]{hyperref}

\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package.
\usepackage{latexsym,amssymb,epsf} % don't remember if these are
   % needed, but their inclusion can't do any damage


\title{ Fourier transform solutions to Maxwell's equation. }
\author{Peeter Joot}
\date{ Jan 29, 2009.  Last Revision: $Date: 2009/02/01 16:41:14 $ }

\begin{document}

\maketitle{}

\tableofcontents
\section{ Motivation. }

In \cite{PJwaveFourier} a Green's function solution to the homogeneous
wave equation

\begin{align}\label{eqn:wave}
\left(\inv{v^2} \partial_{tt} -\partial_{xx} -\partial_{yy} -\partial_{zz} \right)\psi = 0
\end{align}

was found to be

\begin{align}\label{eqn:greensSolution3d}
{\psi}(x,y,z,t) &= \IIinf \phi(u,w,r) G( x-u, y-w, z-r, t) du dw dr \\
G(x,y,z,t) &= \inv{({2\pi})^3} \IIinf \exp\left( i k x + i m y + i n z \pm i \sqrt{k^2 + m^2 + n^2} v t \right) dk dm dn
\end{align}

The aim of this set of notes is to explore the same ideas to the forced wave
equations for the four vector potentials of the Lorentz gauge Maxwell equation.

Such solutions can be used to find the Faraday bivector or its associated
tensor components.

Note that the specific form of the Fourier transform used in these notes continues to be

\begin{align}
%\hat{f}(\Bk) &= \inv{(\sqrt{2\pi})^n} \IIinf f(\Bx) \exp\left( -i k_j x^j \right) d^n x \\
%{f}(\Bx) &= \inv{(\sqrt{2\pi})^n} \IIinf \hat{f}(\Bk) \exp\left( i k_j x^j \right) d^n k
\hat{f}(\Bk) &= \inv{(\sqrt{2\pi})^n} \IIinf f(\Bx) \exp\left( -i \Bk \cdot \Bx \right) d^n x \\
{f}(\Bx) &= \inv{(\sqrt{2\pi})^n} \IIinf \hat{f}(\Bk) \exp\left( i \Bk \cdot \Bx \right) d^n k
\end{align}

\section{ Forced wave equation. }

\subsection{ One dimensional case. }

A good starting point is the reduced complexity one dimensional forced
wave equation

\begin{align}
\left( \inv{v^2} \partial_{tt} -\partial_{xx} \right)\psi = \rho
\end{align}

Fourier transforming to to the wave number domain, with application of integration by parts twice (each toggling the sign of the spatial
derivative term) we have

\begin{align}
\inv{v^2}\hat{\psi}_{tt} - (-i k)^2 \hat{\psi} = \hat{\rho}
\end{align}

This leaves us with a linear differential equation of the following form to solve

\begin{align}\label{eqn:waveNumEquationToSolve}
f'' + \alpha^2 f = g
\end{align}

Out of line solution of this can be found below in equation \ref{eqn:solutionToWaveNumberDomainEquation}, where we have
$f = \hat{\psi}$, $\alpha = k v$, and $g = \hat{\rho} v^2$.  Our solution for the wave function in the wave number domain is now completely
specified

\begin{align*}
\hat{\psi}(k, t) = \frac{v}{k} \int_{u=t_0(k)}^t \hat{\rho}(u) \sin( k v (t-u) ) du
\end{align*}

Here because of the partial differentiation we have the flexibility to make the initial time a function of the wave number $k$, but it is probably
more natural to just set $t_0 = -\infty$

\begin{align*}
\hat{\psi}(k, t) = \frac{v}{k} \int_{u = -\infty}^t \hat{\rho}(k, u) \sin( k v (t-u) ) du
\end{align*}

But seeing the integral in this form suggests a change of variables $w = t-u$, which gives us our final wave function in the wave number domain with all the time
dependency removed from the integration limits

\begin{align*}
\hat{\psi}(k, t) = \frac{v}{k} \int_{w = 0}^\infty \hat{\rho}(k, t-w) \sin( k v w ) dw
\end{align*}

With this our wave function is

\begin{align*}
{\psi}(x, t)
&=
\inv{\sqrt{2\pi}} \IIinf
\left(
\frac{v}{k} \int_{w = 0}^\infty \hat{\rho}(k, t-w) \sin( k v w ) dw
%\frac{v}{k} \int_{u = -\infty}^t \hat{\rho}(u) \sin( k v (t-u) ) du
\right) \exp( i k x ) dk \\
\end{align*}


But we also have

\begin{align*}
\hat{\rho}(k,t) &= \inv{\sqrt{2\pi}} \int_{-\infty}^\infty {\rho}(x, t) \exp( -i k x ) dx
\end{align*}

Reassembling we have

%\begin{align*}
%{\psi}(x, t)
%&=
%\inv{\sqrt{2\pi}} \IIinf
%\left( \frac{v}{k} \int_{u = -\infty}^t
%\left(
%\inv{\sqrt{2\pi}} \int_{-\infty}^\infty {\rho}(x) \exp( -i u x ) dx
%\right)
%\sin( k v (t-u) ) du \right) \exp( i k x ) dk \\
%\end{align*}

%\begin{align*}
%{\psi}(x, t)
%&= 
%\inv{\sqrt{2\pi}}
%\int_{k = -\infty}^\infty
%\left(
%\frac{v}{k}
%\int_{w = 0}^\infty
%\left(
%\inv{\sqrt{2\pi}}
%\int_{y=-\infty}^\infty {\rho}(y, t-w) \exp( -i k y ) dy
%\right)
%\sin( k v w ) dw
%\right) \exp( i k x ) dk
%\end{align*}

\begin{align*}
{\psi}(x, t)
&= 
\int_{k = -\infty}^\infty
\int_{w = 0}^\infty
\int_{y=-\infty}^\infty
\frac{v}{ 2 \pi k}
{\rho}(y, t-w) 
\sin( k v w ) 
\exp( i k (x-y) ) 
dy
dw
dk
\end{align*}

Rearranging a bit we have

\begin{align}
{\psi}(x, t)
&=
%\int_{y=-\infty}^\infty
%\int_{w = 0}^\infty
%{\rho}(y, t-w) G(x -y, t) dy dw \\
%
% x-y=z
% y=x-z
% dy = -dz
% \int_y -> -\int_z
%
\int_{z=-\infty}^\infty
\int_{w = 0}^\infty
{\rho}(x-z, t-w) G(z, t) dz dw \\
G(x, w) &=
\int_{k = -\infty}^\infty
\frac{v}{2\pi k}
\sin( k v w )
\exp( i k x )
dk
%\\
\end{align}

We see that our charge density summed over all space contributes to the wave function, but it is the charge density at that spatial location as it existed at a specific previous time.
Exactly what that previous time is is a rather complex looking function of our integral kernel.

%This is
%\begin{align*}
%{\psi}(x, t)
%&=
%\IIinf {\rho}(x) G( x, t ) dx \\
%G(x, t) &=
%\int_{k=-\infty}^\infty
%\int_{u = -\infty}^t
%\frac{v}{2 k \pi}
%\sin( k v (t-u) )
%\exp( i (k-u) x )
%dk
%du
%\\
%\end{align*}

%or
%\begin{align*}
%% t-u = w
%% u = t-w
%% w(u=t) = 0
%% w(u=-\infty) = \infty
%G(x, t) &=
%\int_{k=-\infty}^\infty
%\int_{w = 0}^{\infty}
%\frac{v}{2 k \pi}
%\sin( k v w )
%\exp( i (k+w-t) x )
%dk
%dw
%\\
%\end{align*}

\subsection{ Three dimensional case. }

Now, lets move on to the 3D case that's of particular interest for electrodynamics.  Our wave equation is now of the form

\begin{align}
\left( \inv{v^2} \PDSq{t}{} -\sum_j \PDSq{x^j}{} \right)\psi = \rho
\end{align}

and our Fourier transformation produces almost the same result, but we have a wave number contribution from each of the three dimensions

\begin{align}
\inv{v^2}\hat{\psi}_{tt} + \Bk^2 \hat{\psi} = \hat{\rho}
\end{align}

Our wave number domain solution is therefore
\begin{align}\label{eqn:waveNumDomainSolution}
\hat{\psi}(\Bk, t) = \frac{v}{\Abs{\Bk}} \int_{\tau = 0}^\infty \hat{\rho}(\Bk, t-\tau) \sin( \Abs{\Bk} v \tau ) d\tau
\end{align}

But our wave number domain charge density is

\begin{align*}
\hat{\rho}(\Bk, t) &= \inv{(\sqrt{2\pi})^3} \IIinf \rho(\Bx, t) \exp\left( -i \Bk \cdot \Bx \right) d^3 x \\
\end{align*}

Our wave number domain result in terms of the charge density is therefore

%\hat{\rho}(\Bk, t-\tau) &= \inv{(\sqrt{2\pi})^3} \IIinf \rho(\Br, t-\tau) \exp\left( -i \Bk \cdot \Br \right) d^3 r \\
\begin{align*}
\hat{\psi}(\Bk, t) =
\frac{v}{\Abs{\Bk}} \int_{\tau = 0}^\infty
%\hat{\rho}(\Bk, t-\tau) &=
\left(
\inv{(\sqrt{2\pi})^3} \IIinf \rho(\Br, t-\tau) \exp\left( -i \Bk \cdot \Br \right) d^3 r
\right)
\sin( \Abs{\Bk} v \tau ) d\tau
\end{align*}

And finally inverse transforming back to the spatial domain we have a complete solution for the inhomogeneous wave equation in terms of the spatial and temporal charge density distribution

\begin{align}
{\psi}(\Bx, t)
%&= \IIinf \int_{t' = 0}^\infty \rho(\Br, t-t') G(\Bx -\Br) d^3 r dt' \\
% x'_j = x_j - r_j
% r_j = x_j - x'_j
% dr_j = -dx'_j
% III dr_1 dr_2 dr_3 = (-1)^3 III d^3 x'
&= \IIinf \int_{t' = 0}^\infty \rho(\Bx -\Bx', t-t') G(\Bx', t') d^3 x' dt' \\
G(\Bx, t)
&= \IIinf
\frac{v}{(2\pi)^3 \Abs{\Bk}}
\sin( \Abs{\Bk} v t )
\exp\left( i \Bk \cdot \Bx \right)
d^3 k
\end{align}

For computational purposes we are probably much better off using
equation \ref{eqn:waveNumDomainSolution}, however,
from an abstract point of form this expression is much prettier.

One can also see the elements of the traditional retarded time expressions for the potential hiding in there, but making that final next step to show this is actually
the case looks like it is
still a job for a different day.

\subsubsection{ Tweak this a bit to put into proper Green's function form. }

Now, it makes sense to redefine $G(\Bx,t)$ above so that we can integrate
uniformly over all space and time.  To do so we can add a unit step function
into the definition, so that $G(\Bx,t<0) = 0$.
Additionally, if we express this convolution it is slightly tidier (and consistent with the normal Green's function notation)
to put the parameter differences in the kernel term.  Such a change of variables will alter the sign of the integral limits
by a factor of $(-1)^4$, but we also have a $(-1)^4$ term from the differentials.  After making these final adjustments
we have a final variation of our integral solution

%
% y_j = x_j - x'_j
% dy_j = x_j - dx'_j
% t' = t - \tau
% dt' = t - d\tau
% 
%&= \int_x'1 \int_x'2 \int_x'3 \int_tau \rho(\Bx -\Bx', t-\tau) G(\Bx', \tau) d^3 x' d\tau \\
%
% =>
%
%&= (-1)^4 \int_y1 \int_y2 \int_y3 \int_t' \rho(\By, t') G(\Bx - \By, t - t') (-1)^4 d^3 y dt' \\
% 
% 
\begin{align}
{\psi}(\Bx, t)
&= \IIinf \rho(\Bx', t') G(\Bx - \Bx', t - t') d^3 x' dt' \\
G(\Bx, t)
&= u(t) \IIinf
\frac{v}{(2\pi)^3 \Abs{\Bk}}
\sin( \Abs{\Bk} v t )
\exp\left( i \Bk \cdot \Bx \right)
d^3 k
\end{align}

Now our inhomogenious solution is expressed nicely as the convolution of our current density over all space and time
with an integral kernel.  That integral kernel is precisely the Green's function for this forced wave equation.

This solution comes with a large number of assumptions.  Along the way we have the assumption that both our wave function
and the charge density was Fourier transformable, and that the wave number domain products were inverse transformable.  We
also had an assumption that the
wave function is sufficiently small at the limits of integration that the intermediate contributions from the 
integration by parts vanished, and finally the big assumption that we were perfectly free to interchange integration order
in an extremely ad-hoc and non-rigorous fashion!

\section{ Maxwell equation solution. }

Having now found Green's function form for the forced wave equation, we can now move to Maxwell's equation

\begin{align*}
\grad F = J/\epsilon_0 c
\end{align*}

In terms of potentials we have $F = \grad \wedge A$, and may also impose the Lorentz gauge $\grad \cdot A = 0$, to give us our four charge/current forced wave equations

\begin{align*}
\grad^2 A = J/\epsilon_0 c
\end{align*}

As scalar equations these are

\begin{align}
\left( \inv{c^2} \PDSq{t}{} -\sum_j \PDSq{x^j}{} \right) A^\mu = \frac{J^\mu}{\epsilon_0 c}
\end{align}

So, from above, also writing $x^0 = ct$, we have

\begin{align}\label{eqn:fourVectorPotentials}
{A^\mu}(\Bx, t)
&= \inv{\epsilon_0 c} \int J^\mu(x') G(x - x') d^4 x' \\
G(x)
&= u(x^0) \int
\frac{1}{(2\pi)^3 \Abs{\Bk}}
\sin( \Abs{\Bk} x^0 )
\exp\left( i \Bk \cdot \Bx \right)
d^3 k
\end{align}

\subsection{ Four vector form for the Green's function. }

Can we put the sine and exponential product in a more pleasing form?  It would be nice to merge the $\Bx$ and $ct$ terms into a 
single four vector form.  One possibility is merging the two

\begin{align*}
\sin( \Abs{\Bk} x^0 ) &\exp\left( i \Bk \cdot \Bx \right) \\
&=
\inv{2i} \left(
\exp\left( i \left( \Bk \cdot \Bx +\Abs{\Bk} x^0 \right) \right)
-\exp\left( i \left( \Bk \cdot \Bx -\Abs{\Bk} x^0 \right) \right)
\right) \\
&=
\inv{2i} \left(
\exp\left( i \Abs{\Bk} \left( \kcap \cdot \Bx + x^0 \right) \right)
-\exp\left( i \Abs{\Bk} \left( \kcap \cdot \Bx - x^0 \right) \right)
\right)
\end{align*}

Here we have a sort of sine like conjugation in the two exponentials.  Can we tidy this up?  Let's write the unit wave number
vector in terms of direction cosines

\begin{align*}
\kcap 
&= \sum_m \sigma_m \alpha_m \\
&= \sum_m \gamma_m \gamma_0 \alpha_m \\
\end{align*}

Allowing us to write

\begin{align*}
\sum_m \gamma^m \alpha_m &= -\kcap \gamma_0
\end{align*}

This gives us
\begin{align*}
\kcap \cdot \Bx + x^0 
&= \alpha_m x^m + x^0 \\
&= (\alpha_m \gamma^m) \cdot (\gamma_j x^j) + \gamma^0 \cdot \gamma_0 x^0 \\
&= (-\kcap \gamma_0 + \gamma_0) \cdot \gamma_\mu x^\mu \\
&= (-\kcap \gamma_0 + \gamma_0) \cdot x
\end{align*}

Similarily we have

\begin{align*}
\kcap \cdot \Bx - x^0  &= (-\kcap \gamma_0 - \gamma_0) \cdot x
\end{align*}

and can now put $G$ in explicit four vector form

\begin{align*}
G(x)
&= 
%u(x \cdot \gamma_0) \int
%\frac{1}{(2\pi)^3 2 i \Abs{\Bk}}
%\left(
%\exp\left( i ((\Abs{\Bk} -\Bk)\gamma_0) \cdot x \right)
%-\exp\left( -i ((\Abs{\Bk} +\Bk)\gamma_0) \cdot x \right)
%\right)
%d^3 k
\frac{u(x \cdot \gamma_0)}{
(2\pi)^3 2 i 
} \int
\left(
\exp\left( i ((\Abs{\Bk} -\Bk)\gamma_0) \cdot x \right)
-\exp\left( -i ((\Abs{\Bk} +\Bk)\gamma_0) \cdot x \right)
\right)
\frac{d^3 k}{ \Abs{\Bk} }
\end{align*}

Hmm, is that really any better?  Intuition says that this whole thing 
can be written as sine with some sort of geometric product conjugate 
terms.

I get as far as writing 

\begin{align*}
i ( \Bk \cdot \Bx \pm \Abs{\Bk} x^0 ) 
&=
(i \gamma_0) \wedge ( \Bk \pm \Abs{\Bk} ) \cdot x
\end{align*}

But that doesn't quite have the conjugate form I was looking for (or does it)?  Have to go back and look at Hestenes's multivector conjugation operation.  Think it had something to do with reversion, but don't recall.

Failing that tidy up the following

\begin{align}
G(x)
&= 
\frac{u(x \cdot \gamma_0)}{ (2\pi)^3 }
\int
\sin( \Abs{\Bk} x \cdot \gamma_0 )
\exp\left( -i (\Bk \gamma_0) \cdot x \right)
\frac{d^3 k}{ \Abs{\Bk} }
\end{align}

is probably about as good as it gets for now.  Note the interesting feature
that we end up essentially integrating over a unit ball in our wave number
space.  This suggests the possibility of simplification using the 
divergence theorem.

\subsection{ Faraday tensor. }

Attempting to find a tidy four vector form for the four vector potentials was in preparation for taking derivatives.
Specifically, applied to \ref{eqn:fourVectorPotentials} we have

\begin{align*}
F^{\mu\nu} = \partial^\mu A^\nu - \partial^\nu A^\mu
\end{align*}

subject to the Lorentz gauge constraint
\begin{align*}
0 = \partial_\mu A^\mu
\end{align*}

If we switch the convolution indexes for our potentials

\begin{align*}
{A^\mu}(\Bx, t) &= \inv{\epsilon_0 c} \int J^\mu(x - x') G(x') d^4 x' \\
\end{align*}

Then the Lorentz gauge condition, after differentiation under the integral sign, is

\begin{align*}
0 = \partial_\mu A^\mu &= \inv{\epsilon_0 c} \int \left(\partial_\mu J^\mu(x - x') \right) G(x') d^4 x' \\
\end{align*}

So we see that the Lorentz gauge seems to actually imply the continuity equation

\begin{align*}
\partial_\mu J^\mu(x) = 0
\end{align*}

Similarily, it appears that we can write our tensor components in terms of current density derivatives

\begin{align}
F^{\mu\nu} 
%&= \partial^\mu A^\nu - \partial^\nu A^\mu \\
&= \inv{\epsilon_0 c} \int \left(\partial^\mu J^\nu(x - x') - \partial^\nu J^\mu(x - x') \right) G(x') d^4 x'
\end{align}

Logicially, I suppose that one can consider the entire problem solved here, pending the completion of this calculus exersize.

In terms of tidyness, it would be nicer seeming use the original convolution, and take derivative differences of the Green's
function.  However, how to do this is not clear to me since this function has no defined derivative
at the $t=0$ points due to the unit step.

\section{ Appendix.  Mechanical details. }

\subsection{ Solving the wave number domain differential equation. }

We wish to solve equation the inhomogeneous equation \ref{eqn:waveNumEquationToSolve}.  Writing this in terms of a linear operator equation this is

\begin{align*}
L(y) &= y'' + \alpha^2 y \\
L(y) &= g
\end{align*}

The solutions of this equation will be formed from linear combinations of the homogeneous problem plus a specific solution of the inhomogeneous problem

By inspection the homogeneous problem has solutions in $\Span \{ e^{ i \alpha x }, e^{ -i \alpha x }\}$.
We can find a solution to the inhomogeneous problem using the variation of parameters method, assuming a solution of the form

\begin{align*}
y  = u e^{ i \alpha x } + v e^{ -i \alpha x }
\end{align*}

Taking derivatives we have
\begin{align*}
y' = u' e^{ i \alpha x } + v' e^{ -i \alpha x } + i \alpha (u e^{ i \alpha x } - v e^{ -i \alpha x })
\end{align*}

The trick to solving this is to employ the freedom to set the $u'$, and $v'$ terms above to zero

\begin{align}\label{eqn:firstConstraint}
u' e^{ i \alpha x } + v' e^{ -i \alpha x } = 0
\end{align}

Given this choice we then have
\begin{align*}
y' &= i \alpha (u e^{ i \alpha x } - v e^{ -i \alpha x }) \\
y'' &=
(i \alpha)^2 (u e^{ i \alpha x } + v e^{ -i \alpha x })
i \alpha (u' e^{ i \alpha x } - v' e^{ -i \alpha x })
\end{align*}

So we have
\begin{align*}
L(y)
&=
(i \alpha)^2 (u e^{ i \alpha x } + v e^{ -i \alpha x })  \\
&+i \alpha (u' e^{ i \alpha x } - v' e^{ -i \alpha x })
+ (\alpha)^2 (u e^{ i \alpha x } + v e^{ -i \alpha x })  \\
&=
i \alpha (u' e^{ i \alpha x } - v' e^{ -i \alpha x })
\end{align*}

With this and \ref{eqn:firstConstraint} we have a set of simultaneous first order linear differential equations to solve

\begin{align*}
\begin{bmatrix}
u' \\
v' \\
\end{bmatrix}
&=
{\begin{bmatrix}
 e^{ i \alpha x } &- e^{ -i \alpha x } \\
 e^{ i \alpha x } &  e^{ -i \alpha x } \\
\end{bmatrix}}^{-1}
\begin{bmatrix}
{g}/{i \alpha} \\
0 \\
\end{bmatrix} \\
&=
\inv{2}
{\begin{bmatrix}
 e^{ -i \alpha x } & e^{ -i \alpha x } \\
 -e^{ i \alpha x } &  e^{ i \alpha x } \\
\end{bmatrix}}
\begin{bmatrix}
{g}/{i \alpha} \\
0 \\
\end{bmatrix} \\
&=
\frac{g}{2 i \alpha}
{\begin{bmatrix}
 e^{ -i \alpha x } \\
 -e^{ i \alpha x } \\
\end{bmatrix}}
\end{align*}

Substituting back into the assumed solution we have
\begin{align*}
y
&= \frac{1}{2 i \alpha} \left(
  e^{ i \alpha x } \int g e^{ -i \alpha x }
- e^{ -i \alpha x } \int g e^{ i \alpha x }
\right) \\
&= \frac{1}{2 i \alpha} \int_{u=x_0}^x g(u) \left( e^{ -i \alpha (u-x) } -e^{ i \alpha (u-x) } \right) du \\
\end{align*}

So our solution appears to be

\begin{align}\label{eqn:solutionToWaveNumberDomainEquation}
y &= \frac{1}{\alpha} \int_{u=x_0}^x g(u) \sin( \alpha(x-u) ) du
\end{align}

A check to see if this is correct is in order to verify this.  Differentiating using \ref{eqn:diffInt} we have

\begin{align*}
y'
&=
{\left.
\frac{1}{\alpha}
g(u) \sin( \alpha(x-u) ) \right\vert}_{u=x}
+\frac{1}{\alpha} \int_{u=x_0}^x \PD{x}{} g(u) \sin( \alpha(x-u) ) du \\
&= \int_{u=x_0}^x g(u) \cos( \alpha(x-u) ) du \\
\end{align*}

and for the second derivative we have

\begin{align*}
y''
&=
{\left. g(u) \cos( \alpha(x-u) ) \right\vert}_{u=x}
- \alpha \int_{u=x_0}^x g(u) \sin( \alpha(x-u) ) du \\
&= g(x) - \alpha^2 y(x)
\end{align*}

Excellent, we have $y'' + \alpha^2 y = g$ as desired.

\subsection{ Differentiation under the integral sign. }

Given an function that is both a function of the integral limits and the integrals kernel

\begin{align*}
f(x) = \int_{u = a(x)}^{b(x)} G(x,u) du,
\end{align*}

lets recall how to differentiate the beastie.  First let $G(x,u) = \PDi{u}{F(x,u)}$ so we have

\begin{align*}
f(x) = F(x,b(x)) - F(x,a(x))
\end{align*}

and our derivative is
\begin{align*}
f'(x)
&=
\PD{x}{F}(x,b(x))
\PD{u}{F}(x,b(x)) b'
-\PD{x}{F}(x,a(x))
-\PD{u}{F}(x,a(x)) a' \\
&=
G(x,b(x)) b'
-G(x,a(x)) a'
+\PD{x}{F}(x,b(x))
-\PD{x}{F}(x,a(x))
\\
\end{align*}

Now, we want $\PDi{x}{F}$ in terms of $G$, and to get there, assuming sufficient continuity, we have from the definition

\begin{align*}
\PD{x}{} G(x,u) 
&= \PD{x}{} \PD{u}{F(x,u)} \\
&= \PD{u}{} \PD{x}{F(x,u)} \\
\end{align*}

Integrating both sides with respect to $u$ we have

\begin{align*}
\int \PD{x}{G} du 
&= \int \PD{u}{} \PD{x}{F(x,u)} du \\
%&= \int \PD{u}{} \left( \PD{x}{F(x,u)} + A(x) \right) du \\
&= \PD{x}{F(x,u)} 
\end{align*}

This allows us to write 

\begin{align*}
\PD{x}{F}(x,b(x))
-\PD{x}{F}(x,a(x))
&=
\int_{a}^b \PD{x}{G}(x,u) du
\end{align*}

and finally

\begin{align}\label{eqn:diffInt}
\frac{d}{dx} \int_{u = a(x)}^{b(x)} G(x,u) du
&=
G(x,b(x)) b'
-G(x,a(x)) a'
+ \int_{a(x)}^{b(x)} \PD{x}{G}(x,u) du
\end{align}

\subsubsection{ Argument logic error above to understand. }

Is the following not also true

\begin{align*}
\int \PD{x}{G} du 
&= \int \PD{u}{} \PD{x}{F(x,u)} du \\
&= \int \PD{u}{} \left( \PD{x}{F(x,u)} + A(x) \right) du \\
&= \PD{x}{F(x,u)} + A(x)u + B
\end{align*}

In this case we have
\begin{align*}
\PD{x}{F}(x,b(x)) -\PD{x}{F}(x,a(x)) &= \int_{a}^b \PD{x}{G}(x,u) du - A(x) ( b(x) - a(x))
\end{align*}

How to reconsile this with the answer I expect (and having gotten it, I believe matches my recollection)?

\bibliographystyle{plainnat}
\bibliography{myrefs}

\end{document}
