\documentclass{article}      % Specifies the document class

\usepackage{amsmath}

%
% shorthand for bold symbols, convenient for vectors and matrices
%
\newcommand{\Ba}[0]{\mathbf{a}}
\newcommand{\Bb}[0]{\mathbf{b}}
\newcommand{\Bc}[0]{\mathbf{c}}
\newcommand{\Bd}[0]{\mathbf{d}}
\newcommand{\Be}[0]{\mathbf{e}}
\newcommand{\Bf}[0]{\mathbf{f}}
\newcommand{\Bg}[0]{\mathbf{g}}
\newcommand{\Bh}[0]{\mathbf{h}}
\newcommand{\Bi}[0]{\mathbf{i}}
\newcommand{\Bj}[0]{\mathbf{j}}
\newcommand{\Bk}[0]{\mathbf{k}}
\newcommand{\Bl}[0]{\mathbf{l}}
\newcommand{\Bm}[0]{\mathbf{m}}
\newcommand{\Bn}[0]{\mathbf{n}}
\newcommand{\Bo}[0]{\mathbf{o}}
\newcommand{\Bp}[0]{\mathbf{p}}
\newcommand{\Bq}[0]{\mathbf{q}}
\newcommand{\Br}[0]{\mathbf{r}}
\newcommand{\Bs}[0]{\mathbf{s}}
\newcommand{\Bt}[0]{\mathbf{t}}
\newcommand{\Bu}[0]{\mathbf{u}}
\newcommand{\Bv}[0]{\mathbf{v}}
\newcommand{\Bw}[0]{\mathbf{w}}
\newcommand{\Bx}[0]{\mathbf{x}}
\newcommand{\By}[0]{\mathbf{y}}
\newcommand{\Bz}[0]{\mathbf{z}}
\newcommand{\BA}[0]{\mathbf{A}}
\newcommand{\BB}[0]{\mathbf{B}}
\newcommand{\BC}[0]{\mathbf{C}}
\newcommand{\BD}[0]{\mathbf{D}}
\newcommand{\BE}[0]{\mathbf{E}}
\newcommand{\BF}[0]{\mathbf{F}}
\newcommand{\BG}[0]{\mathbf{G}}
\newcommand{\BH}[0]{\mathbf{H}}
\newcommand{\BI}[0]{\mathbf{I}}
\newcommand{\BJ}[0]{\mathbf{J}}
\newcommand{\BK}[0]{\mathbf{K}}
\newcommand{\BL}[0]{\mathbf{L}}
\newcommand{\BM}[0]{\mathbf{M}}
\newcommand{\BN}[0]{\mathbf{N}}
\newcommand{\BO}[0]{\mathbf{O}}
\newcommand{\BP}[0]{\mathbf{P}}
\newcommand{\BQ}[0]{\mathbf{Q}}
\newcommand{\BR}[0]{\mathbf{R}}
\newcommand{\BS}[0]{\mathbf{S}}
\newcommand{\BT}[0]{\mathbf{T}}
\newcommand{\BU}[0]{\mathbf{U}}
\newcommand{\BV}[0]{\mathbf{V}}
\newcommand{\BW}[0]{\mathbf{W}}
\newcommand{\BX}[0]{\mathbf{X}}
\newcommand{\BY}[0]{\mathbf{Y}}
\newcommand{\BZ}[0]{\mathbf{Z}}

\newcommand{\Bzero}[0]{\mathbf{0}}
\newcommand{\Btheta}[0]{\boldsymbol{\theta}}
\newcommand{\Btau}[0]{\boldsymbol{\tau}}
\newcommand{\Bomega}[0]{\boldsymbol{\omega}}

%
% shorthand for unit vectors
%
\newcommand{\acap}[0]{\hat{\Ba}}
\newcommand{\bcap}[0]{\hat{\Bb}}
\newcommand{\ccap}[0]{\hat{\Bc}}
\newcommand{\dcap}[0]{\hat{\Bd}}
\newcommand{\ecap}[0]{\hat{\Be}}
\newcommand{\fcap}[0]{\hat{\Bf}}
\newcommand{\gcap}[0]{\hat{\Bg}}
\newcommand{\hcap}[0]{\hat{\Bh}}
\newcommand{\icap}[0]{\hat{\Bi}}
\newcommand{\jcap}[0]{\hat{\Bj}}
\newcommand{\kcap}[0]{\hat{\Bk}}
\newcommand{\lcap}[0]{\hat{\Bl}}
\newcommand{\mcap}[0]{\hat{\Bm}}
\newcommand{\ncap}[0]{\hat{\Bn}}
\newcommand{\ocap}[0]{\hat{\Bo}}
\newcommand{\pcap}[0]{\hat{\Bp}}
\newcommand{\qcap}[0]{\hat{\Bq}}
\newcommand{\rcap}[0]{\hat{\Br}}
\newcommand{\scap}[0]{\hat{\Bs}}
\newcommand{\tcap}[0]{\hat{\Bt}}
\newcommand{\ucap}[0]{\hat{\Bu}}
\newcommand{\vcap}[0]{\hat{\Bv}}
\newcommand{\wcap}[0]{\hat{\Bw}}
\newcommand{\xcap}[0]{\hat{\Bx}}
\newcommand{\ycap}[0]{\hat{\By}}
\newcommand{\zcap}[0]{\hat{\Bz}}
\newcommand{\thetacap}[0]{\hat{\Btheta}}

%
% to write R^n and C^n in a distinguishable fashion.  Perhaps change this
% to the double lined characters upon figuring out how to do so.
%
\newcommand{\C}[1]{${\BC}^{#1}$}
\newcommand{\R}[1]{${\BR}^{#1}$}

%
% various generally useful helpers
%

% derivative of #1 wrt. #2:
\newcommand{\D}[2] {\frac {d#2} {d#1}}

\newcommand{\inv}[1]{\frac{1}{#1}}
\newcommand{\cross}[0]{\times}

\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\innerprod}[2]{\langle{#1}, {#2}\rangle}
\newcommand{\dotprod}[2]{#1 \cdot #2}
\newcommand{\crossprod}[2]{#1 \cross #2}
\newcommand{\tripleprod}[3]{\dotprod{\crossprod{#1}{#2}}{#3}}

%
% A few miscellaneous things specific to this document
%
\newcommand{\crossop}[1]{\crossprod{#1}{}}

\newcommand{\PDP}[2]{\BP^{#1}\BD{\BP^{#2}}}
\newcommand{\PDPDP}[3]{\Bv^T\BP^{#1}\BD\BP^{#2}\BD\BP^{#3}\Bv}

\newcommand{\Mp}[0]{
\begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0
\end{bmatrix}
}
\newcommand{\Mpp}[0]{
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0
\end{bmatrix}
}
\newcommand{\Mppp}[0]{
\begin{bmatrix}
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
}
\newcommand{\Mpu}[0]{
\begin{bmatrix}
u_1 & 0 & 0 & 0 \\
0 & u_2 & 0 & 0 \\
0 & 0 & u_3 & 0 \\
0 & 0 & 0 & u_4
\end{bmatrix}
}

%
% The real thing:
%

                             % The preamble begins here.
\title{ Covariant/vector derivative notes, plus notes on raised and lowered indexes.  }
\author{Peeter Joot}         % Declares the author's name.

%\date{}        % Deleting this command produces today's date.

\begin{document}             % End of preamble and beginning of text.

\maketitle{}

\section{Motivation.}

My notes on tensors, mostly from Geometric Algebra for Physicists.  Write up enough notes for myself that I can understand the topics (if I can't explain to myself I don't understand sufficiently).  Conclude with the
solution of problem 6.1 to demonstrate the frame independence of the
covariant derivative.

\subsection{ Raised and lowered indexes. Coordinates of vectors with non-orthonormal frames. }

Let $\{ e_i \}$ represent a frame of not necessarily orthonormal basis vectors for a metric space, and $\{ e^i \}$ represent the reciprocal frame.

The reciprocal frame vectors are defined by the relation:

\begin{equation}
e_i \cdot e^j = {\delta_i}^j.
\end{equation}

Lets compute the coordinates of a vector $x$ in terms of both frames:

\[
x = \sum \alpha_j e_j = \sum \beta_j e^j
\]

Forming $x \cdot e^i$, and $x \cdot e_i$ respectively solves for the $\alpha$, and $\beta$ coefficients

\[
x \cdot e^i = \sum \alpha_j e_j \cdot e^i = \sum \alpha_j {\delta_j}^i = \alpha_i
\]

\[
x \cdot e_i = \sum \beta_j e^j \cdot e_i = \sum \beta_j {\delta_i}^j = \beta_i
\]

Thus, the reciprocal frame vectors allow for simple determination of coordinates for an arbitrary frame. We can summarize this as follows:

\[
x = \sum ( x \cdot e^i ) e_i = \sum ( x \cdot e_i ) e^i
\]

Now, for orthonormal frames, where $e_i = e^i$ we are used to writing:

\[
x = \sum x_i e_i,
\]

however for non-orthonormal frames the convention is to mix raised and lowered indexes as follows:

\[
x = \sum x^i e_i = \sum x_i e^i.
\]

Where, as demonstrated above these generalized coordinates have the values, $x^i = x \cdot e^i$, and $x_i = x \cdot e_i$.  This is a strange seeming notation at
first especially since most of linear algebra is done with always lowered (or always upper for some authors) indexes.  However one quickly gets used to it, after seeing how powerful the reciprocal frame concept is for dealing with non-orthonormal frames (otherwise one has to drag along matrices and their inverses to express the same vector decompositions).

\subsection{ Metric tensor. }

It is customary in tensor formulations of physics to utilize a metric tensor to express the dot product.

Compute the dot product using the coordinate vectors

\[
x \cdot y = \left(\sum x^i e_i \right)\left(\sum y^j e_j \right) = \sum x^i y^j \left( e_i \cdot e_j \right)
\]

\[
x \cdot y = \left(\sum x_i e^i \right)\left(\sum y_j e^j \right) = \sum x_i y_j \left( e^i \cdot e^j \right)
\]

Introducing second rank (symmetric) tensors for the dot product pairs $ e_i \cdot e_j = g_{ij}$, and $ g^{ij} = e^i \cdot e^j $ we have

\[
x \cdot y = \sum x_i y_j g^{ij} = \sum x^i y^j g_{ij} = \sum x_i y^i = \sum x^i y_i
\]

We see that the metric tensor provides a way to specify the dot product in index notation, and removes the explicit references to the original frame vectors.  Mixed indexes also removes the references to the original frame vectors, but additionally eliminates the need for either of the metric tensors.

Note that it is also common to see Einstein summation convention employed, which omits the $\sum$:

\[
x \cdot y = x_i y_j g^{ij} = x^i y^j g_{ij} = x^i y_i = x_i y^i
\]

Here summation over all matched upper, lower index pairs is implied.

\subsection{ Metric tensor relations to coordinates. }

Given a coordinate expression of a vector, we dot that with the frame vectors to observe the relation between coordinates and the metric tensor:

\[
x \cdot e_i = \sum x^j e_j \cdot e_i = \sum x^j g_{ij}
\]

\[
x \cdot e^i = \sum x_j e^j \cdot e^i = \sum x_j g^{ij}
\]

The metric tensors can therefore be used be used to express the relations between the upper and lower index coordinates:

\begin{align}
x_i &= \sum g_{ij} x^j \label{eqn:metric_upper_to_lower} \\
x^i &= \sum g^{ij} x_j \label{eqn:metric_lower_to_upper}
\end{align}

It is therefore apparent that the matrix of the index lowered metric tensor $g_{ij}$ is the inverse of the matrix for the raised index metric tensor $g^{ij}$.

\subsection{ Metric tensor as a Jacobian }

The relations of equations \ref{eqn:metric_upper_to_lower}, and \ref{eqn:metric_lower_to_upper} show that the metric tensor can be expressed in terms of partial derivatives:

\begin{align}
\frac{\partial x_i }{\partial x^j } &= g_{ij} \\
\frac{\partial x^i }{\partial x_j } &= g^{ij}
\end{align}

Therefore the metric tensors can also be expressed as Jacobian matrices (not Jacobian determinants) :

\begin{align}
g_{ij} &= \frac{\partial (x_1, \cdots, x_n) }{\partial (x^1, \cdots, x^n) } \\
g^{ij} &= \frac{\partial (x^1, \cdots, x^n) }{\partial (x_1, \cdots, x_n) }
\end{align}

Will this be useful in any way?

\subsection{ Change of basis. }

To perform a change of basis from one non-orthonormal basis $\{e_i\}$ to a second $\{f_i\}$, relations between the sets of vectors
are required.  Using Greek indexes for the $f$ frame, and English for the $e$ frame, those are:

\begin{align*}
e_i 		&= \sum f^{\mu} e_i \cdot f_{\mu} 	= \sum f_{\mu} e_i \cdot f^{\mu} \\
f_{\alpha} 	&= \sum e^k f_{\alpha} \cdot e_k 	= \sum e_k f_{\alpha} \cdot e^k \\
e^i 		&= \sum f^{\mu} e^i \cdot f_{\mu} 	= \sum f_{\mu} e^i \cdot f^{\mu} \\
f^{\alpha} 	&= \sum e^k f^{\alpha} \cdot e_k 	= \sum e_k f^{\alpha} \cdot e^k 
\end{align*}

Following GAFP we can write the dot product terms as a second order tensors $f$ (ie: matrix relation) :

\begin{align*}
e_i 		&= \sum f^{\mu} f_{i\mu}  	= \sum f_{\mu} {f_i}^{\mu} \\
f_{\alpha} 	&= \sum e^k f_{k\alpha} 	= \sum e_k {f^k}_{\alpha} \\
e^i 		&= \sum f^{\mu} {f^i}_{\mu} 	= \sum f_{\mu} f^{i \mu} \\
f^{\alpha} 	&= \sum e^k {f_k}^{\alpha}  	= \sum e_k f^{k\alpha}
\end{align*}

Looking at these relations in pairs, such as

\begin{align*}
f_{\alpha} 	&= \sum e^k f_{k\alpha} \\ 
e^i 		&= \sum f_{\mu} f^{i \mu} 
\end{align*}

and 

\begin{align*}
e_i 		&= \sum f^{\mu} f_{i\mu} \\
f^{\alpha} 	&= \sum e_k f^{k\alpha}
\end{align*}

It is clear that $f_{i\alpha}$ is the inverse of $f^{i\alpha}$.  FIXME: write this out explicitly in index notation, to specify
more exactly the inverse relationship ... that will help clarify the covariant derivative stuff later.  There are also inverse relationships for the mixed index tensors above.  Can those be used in the covariant derivative calculation to simplify things?

Note that all these various tensors are related to each other using the metric tensors for $f$ and $e$.  FIXME: show example.  Also note that using this notation the metric tensors $g_{ij}$ and $g_{\alpha\beta}$ are two completely different linear functions, and careful use of the index conventions are required to keep these straight.

\subsection{ Covariant derivative. }

GAFP exercise 6.1.  Show that the vector derivative:

\begin{equation}
\nabla = \sum e^i \frac{\partial}{\partial x^i}
\end{equation}

is not frame dependent.

To show this we will need to utilize the chain rule to rewrite the partials in terms of the alternate frame:

\begin{align*}
\frac{\partial}{\partial x^i} &= \sum \frac{\partial x^\alpha}{\partial x^i} \frac{\partial}{\partial x^\alpha} 
\end{align*}

To evaluate the first partial here, we write the coordinates of a vector in terms of both, and take dot products:

\begin{align*}
\left(\sum x^{\gamma} f_{\gamma}\right) \cdot f^{\alpha} = \left(\sum x^i e_i\right) \cdot f^{\alpha} \\
\end{align*}
\begin{align*}
x^{\alpha} &= \sum x^i {f_i}^{\alpha} \\
\end{align*}
\begin{align*}
\frac{\partial x^{\alpha}}{\partial x^i} &= {f_i}^{\alpha}
\end{align*}

Similar expressions for the other change of basis tensors is also possible, but
not required for this problem.

With this result we have the partial reexpressed in terms of coordinates
in the new frame.

\begin{align*}
\frac{\partial}{\partial x^i} &= \sum {f_i}^{\alpha} \frac{\partial}{\partial x^\alpha} 
\end{align*}

Combine this with the alternate contravariant frame vector as calculated above:

\[
e^i = \sum f^{\mu} {f^i}_{\mu}
\]

and we have:

\begin{align*}
\sum_i e^i \frac{\partial}{\partial x^i}
&= \sum_i \left(\sum_{\mu} f^{\mu} {f^i}_{\mu} \right) \left( \sum_{\alpha} {f_i}^{\alpha} \frac{\partial}{\partial x^\alpha}\right) \\
&= \sum_{\mu \alpha} \left(f^{\mu} \frac{\partial}{\partial x^\alpha} \right) \sum_i {f^i}_{\mu} {f_i}^{\alpha} \\
&= \sum_{\mu \alpha} \left(f^{\mu} \frac{\partial}{\partial x^\alpha} \right) {\delta_{\mu}}^{\alpha} \\
&= \sum_{\alpha} f^{\alpha} \frac{\partial}{\partial x^\alpha} \\
\end{align*}

FIXME: Proper justification of the step $\sum_i {f^i}_{\mu} {f_i}^{\alpha} = {\delta_{\mu}}^{\alpha}$ is missing.  This is possible due
to the imprecisely noted inverse relationships pointed out above.

Note that my original paper derivation of the above used only the tensors $f_{i\alpha}$, and $f^{i\alpha}$ instead of the mixed index versions used here.  That worked but also required a pair of metric tensors, and one more step to sum over those tensors to get at the final result.

\end{document}               % End of document.
