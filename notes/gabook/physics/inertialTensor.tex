%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

% 
% 
\chapter{Inertia Tensor}
\label{chap:inertialTensor}
%\date{Feb 15, 2008.  inertialTensor.tex}

\citep{doran2003gap}
derives the angular momentum for rotational motion in the following form

\[
L = R \left( \int \Bx \wedge (\Bx \cdot \Omega_B) dm \right) R^\dagger
\]

and calls the integral part, the inertia tensor

\[
\emph{I}(B) = \int \Bx \wedge (\Bx \cdot \Omega_B) dm
\]

which is a linear mapping from bivectors to bivectors.  To understand the
form of this I found it helpful to expanding the wedge product
part of this explicitly for the \R{3} case.

Ignoring the sum in this expansion write

\[
f(B) = \Bx \wedge (\Bx \cdot B)
\]

And writing $\Be_{ij} = \Be_i \Be_j$ introduce a basis

\[
b = \{ \Be_1 I, \Be_2 I, \Be_3 I \} = \{ \Be_{23}, \Be_{31}, \Be_{12} \}
\]

for the \R{3} bivector product space.

Now calculate $f(B)$ for each of the basis vectors

\begin{align*}
f(\Be_1 I) 
&= \Bx \wedge (\Bx \cdot \Be_{23}) \\
&= ( x_1 \Be_1 + x_2 \Be_2 + x_3 \Be_3) \wedge (x_2 \Be_3 - x_3 \Be_2) \\
\end{align*}

Completing this calculation for each of the unit basic bivectors, we have
%%+ (           + x_2 \Be_2            ) \wedge (x_2 \Be_3            ) \\
%%+ (                       + x_3 \Be_3) \wedge (          - x_3 \Be_2) \\
%+ ( x_1 \Be_1                        ) \wedge (x_2 \Be_3            ) \\
%+ ( x_1 \Be_1                        ) \wedge (          - x_3 \Be_2) \\
%%+ (           + x_2 \Be_2            ) \wedge (          - x_3 \Be_2) \\ %% = 0
%%+ (                       + x_3 \Be_3) \wedge (x_2 \Be_3            ) \\ %% = 0
\begin{align*}
f(\Be_1 I) &= (x_2^2 + x_3^2) \Be_{23} - (x_1 x_2) \Be_{31} - (x_1 x_3) \Be_{12} \\
f(\Be_2 I) &= -(x_1 x_2) \Be_{23} + (x_1^2 + x_3^2) \Be_{31} - (x_2 x_3) \Be_{12} \\
f(\Be_3 I) &= -(x_1 x_3) \Be_{23} - (x_2 x_3) \Be_{31} + (x_1^2 + x_2^2) \Be_{12} \\
\end{align*}

Observe that taking dot products with $(\Be_i I)^\dagger$ will select just the $\Be_i I$ term of the result, so one can
form the matrix of this linear transformation that maps bivectors in basis $b$ to image vectors also in basis $b$ as follows

\[
\matrixoftx{\emph{I}(B)}{b}{b}
=
\coordvec{\emph{I}(\Be_i I) \cdot (\Be_j I)^\dagger}{ij}
=
\int {\begin{bmatrix}
x_2^2 + x_3^2  & - x_1 x_2  & - x_1 x_3 \\
-x_1 x_2  & x_1^2 + x_3^2  & - x_2 x_3  \\
-x_1 x_3  & -x_2 x_3  & x_1^2 + x_2^2  \\
\end{bmatrix}} dm
\]

Here the notation $\matrixoftx{A}{b}{c}$ is borrowed from
\citep{damiano1988cla} for the matrix of a linear transformation that
takes one from basis $b$ to $c$.

Observe that this (\R{3} specific expansion) can also be written in a more
typical tensor notation with $\matrixoftx{\emph{I}}{b}{b} = \coordvec{I_{ij}}{ij}$

\[
I_{ij} 
= \emph{I}(\Be_i I) \cdot (\Be_j I)^\dagger 
= \int (\delta_{ij} \Bx^2 - x_i x_j) dm
\]

Where, as usual for tensors, the meaning of the indexes and whether summation is required is implied.  In this case
the coordinate transformation matrix for this linear transformation has components $I_{ij}$ (and no summation).

\section{orthogonal decomposition of a function mapping a blade to a blade}

Arriving at this result without explicit expansion is also possible by observing that an orthonormal decomposition of a 
function can be written in terms of an orthogonal basis $\{\sigma_i\}$ as follows:

\begin{equation}
f(B) = \sum_i (f(B) \cdot \sigma_i) \cdot \frac{1}{\sigma_i}
\end{equation}\label{eqn:itensor:bladeOrthogDecomp}

The dot product is required since the general product of two bivectors has grade-0, grade-2, and grade-4 terms (with a similar mix of higher grade terms for k-blades).

Perhaps unobviously since one is not normally used to seeing a scalar-vector dot product, this formula is not only true for bivectors, but any grade blade, including
vectors.  To verify this recall that the 
general definition of the dot product is the lowest grade term of the geometric product of two blades.  For example with grade $i,j$ blades $a$, and $b$ respectively
the dot product is:

\[
a \cdot b = \langle a b \rangle_{\abs{i-j}}
\]

So, for a scalar-vector dot product is just the scalar product of the two

\[
a \cdot \Bx = \langle{ a \Bx }\rangle_1 = a \Bx
\]

The inverse in (\ref{eqn:itensor:bladeOrthogDecomp}) can be removed by reversion, and for a grade-r blade this sum of projective terms then becomes:

\begin{equation}
f(B) = (-1)^{r(r-1)/2} \frac{1}{{\abs{\sigma_i}}^2}\sum_i (f(B) \cdot \sigma_i) \cdot {\sigma_i}
\end{equation}

For an orthonormal basis we have

\[
\sigma_i \sigma_i^\dagger = \abs{\sigma_i}^2 = 1
\]

Which allows for a slightly simpler set of projective terms:

\begin{equation}
f(B) = (-1)^{r(r-1)/2} \sum_i (f(B) \cdot \sigma_i) \cdot {\sigma_i}
\end{equation}\label{eqn:itensor:OrthonormalDecomp}

\section{coordinate transformation matrix for a couple other linear transformations}

Seeing a function of a bivector for the first time is kind of intriguing.  We can form the matrix of such a linear transformation
from a basis of the bivector space to the space spanned by function.  For fun, let's calculate that matrix for the basis $b$ above
for the following function:

\[
f(B) = \Be_1 \wedge (\Be_2 \cdot B)
\]

For this function operating on \R{3} bivectors we have:

\begin{align*}
f(\Be_{23}) &= \Be_1 \wedge (\Be_2 \cdot \Be_{23}) = -\Be_{31} \\
f(\Be_{31}) &= \Be_1 \wedge (\Be_2 \cdot \Be_{31}) = 0 \\
f(\Be_{12}) &= \Be_1 \wedge (\Be_2 \cdot \Be_{12}) = 0 \\
\end{align*}

So

\[
\matrixoftx{f}{b}{b}
= 
\begin{bmatrix}
-1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{bmatrix}
\]

For \R{4} one orthonormal basis is

\[
b = \{ 
\Be_{12},
\Be_{13},
\Be_{14},
\Be_{23},
\Be_{24},
\Be_{34}
\}
\]

A basis for the span of $f$ is $b' = \{
\Be_{13},
\Be_{14}
\}$.  Like any other coordinate transformation associated with a linear transformation we can write the matrix of the transformation that
takes a coordinate vector in one basis into a coordinate vector for the basis for the image:

\[
\coordvec{f(x)}{b'}
=
\matrixoftx{f}{b}{b'}
\coordvec{x}{b}
\] 

For this function $f$ and these pair of basis bivectors we have:

\[
\matrixoftx{f}{b}{b'}
= 
\begin{bmatrix}
0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\]

\section{Equation 3.126 details}

This statement from GAFP deserves expansion (or at least an exercise):

\[
A \cdot ( \Bx \wedge (\Bx \cdot B) )
= \langle{A \Bx (\Bx \cdot B)}\rangle
= \langle (A \cdot \Bx) \Bx B \rangle
= B \cdot ( \Bx \wedge (\Bx \cdot A) )
\]

Perhaps this is obvious to the author, but wasn't to me.  To clarify this observe the following product

\[
\Bx ( \Bx \cdot B ) = \Bx \cdot ( \Bx \cdot B ) + \Bx \wedge ( \Bx \cdot B ) 
\]

By writing $B = \Bb \wedge \Bc$ we can show that the dot product part of this product is zero:

\begin{align*}
\Bx \cdot ( \Bx \cdot B ) 
&= \Bx \cdot ( (\Bx \cdot \Bb) \Bc - (\Bx \cdot \Bc) \Bb ) \\
&= (\Bx \cdot \Bc) (\Bx \cdot \Bb) - (\Bx \cdot \Bb) (\Bx \cdot \Bc)) \\
&= 0
\end{align*}

This provides the justification for the wedge product removal in the text, since
one can write

\begin{equation}
\Bx \wedge ( \Bx \cdot B ) = \Bx ( \Bx \cdot B )
\end{equation}\label{eqn:itensor:inertiaWedgeToProduct}

Although it wasn't stated in the text (\ref{eqn:itensor:inertiaWedgeToProduct}), can
be used to put this inertia product in a pure dot product form

\begin{align*}
A^\dagger \cdot (\Bx \wedge (\Bx \cdot B) )
&= -\langle {A \Bx (\Bx \cdot B)} \rangle \\
&= \langle (\Bx \cdot A - A \wedge \Bx)(\Bx \cdot B) \rangle \\
\end{align*}

The trivector-vector part of this product has only vector and trivector components
\[
(A \wedge \Bx)(\Bx \cdot B) = \langle{ (A \wedge \Bx)(\Bx \cdot B)}\rangle_1 + \langle{(A \wedge \Bx)(\Bx \cdot B)}\rangle_3
\]

So $\langle{(A \wedge \Bx)(\Bx \cdot B)}\rangle_0 = 0$, and one can write

\begin{equation}
A^\dagger \cdot (\Bx \wedge (\Bx \cdot B) ) = (\Bx \cdot A) \cdot (\Bx \cdot B)
\end{equation}\label{eqn:itensor:iTensorTripleDot}

As pointed out in the text this is symmetric.  That can't be more clear than in (\ref{eqn:itensor:iTensorTripleDot}).

\section{Just for fun.  General dimension component expansion of inertia tensor terms}

This triple dot product expansion allows for a more direct component expansion of the component form of the inertia tensor.
There are three general cases to consider.

\begin{itemize}
\item The diagonal terms:

\[
(\Bx \cdot \sigma_i) \cdot (\Bx \cdot \sigma_i) 
= (\Bx \cdot \sigma_i)^2
\]

Writing $\sigma_i = \Be_{st}$ where $s \ne t$, we have

\begin{align*}
(\Bx \cdot \Be_{st})^2
&=
( (\Bx \cdot \Be_s) \Be_t -(\Bx \cdot \Be_t) \Be_s )^2 \\
&=
x_s^2 + x_t^2 - 2 x_s x_t \Be_t \cdot \Be_s \\
&=
x_s^2 + x_t^2
\end{align*}

\item Off diagonal terms where basis bivectors have a line of intersection (always true for \R{3}).

Here, ignoring the potential variation in sign, we can write the two basis bivectors as $\sigma_i = \Be_{si}$ and $\sigma_j = \Be_{ti}$, where $s \ne t \ne i$.  Computing the products we have

\begin{align*}
(\Bx \cdot \sigma_i) \cdot (\Bx \cdot \sigma_j)
&= (\Bx \cdot \Be_{si}) \cdot (\Bx \cdot \Be_{ti})  \\
&= ((\Bx \cdot \Be_s) \Be_i - (\Bx \cdot \Be_i) \Be_s) \cdot ((\Bx \cdot \Be_t) \Be_i - (\Bx \cdot \Be_i) \Be_t) \\
&= (x_s \Be_i - x_i \Be_s) \cdot (x_t \Be_i - x_i \Be_t) \\
&= x_s x_t \\
\end{align*}

\item Off diagonal terms where basis bivectors have no intersection.

An example from \R{4} are the two bivectors $\Be_1 \wedge \Be_2$ and $\Be_3 \wedge \Be_4$

In general, again ignoring the potential variation in sign, we can write the two basis bivectors as $\sigma_i = \Be_{su}$ and $\sigma_j = \Be_{tv}$, where $s \ne t \ne u \ne v$.  Computing the products we have

\begin{align*}
(\Bx \cdot \sigma_i) \cdot (\Bx \cdot \sigma_j)
&= (\Bx \cdot \Be_{su}) \cdot (\Bx \cdot \Be_{tv})  \\
&= ((\Bx \cdot \Be_s) \Be_u - (\Bx \cdot \Be_u) \Be_s) \cdot ((\Bx \cdot \Be_t) \Be_v - (\Bx \cdot \Be_v) \Be_t) \\
&= 0 \\
\end{align*}

\end{itemize}

For example, choosing basis $\sigma = \{ \Be_{12}, \Be_{13}, \Be_{14}, \Be_{23}, \Be_{24}, \Be_{34} \}$ the coordinate transformation matrix can be written out

\[
\matrixoftx{f}{\sigma}{\sigma}
=
\begin{bmatrix}
x_1^2 + x_2^2     &     x_2 x_3      &     x_2 x_4    &     -x_1 x_3      &    -x_1 x_4    &     0 \\
x_2 x_3           & x_1^2 + x_3^2    &     x_3 x_4      &     x_1 x_2    &            0  & -x_1 x_4 \\
x_2 x_4           &  x_3 x_4         & x_1^2 + x_4^2    &         0    &      x_1 x_2    & x_1 x_3 \\
-x_1 x_3          &  x_1 x_2         &          0     & x_2^2 + x_3^2  &      x_3 x_4    & -x_2 x_4 \\
-x_1 x_4          &  0             &     x_1 x_2      &     x_3 x_4    & x_1^2 + x_4^2   & x_2 x_3 \\
   0            &  -x_1 x_4        &     x_1 x_3      &    -x_2 x_4    &      x_2 x_3    &  x_3^2 + x_4^2 \\
\end{bmatrix}
\]

\section{Example calculation.  Masses in a line}

Pick some points on the x-axis, $\Br^{(i)}$ with masses $m_i$.
The (\R{3}) inertia tensor with respect to basis $\{\Be_i I\}$, is

\[
\sum_i {
\begin{bmatrix}
 0 & 0               & 0      \\
 0 & (r^{(i)}_1)^2     & 0      \\
 0 & 0               & (r^{(i)}_1)^2  \\
\end{bmatrix}
} m_i
= \sum{ m_i \Br_i^2}
\begin{bmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
\]

Observe that in this case the inertia tensor here only has components in the $zx$ and $xy$ planes (no component in the yz plane that is perpendicular to the line).

\section{Example calculation.  Masses in a plane}

Let $x = re^{i\theta}\Be_1$, where $i=\Be_1 \wedge \Be_2$ be a set of points in the $xy$ plane, and use $\sigma = \{\sigma_i = \Be_i I\}$ as the basis for the \R{3} bivector space.

We need to compute

\begin{align*}
\Bx \cdot \sigma_i 
&= r(e^{i\theta} \Be_1) \cdot ( \Be_i I ) \\
&= r\langle{ e^{i\theta} \Be_1 \Be_i I }\rangle \\
\end{align*}

Calculation of the inertia tensor components has three cases, depending on the value of $i$

\begin{itemize}
\item $i=1$

\begin{align*}
\frac{1}{r}(\Bx \cdot \sigma_i) 
&= {\langle e^{i\theta} I\rangle}_1 \\
&= i \sin\theta I \\
&= - \Be_3 \sin\theta \\
\end{align*}

\item $i=2$

\begin{align*}
\frac{1}{r}(\Bx \cdot \sigma_i) 
&= {\langle e^{i\theta} \Be_1 \Be_2 I\rangle}_1 \\
&= -{\langle e^{i\theta} \Be_3 \rangle}_1 \\
&= - \Be_3 \cos\theta \\
\end{align*}

\item $i=3$

\begin{align*}
\frac{1}{r}(\Bx \cdot \sigma_i) 
&= {\langle e^{i\theta} \Be_1 \Be_3 (\Be_3 \Be_1 \Be_2) \rangle}_1 \\
&= {\langle e^{i\theta} \Be_2 \rangle}_1 \\
&= e^{i\theta} \Be_2 \\
\end{align*}

\end{itemize}

Thus for $i=\{1, 2, 3\}$, the diagonal terms are
\[
(\Bx \cdot \sigma_i)^2 = r^2 \{ \sin^2 \theta, \cos^2 \theta, 1\}
\]

and the non-diagonal terms are
\[
(\Bx \cdot \sigma_1) \cdot (\Bx \cdot \sigma_2) = r^2 \sin\theta \cos\theta
\]
\[
(\Bx \cdot \sigma_1) \cdot (\Bx \cdot \sigma_3) = 0
\]
\[
(\Bx \cdot \sigma_2) \cdot (\Bx \cdot \sigma_3) = 0
\]

Thus, with indexes implied ($r=\Br_i$, $\theta = \theta_i$, and $m = m_i$, the inertia tensor is

\begin{align*}
\matrixoftx{\emph{I}}{\sigma}{\sigma}
&= 
\sum m r^2
{
\begin{bmatrix}
\sin^2 \theta  & \sin\theta \cos \theta & 0 \\
\sin\theta \cos \theta & \cos^2 \theta & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
} \\
\end{align*}

It is notable that this can be put into double angle form
\begin{align*}
\matrixoftx{\emph{I}}{\sigma}{\sigma}
&=
\sum 
m r^2
{
\begin{bmatrix}
\frac{1}{2} (1 - \cos 2\theta)    &   \frac{1}{2} \sin 2\theta      &     0 \\
\frac{1}{2} \sin 2\theta      &     \frac{1}{2}(1 + \cos 2\theta)    &    0 \\
0 & 0 & 1 \\
\end{bmatrix}
} \\
&=
\frac{1}{2} 
\sum 
m r^2
\left(
I + 
{
\begin{bmatrix}
- \cos 2\theta    &    \sin 2\theta     &    0 \\
\sin 2\theta      &     \cos 2\theta    &    0 \\
0 & 0 & 1 \\
\end{bmatrix}
}
\right) \\
\end{align*}

So if grouping masses along each distinct line in the plane, those components of the inertia tensor can be thought of as 
functions of twice the angle.  This is natural in terms of a rotor interpretation, which is likely possible since each of
these groups of masses in a line can be diagonalized with a rotation.

It can be verified that the following $xy$ plane rotation diagonalizes all the terms of constant angle.  Writing

\[
R_\theta =
\begin{bmatrix}
\cos\theta & -\sin\theta & 0 \\
\sin\theta & \cos\theta & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
\]

We have
\begin{align*}
\matrixoftx{\emph{I}}{\sigma}{\sigma}
&=
\sum 
m r^2
R_{-\theta}
\begin{bmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{bmatrix}
R_\theta
\end{align*}
