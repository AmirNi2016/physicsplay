%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

% 
% 
%\input{../peeter_prologue.tex}

\chapter{Bivector grades of the squared angular momentum operator}
\label{chap:bivectorSelect}

%\blogpage{http://sites.google.com/site/peeterjoot/math2009/bivectorSelect.pdf}
%\date{Sept 6, 2009}
%\revisionInfo{$RCSfile: bivectorSelect.tex,v $ Last $Revision: 1.3 $ $Date: 2009/10/22 02:07:20 $}

%\beginArtWithToc
\beginArtNoToc

\section{Motivation}

The aim here is to extract the bivector grades of the squared angular momentum operator

\begin{align}\label{eqn:bivectorSelect:goo1}
\gpgradetwo{ (x \wedge \grad)^2 } \questionEquals \cdots
\end{align}

I had tried this before and believe gotten it wrong.  Take it super slow and dumb and careful.

\section{Non-operator expansion}

Suppose $P$ is a bivector, $P = (\gamma^k \wedge \gamma^m) P_{km}$, the grade two product with a different unit bivector is

\begin{align*}
&\gpgradetwo{ (\gamma_a \wedge \gamma_b) (\gamma^k \wedge \gamma^m) } P_{km} \\
&= 
\gpgradetwo{ (\gamma_a \gamma_b - \gamma_a \cdot \gamma_b) (\gamma^k \wedge \gamma^m) } P_{km} \\
&= 
\gpgradetwo{ \gamma_a (\gamma_b \cdot (\gamma^k \wedge \gamma^m)) } P_{km} 
+ \gpgradetwo{ \gamma_a (\gamma_b \wedge (\gamma^k \wedge \gamma^m)) } P_{km} 
- (\gamma_a \cdot \gamma_b) (\gamma^k \wedge \gamma^m) P_{km} \\
&= 
(\gamma_a \wedge \gamma^m) P_{b m} -(\gamma_a \wedge \gamma^k) P_{k b} - (\gamma_a \cdot \gamma_b) (\gamma^k \wedge \gamma^m) P_{km} \\
&+ (\gamma_a \cdot \gamma_b) (\gamma^k \wedge \gamma^m) P_{km} 
- (\gamma_b \wedge \gamma^m) P_{a m} 
+ (\gamma_b \wedge \gamma^k) P_{k a} 
\\
&= 
(\gamma_a \wedge \gamma^c) (P_{b c} -P_{c b})
+ (\gamma_b \wedge \gamma^c) (P_{c a} -P_{a c} )
\\
\end{align*}

This same procedure will be used for the operator square, but we have the complexity of having the second angular momentum operator change the first bivector result.

\section{Operator expansion}

In the first few lines of the bivector product expansion above, a blind replacement $\gamma_a \rightarrow x$, and $\gamma_b \rightarrow \grad$ gives us

\begin{align*}
&\gpgradetwo{ (x \wedge \grad) (\gamma^k \wedge \gamma^m) } P_{km}  \\
&= 
\gpgradetwo{ (x \grad - x \cdot \grad) (\gamma^k \wedge \gamma^m) } P_{km} \\
&= 
\gpgradetwo{ x (\grad \cdot (\gamma^k \wedge \gamma^m)) } P_{km} 
+ \gpgradetwo{ x (\grad \wedge (\gamma^k \wedge \gamma^m)) } P_{km} 
- (x \cdot \grad) (\gamma^k \wedge \gamma^m) P_{km} \\
\end{align*}

Using $P_{km} = x_k \partial_m$, eliminating the coordinate expansion we have an intermediate result that gets us partway to the desired result

\begin{align}\label{eqn:bivectorSelect:goo2}
\gpgradetwo{ (x \wedge \grad)^2 }
&=
\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) } 
+ \gpgradetwo{ x (\grad \wedge (x \wedge \grad)) } 
- (x \cdot \grad) (x \wedge \grad) 
\end{align}

An expansion of the first term should be easier than the second.  Dropping back to coordinates we have

\begin{align*}
\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) } 
&=
\gpgradetwo{ x (\grad \cdot (\gamma^k \wedge \gamma^m)) } x_k \partial_m \\
&=
\gpgradetwo{ x (\gamma_a \partial^a \cdot (\gamma^k \wedge \gamma^m)) } x_k \partial_m \\
&=
\gpgradetwo{ x \gamma^m \partial^k } x_k \partial_m 
-\gpgradetwo{ x \gamma^k \partial^m } x_k \partial_m  \\
&=
x \wedge (\partial^k x_k \gamma^m \partial_m )
- x \wedge (\partial^m \gamma^k x_k \partial_m ) \\
\end{align*}

Okay, a bit closer.  Backpedaling with the reinsertion of the complete vector quantities we have

\begin{align}\label{eqn:bivectorSelect:goo3}
\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) } &= x \wedge (\partial^k x_k \grad ) - x \wedge (\partial^m x \partial_m ) 
\end{align}

Expanding out these two will be conceptually easier if the functional operation is made explicit.  For the first

\begin{align*}
x \wedge (\partial^k x_k \grad ) \phi
&=
x \wedge x_k \partial^k (\grad \phi)
+x \wedge ((\partial^k x_k) \grad) \phi \\
&=
x \wedge ((x \cdot \grad) (\grad \phi))
+ n (x \wedge \grad) \phi
\end{align*}

In operator form this is

\begin{align}\label{eqn:bivectorSelect:goo4}
x \wedge (\partial^k x_k \grad ) &= n (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad ) 
\end{align}

Now consider the second half of (\eqnref{eqn:bivectorSelect:goo3}).  For that we expand

\begin{align*}
x \wedge (\partial^m x \partial_m ) \phi
&=
x \wedge (x \partial_m \partial^m \phi)
+ x \wedge ((\partial^m x) \partial_m \phi)
\end{align*}

Since $x \wedge x = 0$, and $\partial^m x = \partial^m x_k \gamma^k = \gamma^m$, we have

\begin{align*}
x \wedge (\partial^m x \partial_m ) \phi
&=
x \wedge (\gamma^m \partial_m ) \phi \\
&=
(x \wedge \grad) \phi
\end{align*}

Putting things back together we have for (\eqnref{eqn:bivectorSelect:goo3})

%\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) } &= x \wedge (\partial^k x_k \grad ) - x \wedge (\partial^m x \partial_m ) 
%x \wedge (\partial^k x_k \grad ) &= n (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad ) 
%x \wedge (\partial^m x \partial_m ) \phi &= (x \wedge \grad) \phi

\begin{align}\label{eqn:bivectorSelect:goo5}
\gpgradetwo{ x (\grad \cdot (x \wedge \grad)) } &= (n-1) (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad ) 
\end{align}

This now completes a fair amount of the bivector selection, and a substitution back into (\eqnref{eqn:bivectorSelect:goo2}) yields

\begin{align}\label{eqn:bivectorSelect:goo6}
\gpgradetwo{ (x \wedge \grad)^2 }
&=
(n-1 - x \cdot \grad) (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad ) 
+ x \cdot (\grad \wedge (x \wedge \grad)) 
\end{align}

The remaining task is to explicitly expand the last vector-trivector dot product.  To do that we use the basic alternation expansion identity

\begin{align}\label{eqn:bivectorSelect:goo7}
a \cdot (b \wedge c \wedge d)
&= 
(a \cdot b) (c \wedge d)
-(a \cdot c) (b \wedge d)
+(a \cdot d) (b \wedge c)
\end{align}

To see how to apply this to the operator case lets write that explicitly but temporarily in coordinates

\begin{align*}
x \cdot ((\grad \wedge (x \wedge \grad)) \phi
&=
(x^\mu \gamma_\mu) \cdot ((\gamma^\nu \partial_\nu ) \wedge (x_\alpha \gamma^\alpha \wedge (\gamma^\beta \partial_\beta))) \phi \\
&=
x \cdot \grad (x \wedge \grad) \phi
-
x \cdot \gamma^\alpha \grad \wedge x_\alpha \grad \phi
+
x^\mu \grad \wedge x \gamma_\mu \cdot \gamma^\beta \partial_\beta  \phi \\
&=
x \cdot \grad (x \wedge \grad) \phi
-
x^\alpha \grad \wedge x_\alpha \grad \phi
+
x^\mu \grad \wedge x \partial_\mu  \phi
\end{align*}

Considering this term by term starting with the second one we have

\begin{align*}
x^\alpha \grad \wedge x_\alpha \grad \phi
&=
x_\alpha (\gamma^\mu \partial_\mu) \wedge x^\alpha \grad \phi \\
&=
x_\alpha \gamma^\mu \wedge (\partial_\mu x^\alpha) \grad \phi 
+x_\alpha \gamma^\mu \wedge x^\alpha \partial_\mu \grad \phi  \\
&=
x_\mu \gamma^\mu \wedge \grad \phi 
+x_\alpha x^\alpha \gamma^\mu \wedge \partial_\mu \grad \phi  \\
&=
x \wedge \grad \phi 
+x^2 \grad \wedge \grad \phi  \\
\end{align*}

The curl of a gradient is zero, since summing over an product of antisymmetric and symmetric indexes $\gamma^\mu \wedge \gamma^\nu \partial_{\mu\nu}$ is zero.  Only one term remains to evaluate in the vector-trivector dot product now

\begin{align}\label{eqn:bivectorSelect:goo8}
x \cdot (\grad \wedge x \wedge \grad) 
&=
(-1 + x \cdot \grad )(x \wedge \grad) 
+
x^\mu \grad \wedge x \partial_\mu  
\end{align}

Again, a completely dumb and brute force expansion of this is

\begin{align*}
x^\mu \grad \wedge x \partial_\mu \phi
&=
x^\mu (\gamma^\nu \partial_\nu) \wedge (x^\alpha \gamma_\alpha) \partial_\mu \phi \\
&=
x^\mu \gamma^\nu \wedge (\partial_\nu (x^\alpha \gamma_\alpha)) \partial_\mu \phi 
+x^\mu \gamma^\nu \wedge (x^\alpha \gamma_\alpha) \partial_\nu \partial_\mu \phi \\
&=
x^\mu (\gamma^\alpha \wedge \gamma_\alpha) \partial_\mu \phi 
+x^\mu \gamma^\nu \wedge x \partial_\nu \partial_\mu \phi \\
\end{align*}

With $\gamma^\mu = \pm \gamma_\mu$, the wedge in the first term is zero, leaving

\begin{align*}
x^\mu \grad \wedge x \partial_\mu \phi
&=
-x^\mu x \wedge \gamma^\nu \partial_\nu \partial_\mu \phi \\
&=
-x^\mu x \wedge \gamma^\nu \partial_\mu \partial_\nu \phi \\
&=
-x \wedge x^\mu \partial_\mu \gamma^\nu \partial_\nu \phi \\
\end{align*}

In vector form we have finally

\begin{align}\label{eqn:bivectorSelect:goo9}
x^\mu \grad \wedge x \partial_\mu \phi &= -x \wedge (x \cdot \grad) \grad \phi 
\end{align}

The final expansion of the vector-trivector dot product is now

\begin{align}\label{eqn:bivectorSelect:goo10}
x \cdot (\grad \wedge x \wedge \grad) 
&=
(-1 + x \cdot \grad )(x \wedge \grad) 
-x \wedge (x \cdot \grad) \grad \phi 
\end{align}

This was the last piece we needed for the bivector grade selection.  Incorporating this into (\eqnref{eqn:bivectorSelect:goo6}), both the $x \cdot \grad x \wedge \grad$, and the $x \wedge (x \cdot \grad) \grad$ terms cancel leaving the surprising simple result

\begin{align}\label{eqn:bivectorSelect:goo11}
\gpgradetwo{ (x \wedge \grad)^2 }
%&=
%(n-1 - x \cdot \grad) (x \wedge \grad) + x \wedge ((x \cdot \grad) \grad ) 
%%+ x \cdot (\grad \wedge (x \wedge \grad)) 
%+(-1 + x \cdot \grad )(x \wedge \grad) 
%-x \wedge (x \cdot \grad) \grad \phi 
&=
(n-2) (x \wedge \grad) 
\end{align}

The power of this result is that it allows us to write the scalar angular momentum operator from the Laplacian as

\begin{align*}
\gpgradezero{ (x \wedge \grad)^2 } 
&= (x \wedge \grad)^2 - \gpgradetwo{ (x \wedge \grad)^2 } - (x \wedge \grad) \wedge (x \wedge \grad) \\
&= (x \wedge \grad)^2 - (n-2) (x \wedge \grad) - (x \wedge \grad) \wedge (x \wedge \grad) \\
&= (-(n-2) + (x \wedge \grad) - (x \wedge \grad) \wedge ) (x \wedge \grad) 
\end{align*}

The complete Laplacian is

\begin{align}\label{eqn:bivectorSelect:goo12}
\grad^2 &= \inv{x^2} (x \cdot \grad)^2 + (n - 2) \inv{x} \cdot \grad 
- \inv{x^2} 
\left(
(x \wedge \grad)^2 - (n-2) (x \wedge \grad) - (x \wedge \grad) \wedge (x \wedge \grad) 
\right)
\end{align}

In particular in less than four dimensions the quad-vector term is necessarily zero.  The 3D Laplacian becomes

\begin{align}\label{eqn:bivectorSelect:goo13}
\spacegrad^2 &= \inv{\Bx^2} (1 + \Bx \cdot \spacegrad)(\Bx \cdot \spacegrad)
+ \inv{\Bx^2} (1 - \Bx \wedge \spacegrad) (\Bx \wedge \spacegrad) 
\end{align}

So any eigenfunction of the bivector angular momentum operator $\Bx \wedge \spacegrad$ is necessarily a simultaneous eigenfunction of the scalar operator.

%%\EndArticle
%\EndNoBibArticle
