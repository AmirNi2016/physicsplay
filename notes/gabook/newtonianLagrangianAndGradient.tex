\documentclass{article}

\input{../peeters_macros.tex}
\newcommand{\grad}[0]{\nabla}
\newcommand{\PD}[2]{ \frac{\partial{#1}}{\partial {#2}} }

\title{ Derivation of Newton's Law from Lagrangian and general gradient.}
\author{Peeter Joot \quad peeter.joot@gmail.com}
\date{August 9, 2008}

\begin{document}

\maketitle{}

\section{}

In the classical limit the Lagrangian action for a point particle in a general
position dependent field is:

\begin{equation}
S = \inv{2} m\Bv^2 - \varphi
\end{equation}

Given the Lagrange equations that minimize the action, it is fairly simple
to derive the Newtonian force law.

\begin{align*}
0
&= \PD{S}{x^i} - \frac{d}{dt}\PD{S}{\dot{x}^i} \\
&= -\PD{\varphi}{x^i} - \frac{d}{dt}\left(m \dot{x}^i\right)
\end{align*}

Multiplication of this result with the unit vector $\Be_i$, and summing over
all unit vectors we have:

\begin{equation*}
\sum \Be_i \frac{d}{dt}\left(m \dot{x}^i\right) = - \sum \Be_i \PD{\varphi}{x^i}
\end{equation*}

Or, using the gradient operator, and writing $\Bv = \sum \Be_i \dot{x}^i$, we have:

\begin{equation}
\BF = \frac{d (m \Bv)}{dt} = - \grad \varphi
\end{equation}

\subsection{ The mistake hiding above. }

Now, despite the use of upper and lower pairs of indexes for the basis vectors and coordinates, this
result is not valid for a general set of basis vectors.  This initially confused the author, since the RHS
sum $\Bv = \sum \Be_i v^i$ is valid for any set of basis vectors independent of the orthonormality of that
set of basis vectors.  This is assuming that these coordinate pairs follow the usual reciprocal relationships:

\begin{equation*}
\Bx = \sum \Be_i x^i
\end{equation*}
\begin{equation*}
x^i = \Bx \cdot \Be^i
\end{equation*}
\begin{equation*}
\Be^i \cdot \Be_j = {\delta^i}_j
\end{equation*}

However, the LHS that implicitly defines the gradient as:
\begin{equation*}
\grad = \sum \Be_i \PD{}{x^i}
\end{equation*}

is a result that is only valid when the set of basis vectors $\Be_i$ is orthonormal.  The general result is
expected instead to be:

\begin{equation*}
\grad = \sum \Be^i \PD{}{x^i}
\end{equation*}

This is how the gradient is defined (without motivation) in Doran/Lasenby.  One can however demonstrate that this definition, and not $\grad = \sum \Be_i \PD{}{x^i}$, is required by doing a computation of something like $\grad \norm{\Bx}^\alpha$ with $\Bx = \sum x^i \Be_i$ for a general basis $\Be_i$ to demonstrate this.  An example of this can be found in the appendix below.

So where did things go wrong?  It was in one of the ``obvious'' skipped steps: $\Bv = \sum \dot{x^i} \dot{x^i}$.  It is in that
spot where there is a hidden orthonormal frame vector requirement since a general basis will have mixed product terms too
(ie: non-diagonal metric tensor).

Expressed in full for general frame vectors the action to minimize is the following:

\begin{equation}
S = \inv{2} m \sum \dot{x}^i \dot{x}^j \Be_i \cdot \Be_j -\varphi
\end{equation}

Or, expressed using a metric tensor $g_{ij} = \Be_i \cdot \Be_j$, this is:

\begin{equation}
S = \inv{2} m \sum \dot{x}^i \dot{x}^j g_{ij} -\varphi
\end{equation}

\subsection{ Equations of motion for vectors in a general frame. }

Now we are in shape to properly calculate the equations of motion from the Lagrangian action minimization equations.

\begin{align*}
0
&= \PD{S}{x^k} - \frac{d}{dt}\PD{S}{\dot{x}^k} \\
&= -\PD{\varphi}{x^k} - \frac{d}{dt} \left( \inv{2} m \sum g_{ij} \PD{\dot{x}^i}{\dot{x}^k} \dot{x}^j + \dot{x}^i \PD{\dot{x}^j}{\dot{x}^k} \right) \\
&= -\PD{\varphi}{x^k} - \frac{d}{dt} \left( \inv{2} m \sum g_{ij} ({\delta^i}_k \dot{x}^j + \dot{x}^i {\delta^j}_k) \right) \\
&= -\PD{\varphi}{x^k} - \frac{d}{dt} \left( \inv{2} m \sum (g_{kj} \dot{x}^j + g_{ik}\dot{x}^i) \right) \\
&= -\PD{\varphi}{x^k} - \frac{d}{dt} \left( m \sum g_{kj} \dot{x}^j \right) \\
\frac{d}{dt} \left( m \sum g_{kj} \dot{x}^j \right) &= -\PD{\varphi}{x^k} \\
\sum \Be^k \frac{d}{dt} \left( m \sum g_{kj} \dot{x}^j \right) &= - \sum \Be^k \PD{\varphi}{x^k} \\
\frac{d}{dt} \left( m \sum_j \dot{x}^j \underbrace{\sum_k \Be^k \Be_k \cdot \Be_j}_{=\Be_j} \right) &= \\
\frac{d}{dt} \left( m \sum_j \dot{x}^j \Be_j \right) &= \\
\end{align*}

The requirement for reciprocal pairs of coordinates and basis frame vectors is due to the
summation $\Bv = \sum \Be_i \dot{x}^i$, and it allows us to write all of the Lagrangian equations
in vector form for an arbitrary frame basis as:

\begin{equation}
\BF = \frac{d (m \Bv)}{dt} = - \sum \Be^k \PD{\varphi}{x^k}
\end{equation}

If we are calling this RHS a gradient relationship in an orthonormal frame, we therefore must define
the following as the gradient for the general frame:

\begin{equation}\label{eqn:gradient}
\grad = \sum \Be^k \PD{}{x^k}
\end{equation}

The Lagrange equations that minimize the action still generate equations of
motion that hold when the coordinate and basis vectors cannot be summed in this fashion.
In such a case, however, the ability to merge the generalized coordinate equations of motion into a single
vector relationship will not be possible.

\section{ Appendix.  Scratch calculations. }

\subsection{ frame vector in terms of metric tensor, and reciprocal pairs. }

\begin{align*}
e_j &= \sum a_k e^k \\
e_j \cdot e_k &= \sum a_i e^i \cdot e_k \\
e_j \cdot e_k &= a_k \\
\implies & \\
e_j &= \sum e_j \cdot e_k e^k \\
e_j &= \sum g_{jk} e^k
\end{align*}

\subsection{ Gradient calculation for an absolute vector magnitude function. }

As a verification that the gradient as defined in equation \ref{eqn:gradient} works as expected, lets do a calculation that we know the answer as computed with an
orthonormal basis.

\begin{equation*}
f(\Br) = \norm{\Br}^\alpha
\end{equation*}

\begin{align*}
\grad f(\Br) 
&= \grad \norm{\Br}^\alpha \\
&= \sum \Be^k \PD{} {x^k} {{\left(\sum x^i x^j g_{ij}\right)}^{\alpha/2}} \\
&= \frac{\alpha}{2} \sum \Be^k {{\left(\sum x^i x^j g_{ij}\right)}^{\alpha/2 -1}} \quad \PD{}{x^k} {\left(\sum x^i x^j g_{ij}\right)} \\
&= \alpha \norm{\Br}^{\alpha-2} \sum \Be^k x^i g_{ki} \\
&= \alpha \norm{\Br}^{\alpha-2} \sum_i x^i \underbrace{\sum_k \Be^k \Be_k \cdot \Be_i}_{=\Be_i} \\
&= \alpha \norm{\Br}^{\alpha-2} \Br
\end{align*}

\end{document}
