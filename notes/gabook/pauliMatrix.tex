\documentclass{article}

\input{../peeters_macros.tex}
\input{../peeters_macros2.tex}

\newcommand{\Clifford}[2]{\mathcal{C}_{\{{#1},{#2}\}}}
\DeclareMathOperator{\tr}{tr}
\newcommand{\trace}[1]{\tr{#1}}
\newcommand{\traceB}[1]{\tr\left({#1}\right)}
\newcommand{\symmetric}[2]{{\{{#1},{#2}\}}}
\newcommand{\antisymmetric}[2]{[{#1},{#2}]}

\newcommand{\PauliI}[0]{
\begin{bmatrix}
1 & 0 \\
0 & 1 \\
\end{bmatrix}
}

\newcommand{\PauliX}[0]{
\begin{bmatrix}
0 & 1 \\
1 & 0 \\
\end{bmatrix}
}

\newcommand{\PauliY}[0]{
\begin{bmatrix}
0 & -i \\
i & 0 \\
\end{bmatrix}
}

\newcommand{\PauliYNoI}[0]{
\begin{bmatrix}
0 & -1 \\
1 & 0 \\
\end{bmatrix}
}

\newcommand{\PauliZ}[0]{
\begin{bmatrix}
1 & 0 \\
0 & -1 \\
\end{bmatrix}
}

\usepackage[bookmarks=true]{hyperref}

\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package. 
\usepackage{latexsym,amssymb,epsf} % don't remember if these are
   % needed, but their inclusion can't do any damage


\title{Translating from Pauli Matrixes to Clifford Algebra}
\author{Peeter Joot}
\date{ Dec 06, 2008.  Last Revision: $Date: 2008/12/07 15:47:03 $ }

\begin{document}
\maketitle{}

\tableofcontents

\section{ Motivation. }

Having learned Clifford Algebra from 
\cite{doran2003gap}, \cite{hestenes1999nfc}, \cite{dorst2007gac}, and other sources
before studying quantum mechanics, trying to work with (and talk to people familiar with) the Pauli and Dirac matrix notation as used in
traditional quantum mechanics becomes difficult.

The aim of this document is to work through equivalents to many Clifford algebra expressions entirely in
commutator and anticommutator notations, instead of the dot and wedge products of Clifford algebra.

\section{ Pauli Matrices }

The matrixes in question are:

\begin{align}
\sigma_1 &= \PauliX \\
\sigma_2 &= \PauliY \\
\sigma_3 &= \PauliZ
\end{align}

These all have positive square as do the traditional Euclidean unit vectors $\Be_i$, and so can be used algebraically as a vector basis for \R{3}.  So any vector that we
can write in coordinates

\begin{align*}
\Bx = x^i \Be_i,
\end{align*}

we can equivalently write (an isomorphism) in terms of the Pauli matrixes

\begin{align}\label{eqn:vectorInPauliBasis}
x = x^i \sigma_i.
\end{align}

\subsection{ Pauli Vector. }
%Pauli Matrix article
\cite{wikipauli} introduces the Pauli vector as a mechanism for mapping between a vector basis and this matrix basis

\begin{align*}
\Bsigma = \sum \sigma_i \Be_i
\end{align*}

This is a curious looking construct with products of $2 x 2$ matrices and \R{3} vectors.  Obviously these are not the usual $3 x 1$ column vector representations.  This 
Pauli vector is thus 
really a notational construct.  If one takes the dot product of a vector expressed using the standard orthonormal Euclidian basis $\{\Be_i\}$ basis, and then
takes the dot product with the Pauli matrix in a mechanical fashion

\begin{align*}
\Bx \cdot \Bsigma &=
(x^i \Be_i) \cdot \sum \sigma_j \Be_j \\
&= \sum_{i,j} x^i \sigma_j \Be_i \cdot \Be_j \\
&= x^i \sigma_i \\
\end{align*}

one arrives at the matrix representation of the vector in the Pauli basis $\{\sigma_i\}$.  Does this construct have any value?  That I don't know, but for the rest of 
these notes the coordinate representation as in equation \ref{eqn:vectorInPauliBasis} will be used directly.

\subsection{ Matrix squares }

It was stated that the Pauli matrices have unit square.  Direct calculation of this
is straightforward, and confirms the assertion

\begin{align*}
{\sigma_1}^2 &= \PauliX \PauliX = \PauliI = I \\
{\sigma_2}^2 &= \PauliY \PauliY = i^2 \PauliYNoI \PauliYNoI = \PauliI = I \\
{\sigma_3}^2 &= \PauliZ \PauliZ = \PauliI = I \\
\end{align*}

Note that unlike the vector (Clifford) square the unit matrix is not a scalar.

\subsection{ Length }
If we are to
operate with Pauli matrixes how do we express our most basic vector operation,
the length?

Examining a vector lying along one direction, say, $\Ba = \alpha\xcap$ we expect

\begin{align*}
\Ba^2 = \Ba \cdot \Ba = \alpha^2 \xcap \cdot \xcap = \alpha^2.
\end{align*}

Lets contrast this to the Pauli square for the same vector $y = \alpha\sigma_1$

\begin{align*}
\Ba^2 = \alpha^2 {\sigma_1}^2
\end{align*}

The wiki article mentions trace, but no application for it.  One application for the trace that one can observe is as a mechanism to convert a diagonal
matrix to a scalar.
In particular, and for this identity matrix one has

\begin{align*}
\alpha^2 = \inv{2} \traceB{\Ba^2}
\end{align*}

So it is plausible to guess that we may be able to express (squared) length as

\begin{align*}
\Abs{x}^2 = \inv{2}\traceB{x^2}
\end{align*}

Let's see if this works by performing the coordinate expansion

\begin{align*}
x^2
&= (x^i \sigma_i)(x^j \sigma_j) \\
&= x^i x^j \sigma_i \sigma_j \\
&= \sum_{i < j} x^i x^j (\sigma_i \sigma_j + \sigma_j \sigma_i) + \sum_i (x^i)^2 {\sigma_i}^2 \\
\end{align*}

As an algebra that is isomorphic to the Clifford Algebra $\Clifford{3}{0}$ it is expected that the $\sigma_i \sigma_j$ matrixes anticommute for $i \ne j$.  Let's verify this

\begin{align*}
\begin{array}{l l l l}
\sigma_1 \sigma_2 &= i \PauliX \PauliYNoI    &= i \PauliZ                                  &=  i \sigma_3 \\
\sigma_2 \sigma_1 &= i \PauliYNoI \PauliX    &= i \begin{bmatrix}-1 & 0 \\0 & 1\end{bmatrix} &= -i \sigma_3 \\
\sigma_3 \sigma_1 &= \PauliZ \PauliX         &=   \begin{bmatrix}0 & 1 \\-1 & 0\end{bmatrix} &=  i \sigma_2 \\
\sigma_1 \sigma_3 &= \PauliX \PauliZ         &=   \begin{bmatrix}0 & -1 \\1 & 0\end{bmatrix} &= -i \sigma_2 \\
\sigma_2 \sigma_3 &= i \PauliYNoI \PauliZ    &= i \PauliX                                  &=  i \sigma_1 \\
\sigma_3 \sigma_2 &= i \PauliZ \PauliYNoI    &= i \begin{bmatrix}0 & -1 \\-1 & 0\end{bmatrix} &= -i \sigma_3 \\
\end{array}
\end{align*}

Thus the sum over the $\{i < j\} = \{12, 23, 13\}$ indexes above is zero.  As a side note here it is also worth observing that
the table of products above can be summarized as

\begin{align}
\sigma_a \sigma_b = 2 i \epsilon_{abc} \sigma_c
\end{align}

Having computed this, our vector square is left with the value

\begin{align*}
x^2 = \sum_i (x^i)^2 I \\
\end{align*}

and the intuition that the length can be expressed using the trace of the squared vector was correct.  Summarizing this, we have for the length of a vector in the Pauli basis

\begin{align}
\Abs{x}^2 = \inv{2} \traceB{x^2} = \sum_i (x^i)^2
\end{align}

\subsection{ Scalar product. }

Having found the expression for the length of a vector in the Pauli basis, the next logical desirable identity is the dot product.  One can guess that
this will be the trace of a scaled symmetric product, but can motivate this without guessing in the usual fashion, by calculating the length of an 
orthonormal sum.

Consider first the length of a general vector sum.  To calculate this we first wish to calculate the matrix square of this sum.

\begin{align*}
(x + y)^2 &= x^2 + y^2 + x y + y x \\
\end{align*}

If these vectors are perpendicular this equals $x^2 + y^2$.  Thus orthonormality implies that
\begin{align*}
x y + y x &= 0 \\
\end{align*}

or,

\begin{align}
y x &= - x y
\end{align}

We have already observed this by direct calculation for the Pauli matrixes themselves.  Now, this is not any different than the usual description of perpendicularity in a Clifford Algebra, and it is notable that there are not any references to matrixes in this argument.  One only requires that a well defined vector
product exists, where the squared vector has a length interpretation.

One matrix dependent observation that can be made is that since the left hand side and the $x^2$, and $y^2$ terms are all diagonal, this symmetric sum must also be diagonal.  Additionally, for the length of this vector sum we then have

\begin{align*}
{\Abs{x + y}}^2
%&= x^2 + y^2 + x y + y x \\
&= \Abs{x}^2 + \Abs{y}^2 + \inv{2}\traceB{x y + y x} \\
\end{align*}

For correspondance with the Euclidean scalar product of two vectors we must then have

\begin{align*}
2 \gpgradezero{x y} &= \inv{2}\traceB{x y + y x}
\end{align*}

Or
\begin{align}
\gpgradezero{x y} &= \inv{4}\traceB{x y + y x}
\end{align}

Here the Clifford algebra grade zero (scalar part) selection notation $\gpgradezero{x y}$ has been used so that the dot product notation $x \cdot y$ can be reserved for an application that is more natural in the Pauli algebra context.

Also note that the symmetric product that we find in the dot product definition, is known in physics as the anticommutator, where the commutator
is the antisymmetric sum.  In the physics notation the anticommutator (symmetric sum) is

\begin{align}\label{eqn:anticommutator}
\symmetric{x}{y} &= x y + y x
\end{align}

So the scalar product can be written

\begin{align}
\gpgradezero{ x y } = \inv{4}\trace{\symmetric{x}{y}}
\end{align}

Similarily, the commutator, an antisymetric product, is denoted:
\begin{align}\label{eqn:commutator}
\antisymmetric{x}{y} &= x y - y x,
\end{align}

A close relationship between this and the wedge product of Clifford Algebra is
expected.

\subsection{ Symmetric and antisymmetric split. }

As with the Clifford product, the symmetric and antisymmetric split of a vector
product is a useful concept.  This can be used to write the product 
of two Pauli basis vectors in terms of the anticommutator and commutator 
products

\begin{align}
x y &= \inv{2} \symmetric{x}{y} + \inv{2} \antisymmetric{x}{y} \\
y x &= \inv{2} \symmetric{x}{y} - \inv{2} \antisymmetric{x}{y}
\end{align}

These follows from the definition of the anticommutator 
\ref{eqn:anticommutator}
and commutator 
\ref{eqn:commutator}
products above, and are the equivalents of the Clifford symmetric and antisymmetric split into dot and wedge products

\begin{align}
x y &= {x} \cdot {y} + {x} \wedge {y} \\
y x &= {x} \cdot {y} - {x} \wedge {y}
\end{align}

Where the dot and wedge products are respectively

\begin{align*}
x \cdot y &= \inv{2}(x y + y x) \\
x \wedge y &= \inv{2}(x y - y x)
\end{align*}

Note the factor of two differences in the two algebraic notations.  In particular very handy Clifford vector product reversal formula

\begin{align*}
y x = - x y + 2 x \cdot y
\end{align*}

has no factor of two in its Pauli anticommutator equivalent

\begin{align}
y x = - x y + \symmetric{x}{y}
\end{align}

\subsection{ Vector inverse. }

It has been observed that the square of a vector is diagonal in this matrix representation, and can therefore be inverted for any non-zero vector

\begin{align*}
x^2 &= \Abs{x}^2 I \\
(x^2)^{-1} &= \Abs{x}^{-2} I \\
\implies \\
x^2 (x^2)^{-1} &= I \\
\end{align*}

So it is therefore quite justifiable to define
\begin{align*}
x^{-2} = \inv{x^2} \equiv \Abs{x}^{-2} I \\
\end{align*}

This allows for the construction of a dual sided vector inverse operation.

\begin{align*}
x^{-1}
&\equiv \inv{\Abs{x}^2} x \\
&= \inv{x^2} x \\
&= x \inv{x^2} \\
\end{align*}

This inverse is a scaled version of the vector itself.

The diagonality of the squared matrix or the inverse of that allows for commutation with x.  This diagonality plays the same role as the scalar in a regular Clifford square.  In either case the square can commute with the vector, and that 
commutation allows the inverse to have both left and right sided action.

Note that like the Clifford vector inverse when the vector is multiplied with this inverse, the product resides outside of the proper \R{3} Pauli basis since the identity matrix is required.

\subsection{ Coordinate extraction. }

Given a vector in the Pauli basis, we can extract the coordinates using the scalar product

\begin{align*}
x = \sum_i \inv{4}\trace{\symmetric{x}{\sigma_i}} \sigma_i
\end{align*}

But do not need to convert to strict scalar form if we are multiplying by a Pauli matrix.  So in anticommutator notation
this takes the form

\begin{align*}
x &= x^i \sigma_i = \sum_i \inv{2}\symmetric{x}{\sigma_i} \sigma_i \\
x^i &= \inv{2}\symmetric{x}{\sigma_i}
\end{align*}

\subsection{ Projection and rejection. }

The usual Clifford algebra trick for projective and rejective split maps naturally to matrix form.  Write

\begin{align*}
x 
&= x a a^{-1} \\
&= (x a) a^{-1} \\
&= \left( \inv{2}\symmetric{x}{a} + \inv{2}\antisymmetric{x}{a} \right) a^{-1} \\
&= \left( \inv{2}\left(x a + a x \right) + \inv{2} \left(x a - a x \right) \right) a^{-1} \\
&= \inv{2}\left(x + a x a^{-1} \right) + \inv{2} \left(x - a x a^{-1} \right) \\
\end{align*}

Since $\symmetric{x}{a}$ is diagonal, this first term is proportional to $a^{-1}$, and thus lines in the direction of $a$ itself.
The second term is perpendicular to $a$.

These are in fact the projection of $x$ in the direction of $a$ and rejection of $x$ from the direction of $a$ respectively.

\begin{align*}
x &= x_\parallel + x_\perp \\
x_\parallel &= \Proj_a(x) = \inv{2}\symmetric{x}{a}a^{-1} = \inv{2}\left(x + a x a^{-1} \right) \\
x_\perp &= \RejName_a(x) = \inv{2} \antisymmetric{x}{a} a^{-1} = \inv{2} \left( x - a x a^{-1} \right) \\
\end{align*}

To complete the verification of this note that the perpendicularity of the $x_\perp$ term can be verified by taking dot products

\begin{align*}
\inv{2}\symmetric{a}{x_\perp} 
&= \inv{4}\left( a \left(x - a x a^{-1} \right) +\left(x - a x a^{-1} \right) a \right) \\
&= \inv{4}\left( a x - a a x a^{-1} + x a - a x a^{-1} a \right) \\
&= \inv{4}\left( a x - x a + x a - a x \right) \\
&= 0
\end{align*}

\subsection{ Space of the vector product. }

Expansion of the anticommutator and commutator in coordinate form shows that these entities lie in a different space than the vectors itself.

For real coordinate vectors in the Pauli basis, all the commutator values are imaginary multiples and thus not representable

\begin{align*}
\antisymmetric{x}{y} 
&= x^a \sigma_a y^b \sigma_b - y^a \sigma_a x^b \sigma_b \\
&= (x^a y^b - y^a x^b) \sigma_a \sigma_b \\
&= 2 i (x^a y^b - y^a x^b) \epsilon_{a b c} \sigma_c
\end{align*}

Similarily, the anticommutator is diagonal, which also falls outside the Pauli vector basis:

\begin{align*}
\symmetric{x}{y} 
&= x^a \sigma_a y^b \sigma_b + y^a \sigma_a x^b \sigma_b \\
&= (x^a y^b + y^a x^b) \sigma_a \sigma_b \\
&= (x^a y^b + y^a x^b) ( I \delta_{a b} + i \epsilon_{a b c} \sigma_c) \\
&= \sum_a (x^a y^a + y^a x^a) I  
+\sum_{a<b}(x^a y^b + y^a x^b) i (\underbrace{\epsilon_{a b c} + \epsilon_{b a c}}_{=0}) \sigma_c \\
&= \sum_a (x^a y^a + y^a x^a) I,
\end{align*}

These correspond to the Clifford dot product being scalar (grade zero), and the wedge defining a grade two space, where grade expresses the minimal degree that a product can be reduced to.  By example a Clifford product of normal unit vectors such as

\begin{align*}
\Be_1 \Be_3 \Be_4 \Be_1 \Be_3 \Be_4 \Be_3 &\propto \Be_3 \\
\Be_2 \Be_3 \Be_4 \Be_1 \Be_3 \Be_4 \Be_3 \Be_5 &\propto \Be_1 \Be_2 \Be_3 \Be_5
\end{align*}

are grade one and four respectively.  The proportionality constant will be dependent on metric of the underlying vector space and the number of permutations required to group terms in pairs of matching indexes.

\subsection{ Completely antisymmetrized product. }

In a Clifford algebra no imaginary number is required to express the antisymmetric (commutator) product.  However, the bivector space can be enumerated using a dual basis defined by multiplication of the vector basis elements with the unit volume trivector.  That is also the case here and gives a geometrical meaning to the imaginaries of the Pauli formulation.

How do we even write the unit volume element in Pauli notation?  This would be

\begin{align*}
\sigma_1 \wedge \sigma_2 \wedge \sigma_3 
&= (\sigma_1 \wedge \sigma_2) \wedge \sigma_3 \\
&= \inv{2} \antisymmetric{\sigma_1}{\sigma_2} \wedge \sigma_3 \\
&= \inv{4} \left( \antisymmetric{\sigma_1}{\sigma_2} \sigma_3 + \sigma_3 \antisymmetric{\sigma_1}{\sigma_2} \right) \\
\end{align*}

So we have
\begin{align}\label{eqn:triplewedgeproduct}
\sigma_1 \wedge \sigma_2 \wedge \sigma_3 
&= \inv{8} \symmetric{\antisymmetric{\sigma_1}{\sigma_2}}{\sigma_3}
\end{align}

Similar expansion of $\sigma_1 \wedge \sigma_2 \wedge \sigma_3 = \sigma_1 \wedge (\sigma_2 \wedge \sigma_3)$, or
$\sigma_1 \wedge \sigma_2 \wedge \sigma_3 = (\sigma_3 \wedge \sigma_1) \wedge \sigma_2$
 shows that we must also have

\begin{align}
\symmetric{\antisymmetric{\sigma_1}{\sigma_2}}{\sigma_3}
= \symmetric{\sigma_1}{\antisymmetric{\sigma_2}{\sigma_3}}
= \symmetric{\antisymmetric{\sigma_3}{\sigma_1}}{\sigma_2}
\end{align}

Until now the differences in notation between the anticommutator/commutator and the dot/wedge product of the Pauli algebra and Clifford algebra respectively have only differed by factors of two, which isn't much of a big deal.  However, having to express the naturally associative wedge product operation in the non-associative looking notation of equation \ref{eqn:triplewedgeproduct} is rather unpleasant seeming.  Looking at an expression of the form gives no mnemonic
hint of the underlying associativity, and actually seems obfuscating.  I suppose that one could get used to it though.

\subsection{ Unit volume pseudoscalar. }

\subsection{ Complete algebraic space. }

% extraction of coordinates from complete complex matrix?

\bibliographystyle{plainnat}
\bibliography{myrefs}

\end{document}
