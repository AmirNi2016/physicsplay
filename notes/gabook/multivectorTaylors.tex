\documentclass{article}

\input{../peeters_macros.tex}
\input{../peeters_macros2.tex}

%\usepackage{listings}
%\usepackage{txfonts} % for ointctr... (also appears to make "prettier" \int and \sum's)
\usepackage[bookmarks=true]{hyperref}

\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package. 
\usepackage{latexsym,amssymb,epsf} % don't remember if these are
   % needed, but their inclusion can't do any damage


\title{ Developing some intuition for Multivariable Taylor Series. }
\author{Peeter Joot \quad peeter.joot@gmail.com }
\date{ April 28, 2009.  Last Revision: $Date: 2009/04/28 23:45:29 $ }

\begin{document}

\maketitle{}
\tableofcontents
\section{ Motivation. }

The book \cite{doran2003gap} uses Geometric Calculus heavily in its 
Lagrangian treament.  In particular it is used in some incomprehensible seeming ways in the 
stress energy tensor treatment.

In the treatment of transformation of the dependent variables (not the field
variables themselves) of field Lagrangians, there is one bit that
appears to be the first order linear term from a multivariable Taylor
series expansion.  Play with multivariable Taylor series here a bit
to develop some intuition with it.

\section{ Single variable case, and generalization of it. }

For the single variable case, Taylor series takes the form

\begin{align}
f(x) = \sum \frac{x^k}{k!} \left. \frac{d^k f(x)}{dx^k} \right\vert_{x=0}
\end{align}

or
\begin{align}
f(x_0 + \epsilon) = \sum \frac{\epsilon^k}{k!} \left. \frac{d^k f(x)}{dx^k} \right\vert_{x=x_0}
\end{align}

As pointed out in \cite{byron1992mca}, this can (as they demonstrated for polynomials) be put into exponential 
operator form

\begin{align}
f(x_0 + \epsilon) = \left. e^{\epsilon d/dx} f(x) \right\vert_{x=x_0}
\end{align}

Without proof, the multivector generalization of this is

\begin{align}
f(x_0 + \epsilon) 
&= \left. e^{\epsilon \cdot \grad} f(x) \right\vert_{x=x_0} \\
&= \sum \inv{k!} \left. {(\epsilon \cdot \grad)^k} f(x) \right\vert_{x=x_0}
\end{align}

Let's work with this, and develop some comfort with what it means, then revisit the proof.

\section{ Directional Derivatives. }

First a definition of directional derivative is required.

In 
\href{http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx}{standard two variable vector calculus} the directional derivative is defined in one of the following ways
\begin{align}
\spacegrad_\Bu f(x,y) &= \lim_{h \rightarrow 0} \frac{f(x + a h, y + b h) - f(x,y)}{h} \\
\Bu &= (a,b)
\end{align}

Or \href{http://en.wikipedia.org/wiki/Directional_derivative}{in a more general vector form} as 

\begin{align}
\spacegrad_\Bu f(\Bx) &= \lim_{h \rightarrow 0} \frac{f(\Bx + h\Bu) - f(\Bx)}{h}
\end{align}

Or \href{http://mathworld.wolfram.com/DirectionalDerivative.html}{in terms of the gradient} as 
\begin{align}
\spacegrad_\Bu f(\Bx) &= \frac{\Bu}{\Abs{\Bu}} \cdot \spacegrad f
\end{align}

Each of these was for a vector parameterized scalar function, although the wikipedia article does mention a vector valued form
that is identical to that use by \cite{doran2003gap}.  Specifically, that is

\begin{align}
(\epsilon \cdot \spacegrad) f(x) 
&= \lim_{h \rightarrow 0} \frac{f(x + h \epsilon) - f(x)}{h} \\
&= \left. \PD{h}{f(x + h \epsilon)} \right\vert_{h=0}
\end{align}

Observe that this defintion as a limit avoids the requirement to define the gradient upfront.  That definition is not neccessarily 
obvious especially for multivector valued functions.

\section{ Work some examples. }

\subsection{ First order linear vector polynomial. }

Let

\begin{align*}
f(x) = a + x
\end{align*}

For this simplest of vector valued vector parameterized functions we have

\begin{align*}
\PD{h}{f(x + h \epsilon)} 
&= \PD{h}{} (a + x + h \epsilon) \\
&= \epsilon \\
&= (\epsilon \cdot \grad) f
\end{align*}

with no requirement to evaluate at $h=0$ to complete the directional derivative computation.

The Taylor series expansion about $0$ is thus

\begin{align*}
f(\epsilon) 
&= (\epsilon \cdot \grad)^0 f + (\epsilon \cdot \grad)^1 f \\
&= a + \epsilon \\
\end{align*}

Nothing else could be expected.

\subsection{ Second order vector parameterized multivector polynomial. }

Now, step up the complexity slightly, and introduce a multivector valued second degree polynomial, say,

\begin{align}\label{eqn:secondOrder}
f(x) = \alpha + a + x y + w x + c x^2 + d x e + x g x
\end{align}

Here $\alpha$ is a scalar, and all the other variables are vectors, so we have grades $\le 3$.

For the first order partial we have
\begin{align*}
\PD{h}{f(x + h \epsilon)} 
&= \PD{h}{} ( \alpha + a + (x + h\epsilon) y + w (x + h\epsilon) + c (x + h\epsilon)^2 + d (x + h\epsilon) e + (x + h\epsilon) g (x + h\epsilon) ) \\
&= 
\epsilon y 
+ w \epsilon
+ c \epsilon (x + h\epsilon) 
+ c (x + h\epsilon) \epsilon
+ c \epsilon 
+ d \epsilon e 
+ \epsilon g (x + h\epsilon) 
+ (x + h\epsilon) g \epsilon \\
\end{align*}

Evaluation at $h=0$ we have

\begin{align*}
(\epsilon \cdot \grad) f
&=
\epsilon y 
+ w \epsilon
+ c \epsilon x 
+ c x \epsilon
+ c \epsilon 
+ d \epsilon e 
+ \epsilon g x 
+ x g \epsilon \\
\end{align*}

By inspection we have

\begin{align*}
(\epsilon \cdot \grad)^2 f
&=
+ 2 c \epsilon^2
+ 2 \epsilon g \epsilon \\
\end{align*}

Combining things forming the Taylor series expansion about the origin we should recover our function

\begin{align*}
f(\epsilon) 
&= \inv{0!} \left. (\epsilon \cdot \grad)^0 f \right\vert_{x=0} 
+ \inv{1!} \left. (\epsilon \cdot \grad)^1 f \right\vert_{x=0} 
+ \inv{2} \left. (\epsilon \cdot \grad)^2 f \right\vert_{x=0} \\
&= \inv{1} (\alpha + a) + \inv{1} (\epsilon y + w \epsilon + c \epsilon + d \epsilon e ) + \inv{2}(2 c \epsilon^2 + 2 \epsilon g \epsilon ) \\
&= \alpha + a + \epsilon y + w \epsilon + c \epsilon + d \epsilon e + c \epsilon^2 + \epsilon g \epsilon \\
%&f(x) = \alpha + a + x y + w x + c x^2 + d x e + x g x
\end{align*}

This should match \ref{eqn:secondOrder}, with an $x = \epsilon$ substituition, and does.  This provides a small verification that this form
of Taylor series does in fact work out fine when one cannot assume that the variables in question commute as is the case here in these functions
with vector factors.

\bibliographystyle{plainnat}
\bibliography{myrefs}

\end{document}
