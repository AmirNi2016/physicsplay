%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

% 
% 
\chapter{Developing some intuition for Multivariable and Multivector Taylor Series}\label{chap:PJmultiTaylors}
\date{April 28, 2009.  multivectorTaylors.tex}

The book \citep{doran2003gap} uses Geometric Calculus heavily in its 
Lagrangian treatment.  In particular it is used in some incomprehensible seeming ways in the 
stress energy tensor treatment.

In the treatment of transformation of the dependent variables (not the field
variables themselves) of field Lagrangians, there is one bit that
appears to be the first order linear term from a multivariable Taylor
series expansion.  Play with multivariable Taylor series here a bit
to develop some intuition with it.

\section{Single variable case, and generalization of it}

For the single variable case, Taylor series takes the form

\begin{align}
f(x) = \sum \frac{x^k}{k!} \left. \frac{d^k f(x)}{dx^k} \right\vert_{x=0}
\end{align}

or
\begin{align}
f(x_0 + \epsilon) = \sum \frac{\epsilon^k}{k!} \left. \frac{d^k f(x)}{dx^k} \right\vert_{x=x_0}
\end{align}

As pointed out in \citep{byron1992mca}, this can (as they demonstrated for polynomials) be put into exponential 
operator form

\begin{align}
f(x_0 + \epsilon) = \left. e^{\epsilon d/dx} f(x) \right\vert_{x=x_0}
\end{align}

Without proof, the multivector generalization of this is

\begin{align}\label{eqn:multivector_taylors:taylorsExponential}
f(x_0 + \epsilon) 
&= \left. e^{\epsilon \cdot \grad} f(x) \right\vert_{x=x_0} 
\end{align}

Or in full,

\begin{align}\label{eqn:multivector_taylors:taylorMulti}
f(x_0 + \epsilon) 
&= \sum \inv{k!} \left. {(\epsilon \cdot \grad)^k} f(x) \right\vert_{x=x_0}
\end{align}

Let's work with this, and develop some comfort with what it means, then revisit the proof.

\section{Directional Derivatives}

First a definition of directional derivative is required.

In 
\href{http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx}{standard two variable vector calculus} the directional derivative is defined in one of the following ways
\begin{align}
\spacegrad_\Bu f(x,y) &= \lim_{h \rightarrow 0} \frac{f(x + a h, y + b h) - f(x,y)}{h} \\
\Bu &= (a,b)
\end{align}

Or \href{http://en.wikipedia.org/wiki/Directional_derivative}{in a more general vector form} as 

\begin{align}
\spacegrad_\Bu f(\Bx) &= \lim_{h \rightarrow 0} \frac{f(\Bx + h\Bu) - f(\Bx)}{h}
\end{align}

Or \href{http://mathworld.wolfram.com/DirectionalDerivative.html}{in terms of the gradient} as 
\begin{align}
\spacegrad_\Bu f(\Bx) &= \frac{\Bu}{\Abs{\Bu}} \cdot \spacegrad f
\end{align}

Each of these was for a vector parametrized scalar function, although the wikipedia article does mention a vector valued form
that is identical to that use by \citep{doran2003gap}.  Specifically, that is

\begin{align}
(\epsilon \cdot \grad) f(x) 
&= \lim_{h \rightarrow 0} \frac{f(x + h \epsilon) - f(x)}{h} \\
&= \left. \PD{h}{f(x + h \epsilon)} \right\vert_{h=0}
\end{align}

Observe that this definition as a limit avoids the requirement to define the gradient upfront.  That definition is not necessarily 
obvious especially for multivector valued functions.

\section{Work some examples}

\subsection{First order linear vector polynomial}

Let

\begin{align*}
f(x) = a + x
\end{align*}

For this simplest of vector valued vector parametrized functions we have

\begin{align*}
\PD{h}{f(x + h \epsilon)} 
&= \PD{h}{} (a + x + h \epsilon) \\
&= \epsilon \\
&= (\epsilon \cdot \grad) f
\end{align*}

with no requirement to evaluate at $h=0$ to complete the directional derivative computation.

The Taylor series expansion about $0$ is thus

\begin{align*}
f(\epsilon) 
&= \left. (\epsilon \cdot \grad)^0 f \right\vert_{x=0} + \left. (\epsilon \cdot \grad)^1 f  \right\vert_{x=0} \\
&= a + \epsilon \\
\end{align*}

Nothing else could be expected.

\subsection{Second order vector parametrized multivector polynomial}

Now, step up the complexity slightly, and introduce a multivector valued second degree polynomial, say,

\begin{align}\label{eqn:multivector_taylors:secondOrder}
f(x) = \alpha + a + x y + w x + c x^2 + d x e + x g x
\end{align}

Here $\alpha$ is a scalar, and all the other variables are vectors, so we have grades $\le 3$.

For the first order partial we have
\begin{align*}
&\PD{h}{f(x + h \epsilon)} \\
&= \PD{h}{} ( \alpha + a + (x + h\epsilon) y + w (x + h\epsilon) + c (x + h\epsilon)^2 + d (x + h\epsilon) e + (x + h\epsilon) g (x + h\epsilon) ) \\
&= 
\epsilon y 
+ w \epsilon
+ c \epsilon (x + h\epsilon) 
+ c (x + h\epsilon) \epsilon
+ c \epsilon 
+ d \epsilon e 
+ \epsilon g (x + h\epsilon) 
+ (x + h\epsilon) g \epsilon \\
\end{align*}

Evaluation at $h=0$ we have

\begin{align*}
(\epsilon \cdot \grad) f
&=
\epsilon y 
+ w \epsilon
+ c \epsilon x 
+ c x \epsilon
+ c \epsilon 
+ d \epsilon e 
+ \epsilon g x 
+ x g \epsilon \\
\end{align*}

By inspection we have

\begin{align*}
(\epsilon \cdot \grad)^2 f
&=
+ 2 c \epsilon^2
+ 2 \epsilon g \epsilon \\
\end{align*}

Combining things forming the Taylor series expansion about the origin we should recover our function

\begin{align*}
f(\epsilon) 
&= \inv{0!} \left. (\epsilon \cdot \grad)^0 f \right\vert_{x=0} 
+ \inv{1!} \left. (\epsilon \cdot \grad)^1 f \right\vert_{x=0} 
+ \inv{2} \left. (\epsilon \cdot \grad)^2 f \right\vert_{x=0} \\
&= \inv{1} (\alpha + a) + \inv{1} (\epsilon y + w \epsilon + c \epsilon + d \epsilon e ) + \inv{2}(2 c \epsilon^2 + 2 \epsilon g \epsilon ) \\
&= \alpha + a + \epsilon y + w \epsilon + c \epsilon + d \epsilon e + c \epsilon^2 + \epsilon g \epsilon \\
% cf:
%&f(x) = \alpha + a + x y + w x + c x^2 + d x e + x g x
\end{align*}

This should match \ref{eqn:multivector_taylors:secondOrder}, with an $x = \epsilon$ substitution, and does.  With the vector factors in these functions commutativity 
assumptions could not be made.  These calculations 
help provide a small verification that this form
of Taylor series does in fact work out fine with such non-commutative variables.

Observe as well that there was really no requirement in this example that $x$ or any of the other factors to be vectors.  If they were all bivectors or trivectors or some mix the calculations would have had the same results.

\section{Proof of the multivector Taylor expansion}

A peek back into \citep{hestenes1999nfc} shows that 
\ref{eqn:multivector_taylors:taylorMulti} was in fact proved, but it was done in a very
sneaky and clever way.  Rather than try to prove treat the multivector
parameters explicitly, the following scalar parametrized hybrid function
was created

\begin{align}
G(\tau) &= F(\Bx_0 + \tau\Ba)
\end{align}

The scalar parametrized function $G(\tau)$ can be Taylor expanded about
the origin, and then evaluated at $1$ resulting in 
\ref{eqn:multivector_taylors:taylorMulti} in terms of powers of $(\Ba \cdot \grad)$.  
I won't reproduce or try to
enhance that proof for myself here since it is actually 
quite clear in the text.  Obviously the trick is non-intuitive enough
that when thinking about how to prove this myself it didn't occur to me.

\section{Explicit expansion for a scalar function}

Now, despite the $a \cdot \grad$ notation being unfamiliar seeming, the 
end result isn't.  Explicit expansion of this for a vector to scalar mapping
will show this.  In fact this will also account for the 
\href{http://en.wikipedia.org/wiki/Hessian_matrix}{Hessian matrix}, as in

\begin{align*}
y=f(\mathbf{x}+\Delta\mathbf{x})\approx f(\mathbf{x}) + J(\mathbf{x})\Delta \mathbf{x} +\frac{1}{2} \Delta\mathbf{x}^\mathrm{T} H(\mathbf{x}) \Delta\mathbf{x}
\end{align*}

providing
not only the background on where this comes from, but also the so often
omitted third order and higher generalizations (most often referred to as $\cdots$).  Poking around a bit I see that the \href{http://en.wikipedia.org/wiki/Taylor_expansion}{wikipedia Taylor Series} does explicitly define the higher order case, but if 
I'd seen that before the connection to the Hessian wasn't obvious.

\subsection{Two variable case}

Rather than start with the general case, the expansion of the first few powers
of $(\Ba \cdot \spacegrad) f$ for the two variable case is enough to show the pattern.
How to further generalize this scalar function case will be clear from 
inspection.

Starting with the first order term, writing $\Ba = (a,b)$ we have

\begin{align*}
(\Ba \cdot \spacegrad) f(x,y) 
&= \left. \PD{\tau}{} f(x + a\tau, y + b\tau) \right\vert_{\tau=0} \\
&= 
\left. \left( \PD{x + a\tau}{} f(x + a\tau, y + b\tau) \PD{\tau}{(x + a\tau)} \right) \right\vert_{\tau=0} \\
&\quad+\left. \left( \PD{y + b\tau}{} f(x + a\tau, y + b\tau) \PD{\tau}{(y + b\tau)} \right) \right\vert_{\tau=0} \\
&= 
a \PD{x}{f} +b \PD{y}{f} \\
&= 
\Ba \cdot (\spacegrad f)
\end{align*}

For the second derivative operation we have
\begin{align*}
(\Ba \cdot \spacegrad)^2 f(x,y) 
&=
(\Ba \cdot \spacegrad) 
\left( (\Ba \cdot \spacegrad) f(x,y) \right) \\
&=
(\Ba \cdot \spacegrad) \left( a \PD{x}{f} +b \PD{y}{f} \right) \\
&= \left. \PD{\tau}{} \left( a \PD{x}{f}(x + a\tau, y + b\tau) + b \PD{y}{f}(x + a\tau, y + b\tau) \right) \right\vert_{\tau=0} \\
\end{align*}

Especially if one makes a temporary substitution of the partials for some other named variables, it is clear this follows as 
before, and one gets

\begin{align*}
(\Ba \cdot \spacegrad)^2 f(x,y) 
&=
a^2 \PDSq{x}{f} + b a \PDD{y}{x}{f} 
+a b \PDD{x}{y}{f} + b^2 \PDSq{y}{f} \\
\end{align*}

Similarly the third order derivative operator gives us

\begin{align*}
(\Ba \cdot \spacegrad)^3 f(x,y) 
&=
a a a \PD{x}{}\PD{x}{}\PD{x}{} f + a b a \PD{x}{}\PD{y}{}\PD{x}{}{f}  \\
\quad&+a a b \PD{x}{}\PD{y}{}\PD{x}{} f + a b b \PD{x}{}\PD{y}{}\PD{y}{}{f} \\
&\quad+b a a \PD{y}{}\PD{x}{}\PD{x}{} f + b b a \PD{y}{}\PD{y}{}\PD{x}{}{f} \\
&\quad+b a b \PD{y}{}\PD{y}{}\PD{x}{} f + b b b \PD{y}{}\PD{y}{}\PD{y}{}{f} \\
&=
a^3 \frac{\partial^3 f}{\partial x^3} 
+ 3 a^2 b \PDSq{x}{}\PD{y}{f}
+ 3 a b^2 \PD{x}{}\PDSq{y}{f} 
+b^3 \frac{\partial^3 f}{\partial y^3} 
\end{align*}

We no longer have the notational nicety of being able to use the gradient notation as was done for the first derivative term.  For the 
first and second order derivative operations, one has the
option of using the gradient and Hessian matrix notations

\begin{align}
(\Ba \cdot \spacegrad) f(x,y) &=
\transpose{\Ba} 
\begin{bmatrix}
f_{x} \\
f_{y} 
\end{bmatrix} 
\\
(\Ba \cdot \spacegrad)^2 f(x,y) 
&=
\transpose{\Ba} 
\begin{bmatrix}
f_{xx} & f_{xy} \\
f_{yx} & f_{yy}
\end{bmatrix}
\Ba
\end{align}

But this won't be helpful past the second derivative.

Additionally, if we continue to restrict oneself to the two variable case, 
it is clear that we have

\begin{align}
(\Ba \cdot \spacegrad)^n f(x,y) 
&=
\sum_{k=0}^{n} \binom{n}{k} a^{n-k} b^{k} 
\left( \PD{x}{} \right)^{n-k}
\left( \PD{y}{} \right)^{k} f(x,y)
\end{align}

But it is also clear that if we switch to more than two variables, a binomial 
series expansion of derivative powers in this fashion will no longer work.  For
example for three (or more) variables, writing for example $\Ba = (a_1, a_2, a_3)$,
we have

\begin{align}
(\Ba \cdot \spacegrad) f(\Bx)
&=
\sum_{i} 
\left( a_i \PD{x_i}{} \right)
f(\Bx) \\
(\Ba \cdot \spacegrad)^2 f(\Bx) 
&=
\sum_{ij} 
\left( a_i \PD{x_i}{} \right)
\left( a_j \PD{x_j}{} \right)
f(\Bx) \\
(\Ba \cdot \spacegrad)^3 f(\Bx) 
&=
\sum_{ijk} 
\left( a_i \PD{x_i}{} \right)
\left( a_j \PD{x_j}{} \right)
\left( a_k \PD{x_k}{} \right)
f(\Bx) 
%\\
%&\vdots
\end{align}

If the partials are all collected into a single indexed object, one really has a tensor.  For the first and second orders we
can represent this tensor in matrix form (as the gradient and Hessian respectively)

\section{Gradient with non-Euclidean basis}

The directional derivative has been calculated above for a scalar function.  There is nothing intrinsic to that argument
that requires an orthonormal basis.

Suppose we have a basis $\{\gamma_\mu\}$, and a reciprocal frame $\{\gamma^\mu\}$.  Let

\begin{align*}
x &= x^\mu \gamma_\mu = x_\mu \gamma^\mu \\
a &= a^\mu \gamma_\mu = a_\mu \gamma^\mu 
\end{align*}

The first order directional derivative is then 

\begin{align*}
(a \cdot \grad) f(x) 
&=
\left. \PD{\tau}{f}(x + \tau a) \right\vert_{\tau=0} \\
\end{align*}

This is
\begin{align}\label{eqn:multivector_taylors:gradDotF}
(a \cdot \grad) f(x) &= \sum_\mu a^\mu \PD{x^\mu}{f}(x) 
\end{align}

Now, we are used to $\grad$ as a standalone object, and want that operator defined such that we can also write \ref{eqn:multivector_taylors:gradDotF}
as
\begin{align*}
a \cdot (\grad f(x))
&=
\left(a^\mu \gamma_\mu \right) \cdot (\grad f(x))
\end{align*}

Comparing these we see that our partials in \ref{eqn:multivector_taylors:gradDotF} do the job provided that we form the vector operator

\begin{align}\label{eqn:multivector_taylors:gradForNonOrtho}
\grad &= \sum_\mu \gamma^\mu \PD{x^\mu}{}
\end{align}

The text \citep{doran2003gap} defines $\grad$ in this fashion, but has no logical motivation of this idea.  One sees
quickly enough that this definition works, and is the required form, but building up to the construction
in a way that builds on previously established ideas is still desirable.
We see here that this reciprocal frame definition of the gradient follows inevitably from the definition
of the directional derivative.  Additionally this is a definition with how 
the directional derivative is defined in a standard Euclidean space with an orthonormal basis.

\section{Work out Gradient for a few specific multivector spaces}

The directional derivative result expressed in \ref{eqn:multivector_taylors:gradDotF} holds
for arbitrarily parametrized multivector spaces, and the image space
can also be a generalized one.  However, the corresponding result
\ref{eqn:multivector_taylors:gradForNonOrtho} for the gradient itself is good only when the
parameters are vectors.  These vector parameters may be non-orthonormal,
and the function
this is applied to doesn't have to be a scalar function.

If we switch to functions parametrized by multivector spaces the vector dot
gradient notation also becomes misleading.  The natural generalization
of the Taylor expansion for such a function, instead
of \ref{eqn:multivector_taylors:taylorsExponential}, or \ref{eqn:multivector_taylors:taylorMulti} should instead
be

\begin{align}\label{eqn:multivector_taylors:taylorsExponentialScalarProd}
f(x_0 + \epsilon) 
&= \left. e^{\gpgradezero{\epsilon \grad}} f(x) \right\vert_{x=x_0} 
\end{align}

Or in full,

\begin{align}\label{eqn:multivector_taylors:taylorMultiScalarProd}
f(x_0 + \epsilon) 
&= \sum \inv{k!} \left. {\gpgradezero{\epsilon \grad}^k} f(x) \right\vert_{x=x_0}
\end{align}

One could alternately express this in a notationally less different form
using the scalar product operator instead of grade selection, if one writes

\begin{align*}
{\epsilon \stardot \grad} &\equiv \gpgradezero{\epsilon \grad} 
\end{align*}

However, regardless of the notation used, the fundamental definition 
is still going to be the same (and the same as in the vector case), which
operationally is

\begin{align*}
{\epsilon \conj \grad} f(x) = \gpgradezero{\epsilon \grad} f(x)
= \left. \PD{h}{f(x + h \epsilon)} \right\vert_{h=0}
\end{align*}

\subsection{Complex numbers}

The simplest grade mixed multivector space is that of the complex numbers.  Let's write out the
directional derivative and gradient in this space explicitly.

Writing 
\begin{align*}
z_0 &= u + i v \\
z &= x + i y \\
\end{align*}

So we have
\begin{align*}
\gpgradezero{z_0 \grad} f(z) 
&= u \PD{x}{f} + v \PD{y}{f} \\
&= u \PD{x}{f} + i v \inv{i} \PD{y}{f} \\
&= \gpgradezero{ z_0 \left( \PD{x}{} + \inv{i} \PD{y}{} \right) } f(z) \\
\end{align*}

and we can therefore identify the gradient operator as

\begin{align*}
\grad_{0,2} &= \PD{x}{} + \inv{i} \PD{y}{} 
\end{align*}

Observe the similarity here between the vector gradient for a 2D Euclidean space, where we can form complex numbers by (left) factoring out a 
unit vector, as in

\begin{align*}
\Bx 
&= e_1 x + e_2 y \\
&= e_1 ( x + e_1 e_2 y ) \\
&= e_1 ( x + i y ) \\
&= e_1 z
\end{align*}

It appears that we can form this complex gradient, by (right) factoring out of the same unit vector from the vector gradient

\begin{align*}
e_1 \PD{x}{} + e_2 \PD{y}{} 
&=
\left( \PD{x}{} + e_2 e_1 \PD{y}{} \right) e_1 \\
&=
\left( \PD{x}{} + \inv{i} \PD{y}{} \right) e_1 \\
&=
\grad_{0,2} e_1 \\
\end{align*}

So, if we write $\spacegrad$ as the \R{2} vector gradient, with $\Bx = e_1 x + e_2 y = e_1 z$ as above, we have

\begin{align*}
\spacegrad \Bx 
&= \grad_{0,2} e_1 e_1 z \\
&= \grad_{0,2} z \\
\end{align*}

This is a rather curious equivalence between 2D vectors and complex numbers.

\subsubsection{Comparison of contour integral and directional derivative Taylor series}

Having a complex gradient is not familiar from standard complex variable theory.  Then again, neither is a non-contour integral formulation
of complex Taylor series.  The two of these ought to be equivalent, which seems to imply there is a contour integral representation of the gradient
in a complex number space too (one of the Hestenes paper's mentioned this
but I didn't understand the notation).

Let's do an initial comparison of the two.  We need a reminder
of the contour integral form of the complex derivative.  For a function
$f(z)$ and its derivatives regular in a neighborhood of a point $z_0$, we can evaluate

\begin{align*}
\ointctrclockwise \frac{f(z) dz}{(z - z_0)^k} 
&=
-\inv{k-1} \ointctrclockwise {f(z) dz}\left( \inv{(z - z_0)^{k-1}} \right)' \\
&=
\inv{k-1} \ointctrclockwise {f'(z) dz}\left( \inv{(z - z_0)^{k-1}} \right) \\
&=
\inv{(k-1)(k-2)} \ointctrclockwise {f^2(z) dz}\left( \inv{(z - z_0)^{k-2}} \right) \\
&=
\inv{(k-1)(k-2)\cdots(k-n)} \ointctrclockwise {f^n(z) dz}\left( \inv{(z - z_0)^{k-n}} \right) \\
% k-n = 1 
% n = k-1
&=
\inv{(k-1)(k-2)\cdots(1)} \ointctrclockwise \frac{f^{k-1}(z) dz}{z - z_0} \\
&= \frac{2 \pi i}{(k-1)!} f^{k-1}(z_0)
\end{align*}

So we have

\begin{align*}
\left. \frac{d^k}{dz^k} f(z) \right\vert_{z_0}
&= 
\frac{k!}{2 \pi i}\ointctrclockwise \frac{f(z) dz}{(z - z_0)^{k+1}} 
\end{align*}

Given this we now have a few alternate forms of complex Taylor series

\begin{align*}
f(z_0 + \epsilon) 
&= \sum \inv{k!} \left. \gpgradezero{\epsilon \grad}^k f(z) \right\vert_{z=z_0} \\
&= \sum \inv{k!} \epsilon^k \left. \frac{d^k}{dz^k} f(z) \right\vert_{z_0} \\
&= \inv{2 \pi i} \sum \epsilon^k \ointctrclockwise \frac{f(z) dz}{(z - z_0)^{k+1}} 
\end{align*}

Observe that the the $0,2$ subscript for the gradient has been dropped above (ie: this is the complex gradient, not the vector 
form).

\subsubsection{Complex gradient compared to the derivative}

A gradient operator has been identified by factoring it out of the directional derivative.  Let's compare this to a
plain old complex derivative.

\begin{align*}
f'(z_0) &= \lim_{z \rightarrow z_0} \frac{ f(z) - f(z_0) }{ z - z_0}
\end{align*}

In particular, evaluating this limit for $z = z_0 + h$, approaching $z_0$ along the x-axis, we have

\begin{align*}
f'(z_0) 
&= \lim_{z \rightarrow z_0} \frac{ f(z) - f(z_0) }{ z - z_0} \\
&= \lim_{h \rightarrow 0} \frac{ f(z_0 + h) - f(z_0) }{ h } \\
&= \PD{x}{f}(z_0)
\end{align*}

Evaluating this limit for $z = z_0 + i h$, approaching $z_0$ along the y-axis, we have

\begin{align*}
f'(z_0) 
&= \lim_{h \rightarrow 0} \frac{ f(z_0 + i h) - f(z_0) }{ i h } \\
&= -i \PD{y}{f}(z_0)
\end{align*}

We have the Cauchy equations by equating these, and if the derivative exists (ie: independent of path) we require at least

\begin{align*}
\PD{x}{f}(z_0) =
-i \PD{y}{f}(z_0)
\end{align*}

Or
\begin{align*}
0 
&=
\PD{x}{f}(z_0) + i \PD{y}{f}(z_0) \\
&=
\tilde{\grad} f(z_0)
\end{align*}

Premultiplying by $\grad$ produces the harmonic equation

\begin{align*}
\grad \tilde{\grad} f = \left( \PDSq{x}{} + \PDSq{y}{} \right) f
\end{align*}

\subsubsection{First order expansion around a point}

The above, while interesting or curious, 
doesn't provide a way to express the differential operator directly in terms of the gradient.

We can write 
\begin{align*}
\left. \gpgradezero{ \epsilon \grad } f(z) \right\vert_{z_0}
&=
\frac{\epsilon}{ 2 \pi i } \ointctrclockwise \frac{f(z) dz}{(z - z_0)^{2}} \\
&=
\epsilon f'(z_0)
\end{align*}

One can probably integrate this in some circumstances (perhaps when f(z) is regular along the straight path from $z_0$ to $z = z_0 + \epsilon$).  If so, then we have

\begin{align*}
\epsilon \int_{s=z_0}^{z} f'(s) ds &= \int_{s=z_0}^{z} \left. \gpgradezero{ \epsilon \grad } f(z) \right\vert_{z=s} ds 
\end{align*}

Or
\begin{align*}
f(z) &= f(z_0) + \int_{s=z_0}^{z} \left. \inv{\epsilon}\gpgradezero{ \epsilon \grad } f(z) \right\vert_{z=s} ds 
\end{align*}

Is there any validity to doing this?  The idea here is to play with some circumstances where we could see 
where the multivector gradient may show up.  Much more play is required, some of which for discovery and the rest
to do things more rigorously.

\subsection{4D scalar plus bivector space}

Suppose we form a scalar, bivector space by factoring out the unit time vector in a Dirac vector representation

\begin{align*}
x 
&= x^\mu \gamma_\mu \\
&= \left( x^0 + x^k \gamma_k \gamma_0 \right) \gamma_0 \\
&= \left( x^0 + x^k \sigma_k \right) \gamma_0 \\
&= q \gamma_0 \\
\end{align*}

This $q$ has the structure of a quaternion-like object (scalar, plus bivector), but the bivectors all have positive square.  Our directional
derivative, for multivector direction $Q = Q^0 + Q^k \sigma_k$ is 

\begin{align*}
\gpgradezero{Q \grad} f(q) 
&= Q^0 \PD{x^0}{f} + \sum_k Q^k \PD{x^k}{f} \\
\end{align*}

So, we can write

\begin{align*}
\grad 
&= \PD{x^0}{} + \sum_k \sigma_k \PD{x^k}{} \\
\end{align*}

We can do something similar for an Euclidean four vector space

\begin{align*}
x 
&= x^\mu e_\mu \\
&= \left( x^0 + x^k e_k e_0 \right) e_0 \\
&= \left( x^0 + x^k i_k \right) e_0 \\
&= q e_0 \\
\end{align*}

Here each of the bivectors $i_k$ have a negative square, much more quaternion-like (and could easily be defined in an
isomorphic fashion).  This time we have

\begin{align*}
\grad 
&= \PD{x^0}{} + \sum_k \inv{i_k} \PD{x^k}{} \\
\end{align*}

