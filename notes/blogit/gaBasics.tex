%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

\input{../latex/blogpost.tex}
\renewcommand{\basename}{gaBasics}
\renewcommand{\dirname}{notes/FIXMEwheretodirname/}
%\newcommand{\dateintitle}{}
%\newcommand{\keywords}{}
%\blogpage{http://sites.google.com/site/peeterjoot/math2009/gaBasics.pdf}
%\date{Aug 20, 2009}
%\revisionInfo{gaBasics.tex}

\input{../latex/peeter_prologue_print2.tex}
%\usepackage{peeters_layout_exercise}
\usepackage{peeters_figures}

\beginArtNoToc

\generatetitle{Introduction to Geometric Algebra}
%\chapter{Introduction to Geometric Algebra}
\label{chap:gaBasics}

% concatonation and reorganization of what was originally
%
% ../gabook/introGa.tex
% ../gabook/gaWiki.tex
% ../gabook/gaGradeDotWedge.tex

%\newtheorem{theorem}{Theorem}[section]
%%\newtheorem{proposition}[theorem]{Proposition}
%%\newtheorem{corollary}[theorem]{Corollary}
%%\newtheorem{lemma}[theorem]{Lemma}
%%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
\newtheorem{axiom}[theorem]{Axiom}
%\newtheorem{exercise}[theorem]{Exercise}
%%\newtheorem{solution}[theorem]{Solution}
\section{Introducing Geometric Algebra}

\subsection{What can we do with Geometric Algebra?}

Geometric Algebra (GA) is a physics rebranding of what is known to mathematicians as Clifford Algebra.  The origin, rationale and pros and cons associated with this rebranding can be found discussed elsewhere.  Having learned the algebra first with the label Geometric Algebra I will continue to refer to it that way here.  Clifford Algebra will be reserved to describe more general algebras not required for the physical applications discussed here.

GA provides an algebraic system that has the scope to incorporate features of a number of algebraic and systems, including traditional cross and dot product based 3D vector algebra, complex numbers, quaternions, and the Pauli and Dirac matrix algebras.

There are extensive opportunities for the application of Geometric Algebra in applied mathematics and physics.  Examples include projection, linear system solution, rotations, and subspace intersection.  The GA toolbox provides a number of elegant coordinate, matrix and tensor free methods.  General spatial rotations will be seen to have an exponential form very much like the polar form of complex numbers.  Vector calculus operations, such as Green's theorem, the divergence theorem, and Stokes theorems (both the 3D and differential forms generalizations) have a natural GA representation without requiring reinterpretation of the object (such as an Electric field) as a differential element as in the exterior calculus of differential forms.

Physics applications of GA include means for expressing torque and angular momentum without a normal, allowing degeneralizations from 3D to pure planar contexts, as well as generalization to 4D (or higher) spaces that we need for relativistic applications.  We have a toolbox that allows many tensor quantities to be described in index free ways.  The student searching for order in the crazy mixed up set of dot and cross product relations of Maxwell's equations, may be surprised that these can be reduced to a single equation \(\grad F = J/\epsilon_0 c\).  We can write the Lorentz force and power equations in the index free covariant form \(dp/d\tau = F \cdot v/c\).  Like rotations we have a matrix free exponential operator form for the Lorentz boosts.  While similar consolidation in electrodynamic theory is possible using the methods of tensor and differential forms, the GA formalism provides a number of new techniques and can perhaps be considered closer to the vector methods that we are used so to.  Many more examples could be listed, but to avoid making filling this introductory description with too much that is scary sounding and discouraging, I will stop here.

\subsection{How to introduce the geometric product?}

The fundamental operation in Geometric Algebra is the Geometric product, a direct product of two or more vectors.  This product is generally not closed, since the result of a multiplication may lie in a different space than the original factors.

One of the most common ways that the geometric product is introduced uses the following definition

\begin{equation}\label{eqn:gaBasics:how:foo1}
\begin{aligned}
\Bx \By \equiv \Bx \cdot \By + \Bx \wedge \By
\end{aligned}
\end{equation}

On the right hand side are two products.  The first is a plain old dot product, producing a scalar (numeric) result, and the second is a wedge product, producing a something-else (bivector).

The dot product \(\Bx \cdot \By\) is familiar to anybody who has taken high school algebra or physics.  The second term \(\Bx \wedge \By\), the ``something-else'', is not a vector nor scalar.  One can in fact identify this quantity as a sort of planar vector, a quantity with a magnitude and orientation in space, but we will come back to this.

This second object, the bivector, and its associated wedge product presents a bit of a chicken and egg problem for instruction and learning.  Personally, as a student, I first saw the wedge product used in the calculus of differential forms.  That was not a subject that I found particularly accessible with an engineering undergrad education.  I would not expect most students with a desire to learn basic vector algebra to have any familiarity with differential forms or its completely antisymmetric product.

The chicken and egg instructional problem associated with using \eqnref{eqn:gaBasics:how:foo1} as a starting point can be overcome, but I will not try to do so here.  Instead, the reader is referred to the excellent Geometric Algebra for Computer Science text \citep{dorst2007gac}.  This is probably one of the clearest and most well written books that taking this route to introduce the algebra, and is highly recommended.

An alternative approach to starting with the dot plus wedge identity of \eqnref{eqn:gaBasics:how:foo1} is to flip it, using properties of the wedge and dot products, to implicitly define the properties of the geometric product

\begin{equation}\label{eqn:gaBasics:how:foo2}
\begin{aligned}
\Bx \cdot \By \equiv \inv{2}(\Bx \By + \By \Bx)
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:how:foo3}
\begin{aligned}
\Bx \wedge \By \equiv \inv{2}(\Bx \By - \By \Bx)
\end{aligned}
\end{equation}

Care is also required with the identities \eqnref{eqn:gaBasics:how:foo2}, and \eqnref{eqn:gaBasics:how:foo3} since they only apply to the product of two vectors.  While generalized dot and wedge products for products of multiple vectors can be defined, trying to learn from a text that presents things this way can be difficult.  One is left trying to swim through a maze of unfamiliar identities not really knowing which ones are the fundamentals.


What I will present here instead is an axiomatic like approach.  An attempt to present and motivate the axioms will be made.  How one can work from those axioms to determine the identities defining the many derived products such as the dot, wedge, generalized dot, generalized wedge, cross product, and commutator products will be discussed.  What does axiomatic like mean?  My goal is an illustration of how to develop some comfort with the tools and applications of Geometric Algebra.  For this initial learning exercise I think it is sufficient to know that a strict axiomatic extraction of properties and identities is possible, and to illustrate the path, but not actually follow it.

The focus of this introduction will be Euclidean \R{N} spaces, where the metric (dot product of all orthonormal unit vectors) is positive definite.  Generalizing the arguments to non positive definite metric spaces is not actually too much more difficult.  While this generalization is actually very important for relativistic treatments, I defer to the Cambridge text \citep{doran2003gap} to make it.  That text does an excellent job presenting an axiomatic approach applicable to general metrics.

\subsection{Modeling numbers as 1D vectors}

Let us start with some thought about possible rules that we could make for the multiplication (or division) of vectors?

Two such rules are assumed to be known, the dot product and the cross product.  Both of these logically map the vector product into a different space.  The dot product provides a mapping from vectors to scalars.  Considering all the pairs of vectors in a plane, the cross product provides a mapping into the from this planar subspace of \R{3} to the dual space for that plane, the set of vectors perpendicular to the plane.  Both of these product examples map pairs of vectors into a different space, so the reader should not necessarily be surprised when the choice for the geometric product ends up being one that also maps into a different space.

We desire the rules for vector multiplication to apply to any number of finite dimensions.  This is not a problem with the dot product which is easily defined for any number of dimensions.  The cross product however, is intrinsically three dimensional.  One cannot unambiguously define a normal to the plane in a four dimensional space to this product does not generalize to higher dimensions.  We can generalize the cross product of three vectors to \R{4} since we can then once again define a normal, but a requirement for an additional direction in the product is not satisfactory.  As well as having no higher dimensional generalization of the cross product, we cannot even degeneralized this product to \R{2}.  With intrinsically planar two dimensional quantities like torque and angular momentum, we ought to have a tool that can express these without resorting to a normal in a space that we may not even be interested in.

Since the desired product should work for \R{1}, as well as \R{2}, or \R{3} or \R{N}, lets first consider the simplest case, that of \R{1}.  If we can motivate a natural seeming multiplication for this space then that should be a good first step towards establishing a workable set of rules for the more general case.  This simple space is a fortunate choice since we have an isomorphic relationship between the real number line and a one dimensional vector space, and may be able to use our rules of multiplication for real numbers as guidance for vector multiplication.  To start down this path, it is therefore reasonable to review in detail the rules we have for real number multiplication.

\begin{enumerate}
\item Product of two positive numbers is positive.  Any consideration of countable sets of objects justifies this rule.

\item Product of a positive and negative number is negative.  Example: multiplying a debt (negative number) increases the amount of the debt.

\item Product of a negative and negative number is positive.

\item Multiplication is distributive.  Product of a sum is the sum of the products.

\begin{equation}\label{eqn:gaBasics:23}
\begin{aligned}
a (b + c) = a b + a c
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:43}
\begin{aligned}
(a + b) c = a c + b c
\end{aligned}
\end{equation}

\item Multiplication is associative.  Changing the order that multiplication is grouped by does not change the result.

\begin{equation}\label{eqn:gaBasics:63}
\begin{aligned}
(a b) c = a (b c)
\end{aligned}
\end{equation}

\item Multiplication is commutative.  Switching the order of multiplication does not change the result.

\begin{equation}\label{eqn:gaBasics:83}
\begin{aligned}
a b = b a
\end{aligned}
\end{equation}

\end{enumerate}

\imageFigure{../../figures/blogit/rotateMinusOneFig1}{Application of two negations as rotation or inversion in space}{fig:rotateMinusOne}{0.3}

My personal grade school exposure to the negative times negative rule was without any sort of justification.  Eventually I found one that I liked, since this can be considered as a special case of the previous rule, as depicted in \cref{fig:rotateMinusOne}.  Geometrically, a multiplication by -1 results in an inversion on the number line, or if one considers the number line to be a line in space, then this is a 180 degree rotation.  Two negative multiplications results in a 360 degree rotation, and thus takes the number back to its original positive or negative segment on its ``number line''.

\subsection{Rules for multiplication of vectors with the same direction}

Having identified the rules for multiplication of numbers, one can use these to define multiplication rules for a simple case, one dimensional vectors.  Conceptually a one dimensional vector space can be thought of like a number line, or the set of all numbers as the set of all scalar multiples of a unit vector of a particular direction in space.

It is reasonable to expect the rules for multiplication of two vectors with the same direction to have some of the same characteristics as multiplication of numbers.  Lets state this algebraically writing the directed distance from the origin to the points \(a\) and \(b\) in a vector notation
%FIXME: picture?

\begin{equation}\label{eqn:gaBasics:103}
\begin{aligned}
\Ba &= a\Be \\
\Bb &= b\Be \\
\end{aligned}
\end{equation}

where \(\Be\) is the unit vector alone the line in question.

The product of these two vectors is

\begin{equation}\label{eqn:gaBasics:123}
\begin{aligned}
\Ba \Bb = a b \Be \Be
\end{aligned}
\end{equation}

Although no specific meaning has yet been given to the \(\Be \Be\) term yet, one can make a few observations about a product of this form.

\begin{enumerate}
\item It is commutative, since \(\Ba \Bb = \Bb \Ba = a b \Be \Be\).
\item It is distributive since numeric multiplication is.
\item The product of three such vectors is distributive (no matter the grouping of the multiplications there will be a numeric factor and a \(\Be \Be \Be\) factor.
\end{enumerate}

These properties are consistent with half the properties of numeric multiplication.  If the other half of the numeric multiplication rules are assumed to also apply we have

\begin{enumerate}
\item Product of two vectors in the same direction is positive (rules 1 and 3 above).
\item Product of two vectors pointing in opposite directions is negative (rule 2 above).
\end{enumerate}

This can only be possible by giving the following meaning to the square of a unit vector

\begin{equation}\label{eqn:gaBasics:143}
\begin{aligned}
\Be \Be = 1
\end{aligned}
\end{equation}

Alternately, one can state that the square of a vector is that vectors squared length.

\begin{equation}\label{eqn:gaBasics:163}
\begin{aligned}
\Ba \Ba = a^2
\end{aligned}
\end{equation}

This property, called the contraction property, is in fact one of the defining axioms of Geometric Algebra.
Augmenting this with the associative and distributive properties we actually have all the defining properties of the geometric product.

What we will see shortly is that in order to retain this squared vector length property for non-colinear
vectors we will be forced to drop the commutative property of numeric multiplication:

\begin{equation}\label{eqn:gaBasics:183}
\begin{aligned}
\Ba \Bb \neq \Bb \Ba
\end{aligned}
\end{equation}

This is a choice that will later be observed to have important consequences.  There are many types of multiplications that do not have the commutative property.  Matrix multiplication is not even necessarily defined when the order is switched.  Other multiplication operations (wedge and cross products) change sign when the order is switched.

Another important choice that will be made in defining the Geometric Algebra axioms is not imposing a requirement for the product of two vectors to also be a vector.  This also breaks from the number line analogy since the product of two numbers was still a number.  To help justify this one can consider a Cartesian pairing of points in a plane to be a product of non-colinear vectors.  Such a pairing has more degrees of freedom than a line, so it is perhaps natural that we have to allow for a product of vectors in different directions to be a ``something-else'' to be determined.
\subsection{The Axioms}

\begin{quotation}
``for a sufficiently talented mathematician could then deduce all the consequences of these two principles.  However, since you are not assumed to be sufficiently talented yet, we shall discuss the consequences in more detail''.  R. Feynman
\end{quotation}

All of the axioms of Geometric Algebra have now been mentioned.  Some attempt to justify the least intuitive of these, the contraction property has been made, and some discussion for reasons for not including other properties (i.e. commutation) also discussed.  Whether this lead up discussion was successful or not, only the reader can say.

To make things more concrete and move on to the details, here is a statement of the complete set of axioms for the geometric product

\begin{axiom}\label{axiom:linearity}
Linearity property.  The geometric product is linear with respect to vector multiplication.

\begin{equation}\label{eqn:gaBasics:203}
\begin{aligned}
\Ba (\Bx + \By) = \Ba \Bx + \Ba\By
\end{aligned}
\end{equation}
\end{axiom}

\begin{axiom}\label{axiom:associative}
The geometric product is associative with respect to multiplication.  When three or more vectors are multiplied, the product is the same regardless of the order of multiplication.

\begin{equation}\label{eqn:gaBasics:223}
\begin{aligned}
\Bx (\By \Bz) = (\Bx \By) \Bz = \Bx \By \Bz
\end{aligned}
\end{equation}
\end{axiom}

\begin{axiom}\label{axiom:contraction}
Contraction property.  Square of a vector is a scalar.  For Euclidean spaces, this scalar is the square of the vector length.

\begin{equation}\label{eqn:gaBasics:243}
\begin{aligned}
\Bx^2 = \Norm{\Bx}^2
\end{aligned}
\end{equation}
\end{axiom}

In the study of general Clifford Algebraic spaces, the contraction property (\ref{axiom:contraction}) is specified as a quadratic form, but for our purposes, we can think of this as just the squared vector length.  In a relativistic context this is where the Minkowski metric comes into play, and the scalar magnitude of a four-vector square will depend on the split into time and spatial components.  For such a relativistic context we will require that the square of the time unit vector differs in sign from the square of the spatial unit vectors.

Perhaps surprisingly, this is all there is to the algebra.  The rest is all consequences and notation!

\begin{exercise}\label{ex:linearityAxiom}
Show that linearity with respect to more than one vector product is implied by the above, and as a consequence linearity with respect to scalar multiplication.
%With the contraction and associative properties (\ref{axiom:contraction}) we also have linearity with respect to multiplication by more than one vector, which in turn implies linearity with respect to scalar multiplication.
\end{exercise}

\subsection{Perpendicular products}

Two immediate consequences of the axioms follow by considering the triangle law construction depicted in \cref{fig:triangleLaw}.

\imageFigure{../../figures/blogit/triangleLawFig2}{Vector sum for Law of Cosines}{fig:triangleLaw}{0.3}

Using the contraction property we have for the squared difference of two vectors

\begin{equation}\label{eqn:gaBasics:perp:foo1}
\begin{aligned}
\Norm{\Bb - \Ba}^2 = (\Bb - \Ba)^2 = \Bb^2 -(\Bb \Ba + \Ba \Bb) + \Ba^2
\end{aligned}
\end{equation}

Observe that the left hand side is a scalar (by contraction), and the squared terms \(\Ba^2 = \Norm{\Ba}^2\), \(\Bb^2 = \Norm{\Bb}^2\) are also scalars.  So, while we cannot yet assign meaning to a general product of vectors, we can however conclude that a symmetric sum of vector products is also a scalar

\begin{equation}\label{eqn:gaBasics:perp:foo2}
\begin{aligned}
\Bb\Ba + \Ba\Bb = \text{scalar}
\end{aligned}
\end{equation}

By Pythagoras's theorem, we have for perpendicular Euclidean vectors

\begin{equation}\label{eqn:gaBasics:perp:foo3}
\begin{aligned}
\Norm{\Bb - \Ba}^2 = \Norm{\Bb}^2 + \Norm{\Ba}^2
\end{aligned}
\end{equation}

So we can for the specific case of perpendicular vectors observe that this symmetric product is in fact zero.  This gives us the second important relationship from the algebra, for vectors \(\Ba \perp \Bb\), we have from \eqnref{eqn:gaBasics:perp:foo2}, and \eqnref{eqn:gaBasics:perp:foo3} an antisymmetric relationship between products

\begin{equation}\label{eqn:gaBasics:introGaFirst:perpabcommutesign}
\Bb\Ba = -\Ba\Bb.
\end{equation}

Equation \eqnref{eqn:gaBasics:introGaFirst:perpabcommutesign} still does not assign any specific meaning to a general product of vectors.  We do however, know that whatever that meaning is, if those vectors are perpendicular, their product produces changes sign with commutation.

\subsection{The dot product}

The triangle law for vectors in \R{N} is typically written

\begin{equation}\label{eqn:gaBasics:dotprod:foo1}
\begin{aligned}
\Norm{\Bb - \Ba}^2
&= \Norm{\Bb}^2 -2 \Norm{\Ba}\Norm{\Bb}\cos\theta + \Norm{\Ba}^2 \\
&= \Norm{\Bb}^2 -2 \Ba \cdot \Bb + \Norm{\Ba}^2
\end{aligned}
\end{equation}

A comparison with \eqnref{eqn:gaBasics:perp:foo1} shows that we can make the identification of the symmetric vector product with the dot product from traditional vector algebra.  That is

\begin{equation}\label{eqn:gaBasics:introGaFirst:dotprod}
\Ba \cdot \Bb \equiv \inv{2}(\Ba \Bb + \Bb \Ba)
\end{equation}

This equivalence can also be demonstrated directly by coordinate expansion.  There is a great deal of emphasis in the GA literature on coordinate free capability.  We all come schooled in linear algebra using coordinate based calculation, so lets relate this funny looking symmetric vector product to the dot product as we know it.  First write our pair of vectors in terms of assumed orthonormal \R{N} Euclidean basis vectors \(\Be_i\)

\begin{equation}\label{eqn:gaBasics:263}
\begin{aligned}
\Ba &= \sum_i{a_i \Be_i} \\
\Bb &= \sum_j{b_j \Be_j}
\end{aligned}
\end{equation}

Here a summation range of \(i,j \in [1,N]\) is implied.  Expanding out the first term in the symmetric product we have

\begin{equation}\label{eqn:gaBasics:283}
\begin{aligned}
\Ba\Bb
&= \sum_{ij}{a_i b_j \Be_i \Be_j} \\
&= \sum_{i = j} {a_i b_j \Be_i \Be_j}
 + \sum_{i \ne j} {a_i b_j \Be_i \Be_j} \\
&=
\sum_{i} a_i b_i \Be_i \Be_i
 + \sum_{i \ne j} {a_i b_j \Be_i \Be_j} \\
\end{aligned}
\end{equation}

Similarly for the commuted product we have
\begin{equation}\label{eqn:gaBasics:303}
\begin{aligned}
\Bb\Ba
&=
\sum_{i} b_i a_i \Be_i \Be_i
 + \sum_{i \ne j} {b_i a_j \Be_i \Be_j} \\
\end{aligned}
\end{equation}

With the observation that our Euclidean basis unit vectors square to unity \(\Be_i^2 = 1\), our symmetric product is therefore

\begin{equation}\label{eqn:gaBasics:323}
\begin{aligned}
\inv{2}(\Ba\Bb + \Bb\Ba)
&=
\sum_{i} b_i a_i + \inv{2}\sum_{i \ne j} {a_i b_j \Be_i \Be_j} + {b_i a_j \Be_i \Be_j} \\
&= \sum_{i} a_i b_i
+ \inv{2}\sum_{i \ne j} a_i b_j \Be_i \Be_j
+ \inv{2}\sum_{i' \ne j'} b_{i'} a_{j'} \Be_{i'} \Be_{j'}
\\
\end{aligned}
\end{equation}

With a change of dummy summation indices we are almost there

\begin{equation}\label{eqn:gaBasics:343}
\begin{aligned}
\inv{2}(\Ba\Bb + \Bb\Ba)
&= \sum_{i} a_i b_i
+ \inv{2}\sum_{i \ne j} a_i b_j (\Be_i \Be_j + \Be_{j} \Be_{i})
\\
\end{aligned}
\end{equation}

Since each pair of different unit vectors are perpendicular, we have \(\Be_i \Be_j = -\Be_j \Be_i\), and this last term is obliterated, leaving just a scalar.  In that scalar we see explicitly that the familiar coordinate representation of the dot product is hiding there in the symmetric sum after all.

\begin{equation}\label{eqn:gaBasics:dotprod:inCoords}
\begin{aligned}
\inv{2}(\Ba\Bb + \Bb\Ba) &= \sum_{i} a_i b_i
\end{aligned}
\end{equation}

We have shown both with correspondence with the dot product from the law of cosines, and with algebraic means that this symmetric product is another representation of the real dot product of vectors.

\begin{exercise}\label{ex:euclideanDot}
If this is at all unclear, expand the symmetric sum explicitly for a pair of vectors in \R{2} or \R{3}.%  Solution (\ref{sol:eucldeanDot}).
\end{exercise}

\subsection{Some examples and terminology}

With the GA form of the dot product defined in terms of the symmetric sum, there is some logic to moving on directly to the antisymmetric sum \((\Ba \Bb - \Bb\Ba)/2\), which also has special significance.  However, we are also in need some terminology.  A pause from the abstract to consider some examples is the perfect opportunity to introduce this nomenclature.

With only the contraction property and knowledge of the anticommutation property of perpendicular vectors we have all the tools required to consider a wide range of vector products.

\begin{itemize}
\item Product of two perpendicular vectors.

FIXME: picture.

Considering just vectors in the \(x-y\) plane to start is enough to see the pattern.  Simplest is just the product of our basis unit vectors, optionally scaled, such as

\begin{equation}\label{eqn:gaBasics:363}
\begin{aligned}
\Be_2 (-3 \Be_1) &= 3 \Be_1\Be_2 \\
\end{aligned}
\end{equation}

We produces a quantity proportional to \(\Be_1 \Be_2\) that does not appear to be reducible any further.  How about one vectors offset from the axis by \(45\) degrees

\begin{equation}\label{eqn:gaBasics:383}
\begin{aligned}
(\Be_1 + \Be_2) (\Be_1 - \Be_2)
&= \mathLabelBox{\Be_1^2 - \Be_2^2}{\(1-1=0\)} + \Be_2 \Be_1 - \Be_1 \Be_2 \\
&= -2\Be_1\Be_2 \\
\end{aligned}
\end{equation}

Again we have a scalar multiple of this two vector product \(\Be_1 \Be_2\).  Finally, picking two perpendicular vectors for an arbitrary angle in the plane we have

\begin{equation}\label{eqn:gaBasics:403}
\begin{aligned}
(\Be_1 \cos\theta + \Be_2 \sin\theta)
(\Be_2 \cos\theta - \Be_1 \sin\theta)
&=
\cos\theta \sin\theta (-\Be_1^2 + \Be_2^2)
+
\Be_1 \Be_2 \cos^2\theta - \Be_2 \Be_1 \sin^2\theta \\
&= \Be_1 \Be_2
\end{aligned}
\end{equation}

Again the result is a scalar multiple of a two vector product (unit scaling in this case since we started with unit vectors).

% bivector not yet defined.
%Such a product generates just a bivector term.

\item Product of two non-colinear non-orthogonal vectors.

FIXME: picture.

By the contraction property the product of any two colinear vectors is a scalar.  We have also seen for products in the \(x-y\) plane that we get a two vector product, apparently unreducable, when multiplying perpendicular vectors.  How about the product of two arbitrarily oriented vectors in a plane?  Picking two random ones and multiplying them we have

\begin{equation}\label{eqn:gaBasics:423}
\begin{aligned}
(\Be_1 + 2\Be_2) (\Be_1 - \Be_2)
= \Be_1\Be_1 -2\Be_2\Be_2 + 2\Be_2\Be_1 - \Be_1\Be_2
= -1 - 3\Be_1\Be_2
\end{aligned}
\end{equation}

We have a curious product result with a scalar component plus a component containing a two vector product, still not yet named.

\item General product of two \R{2} vectors.

Utilizing a coordinate representation for two vectors confined to a plane with orthonormal basis vectors \(\Be_1\) and \(\Be_2\) we have for their product

\begin{equation}\label{eqn:gaBasics:443}
\begin{aligned}
(a_1 \Be_1 + a_2 \Be_2) (b_1 \Be_1 + b_2 \Be_2)
&=
a_1 b_1 \Be_1 \Be_1
+a_2 b_2 \Be_2 \Be_2
+a_1 b_2 \Be_1 \Be_2
+a_2 b_1 \Be_2 \Be_1 \\
&=
a_1 b_1 +a_2 b_2 +(a_1 b_2 -a_2 b_1) \Be_1 \Be_2
\end{aligned}
\end{equation}

Again we see the same pattern.  We have a scalar part, and a two vector product part.

\item Squaring the product of two perpendicular vectors
\begin{equation}\label{eqn:gaBasics:463}
\begin{aligned}
(\Be_1\Be_2)^2
=
(\Be_1\Be_2)(-\Be_2\Be_1)
=
-\Be_1(\Be_2\Be_2)\Be_1
=
-\Be_1\Be_1
=
-1
\end{aligned}
\end{equation}

It is not a coincidence that this behaves like the complex imaginary \(i\), with both squaring to minus one.  Within the framework of geometric algebra we will have a great deal of flexibility to introduce many different sorts of geometric ``imaginaries'' as convenient.  For vectors in a plane, the product of any two perpendiculars can be used as an imaginary, allowing for a complexification of the geometric quantities in the plane that can be added to vectors perpendicular to the plane.  More of this will be seen later.

\item Squaring the product of three perpendicular vectors

\begin{equation}\label{eqn:gaBasics:483}
\begin{aligned}
(\Be_1\Be_2\Be_3)^2
&=
\Be_1\Be_2\Be_3 \Be_1\Be_2\Be_3 \\
&=
-\Be_1\Be_2\Be_1 \Be_3\Be_2\Be_3 \\
&=
\Be_1\Be_1\Be_2 \Be_3\Be_2\Be_3 \\
&=
\Be_2 \Be_3\Be_2\Be_3 \\
&=
-1
\end{aligned}
\end{equation}

This is yet another ``imaginary'', but this time one that is spatial, not planar.

\begin{exercise}\label{ex:R3pseudoscalarCommute}
Show that the product \(\Be_1 \Be_2 \Be_3\) commutes with each of the vectors \(\Be_k\), and all of the two vector products \(\Be_j \Be_k\) (\(j \ne k\)).
\end{exercise}

\item Higher order products of three vectors in a plane.

Let us take a vector in a plane with the ``imaginary'' for that plane.

\begin{equation}\label{eqn:gaBasics:503}
\begin{aligned}
(x\Be_1 + y\Be_2) \Be_1\Be_2
=
x\Be_2 - y\Be_1
\end{aligned}
\end{equation}

This rotates the vector counterclockwise by 90 degrees, very much like the rotation that we have multiplying by the imaginary in complex arithmetic.  This is not an exact equivalence since for this vector product order matters.  Reversing it we have a rotation in the opposite sense

\begin{equation}\label{eqn:gaBasics:523}
\begin{aligned}
\Be_1\Be_2 (x\Be_1 + y\Be_2)
=
-x\Be_2 + y\Be_1
\end{aligned}
\end{equation}

\item Higher order non-planar products of three vectors.

When we have a product of three perpendicular vectors, we again have an irreducible result

\begin{equation}\label{eqn:gaBasics:543}
\begin{aligned}
\Be_1\Be_2 (\alpha \Be_3) &= \alpha \Be_1\Be_2 \Be_3
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:563}
\begin{aligned}
(\Be_1 + \Be_2)(\Be_2 -\Be_1) (3\Be_3)
&= 2 \Be_1 \Be_2 (3\Be_3)  \\
&= 6 \Be_1 \Be_2 \Be_3 \\
\end{aligned}
\end{equation}

If those vectors are not all perpendicular this product will be of a different sort

\begin{equation}\label{eqn:gaBasics:583}
\begin{aligned}
(\Be_1 + \Be_2)(\Be_2) (3\Be_3)
&= (2 \Be_1 \Be_2 + 1)(3\Be_3)  \\
&= 6 \Be_1 \Be_2 \Be_3 + 3 \Be_3 \\
\end{aligned}
\end{equation}

We now have vector and three vector products in the result.

\item General \R{3} geometric product of two vectors.

For two general vectors in \R{3} we have for the product after a bit of expansion and regrouping

\begin{equation}\label{eqn:gaBasics:603}
\begin{aligned}
\Ba \Bb
&=
(a_1\Be_1
+a_2\Be_2
+a_3\Be_3)
(b_1\Be_1
+b_2\Be_2
+b_3\Be_3) \\
&=
\Ba \cdot \Bb
+\DETuvij{a}{b}{2}{3} \Be_2 \Be_3
+\DETuvij{a}{b}{1}{3} \Be_1 \Be_3
+\DETuvij{a}{b}{1}{2} \Be_1 \Be_2
\end{aligned}
\end{equation}

Utilizing a cross product like determinant mnemonic this is

\begin{equation}\label{eqn:gaBasics:623}
\begin{aligned}
\Ba \Bb
&=
\Ba \cdot \Bb +
\begin{vmatrix}
\Be_2\Be_3 & \Be_3\Be_1 & \Be_1\Be_2 \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
\end{vmatrix} \\
&=
\Ba \cdot \Bb +
\Be_1 \Be_2 \Be_3
\begin{vmatrix}
\Be_1 & \Be_2 & \Be_3 \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
\end{vmatrix} \\
\end{aligned}
\end{equation}

We see therefore that the geometric product can be considered a scaled sum of the dot and cross products.

\begin{equation}\label{eqn:gaBasics:r3Cross}
\begin{aligned}
\Ba \Bb
&=
\Ba \cdot \Bb + \Be_1 \Be_2 \Be_3 (\Ba \cross \Bb)
\end{aligned}
\end{equation}

While the geometric product is not limited to three dimensions, when this restriction is imposed we see that it incorporates both the dot product (symmetric sum) and the cross product (\(\Be_1 \Be_2 \Be_3\) times the antisymmetric sum).  We will give a name to the antisymmetric sum shortly, and see that working with that instead of the cross product allows for the generalization from \R{3} and degeneralization to \R{2}.

\begin{exercise}\label{ex:R3pseudoscalarBivectorProducts}
By considering each of the products \(\Be_1 \Be_2 \Be_3 (\Be_2 \Be_3)\), \(\Be_1 \Be_2 \Be_3 (\Be_3 \Be_1)\), \(\Be_1 \Be_2 \Be_3 (\Be_1 \Be_2)\), verify

\begin{equation}\label{eqn:gaBasics:643}
\begin{aligned}
\Be_1 \Be_2 \Be_3
\begin{vmatrix}
\Be_1 & \Be_2 & \Be_3 \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
\end{vmatrix}
&=
\begin{vmatrix}
\Be_2\Be_3 & \Be_3\Be_1 & \Be_1\Be_2 \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
\end{vmatrix} \\
\end{aligned}
\end{equation}
\end{exercise}

\end{itemize}

\subsection{Solutions to Exercises}

\begin{itemize}
\item \ref{ex:linearityAxiom}: Scalar and multivector Linearity.

Multiple vector linearly follows from the distribution axiom, and repeated application of the vector linearity property

\begin{equation}\label{eqn:gaBasics:663}
\begin{aligned}
\Ba \Bb (\Bc + \Bd)
&=
\Ba (\Bb (\Bc + \Bd)) \\
&=
\Ba (\Bb \Bc + \Bb \Bd) \\
&=
\Ba \Bb \Bc + \Ba \Bb \Bd \\
&=
(\Ba \Bb) \Bc + (\Ba \Bb) \Bd \\
\end{aligned}
\end{equation}

For \(\Ba = \Bb\) we then have the special case

\begin{equation}\label{eqn:gaBasics:683}
\begin{aligned}
\Ba \Ba (\Bc + \Bd)
&=
(\Ba \Ba) \Bc + (\Ba \Ba) \Bd \\
\end{aligned}
\end{equation}

or with \(\alpha = \Abs{\Ba}^2 = \Ba^2\) we have
\begin{equation}\label{eqn:gaBasics:703}
\begin{aligned}
\alpha (\Bc + \Bd)
&=
\alpha \Bc + \alpha \Bd \\
\end{aligned}
\end{equation}

\item \ref{ex:euclideanDot}.  \R{3} symmetric product expansion.%\label{sol:eucldeanDot}

Showing that the symmetric sum is equivalent to the dot product does not take any too-clever index games.  For \R{3} proceeding in the straight forward dumb way we have

\begin{equation}\label{eqn:gaBasics:723}
\begin{aligned}
\Ba \Bb
&= (a_1 \Be_1 + a_2 \Be_2 + a_3 \Be_3) (b_1 \Be_1 + b_2 \Be_2 + b_3 \Be_3) \\
&=
a_1 b_1 \Be_1 \Be_1
+ a_2 b_2 \Be_2 \Be_2
+ a_3 b_3 \Be_3 \Be_3
\\
&+ a_1 b_2 \Be_1 \Be_2
+ a_1 b_3 \Be_1 \Be_3
+ a_2 b_1 \Be_2 \Be_1
+ a_2 b_3 \Be_2 \Be_3
+ a_3 b_2 \Be_3 \Be_2
+ a_3 b_3 \Be_3 \Be_3 \\
&=
a_1 b_1 + a_2 b_2 + a_3 b_3
+ ( a_1 b_2 - a_2 b_1 ) \Be_1 \Be_2
+ ( a_2 b_3 - a_3 b_2 ) \Be_2 \Be_3
+ ( a_3 b_1 - a_1 b_3 ) \Be_3 \Be_1
\end{aligned}
\end{equation}

Similarly
\begin{equation}\label{eqn:gaBasics:743}
\begin{aligned}
\Bb \Ba
&=
a_1 b_1 + a_2 b_2 + a_3 b_3
- ( a_1 b_2 - a_2 b_1 ) \Be_1 \Be_2
- ( a_2 b_3 - a_3 b_2 ) \Be_2 \Be_3
- ( a_3 b_1 - a_1 b_3 ) \Be_3 \Be_1
\end{aligned}
\end{equation}

Adding and scaling by \(1/2\) the desired result is obtained

\begin{equation}\label{sol:dotprod:inCoords}
\begin{aligned}
\inv{2}(\Ba\Bb + \Bb\Ba) &= a_1 b_1 + a_2 b_2 + a_3 b_3
\end{aligned}
\end{equation}

\item \ref{ex:R3pseudoscalarCommute} Pseudoscalar commutation with vectors and bivectors.

TODO.
%Show that the product \(\Be_1 \Be_2 \Be_3\) commutes with each of the vectors \(\Be_k\), and all of the two vector products \(\Be_j \Be_k\) (\(j \ne k\)).

\item \ref{ex:R3pseudoscalarBivectorProducts}.  Pseudoscalar bivector products in \R{3}

With pairwise reordering (changing sign) or cancellation of paired unit vectors we have

\begin{equation}\label{eqn:gaBasics:763}
\begin{aligned}
\Be_1 \Be_2 \Be_3 (\Be_3 \Be_2)
&=
\Be_1 \Be_2 (\Be_3 \Be_3) \Be_2 \\
&=
\Be_1 \Be_2 \Be_2 \\
&=
\Be_1 \\
\end{aligned}
\end{equation}

and

\begin{equation}\label{eqn:gaBasics:783}
\begin{aligned}
\Be_1 \Be_2 \Be_3 (\Be_1 \Be_3)
&=
-\Be_1 \Be_2 \Be_3 (\Be_3 \Be_1) \\
&=
-\Be_1 \Be_2 \Be_1 \\
&=
\Be_1 \Be_1 \Be_2 \\
&=
\Be_2 \\
\end{aligned}
\end{equation}

and

\begin{equation}\label{eqn:gaBasics:803}
\begin{aligned}
\Be_1 \Be_2 \Be_3 (\Be_2 \Be_1)
&=
-\Be_1 \Be_2 \Be_2 \Be_3 \Be_1 \\
&=
-\Be_1 \Be_3 \Be_1 \\
&=
\Be_1 \Be_1 \Be_3 \\
&=
\Be_3 \\
\end{aligned}
\end{equation}

\end{itemize}

\section{TO INTEGRATE}

\subsection{Coordinate expansion of the geometric product}

A powerful feature of geometric algebra is that it allows for coordinate free results, and the avoidance of basis selection that coordinates require.  While this is true, explicit coordinate expansion, especially initially while making the transition from coordinate based vector algebra, is believed to add clarity to the subject.


Despite not yet knowing what meaning to give to the geometric product of two general (non-colinear) vectors, given the axioms above and their consequences we actually have enough information to completely expand the geometric product of two vectors in terms of these coordinates:

\begin{equation}\label{eqn:gaBasics:823}
\begin{aligned}
\Ba\Bb
&= \sum_{ij}{a_i b_j \Be_i \Be_j} \\
&= \sum_{i = j} {a_i b_j \Be_i \Be_j}
 + \sum_{i \ne j} {a_i b_j \Be_i \Be_j} \\
&= \sum_{i} {a_i b_i \Be_i \Be_i}
 + \sum_{i < j} a_i b_j \Be_i \Be_j
 + \sum_{j < i} a_i b_j \Be_i \Be_j \\
&= \sum_{i} {a_i b_i}
 + \sum_{i < j} a_i b_j \Be_i \Be_j + a_j b_i \Be_j \Be_i \\
&= \sum_{i} {a_i b_i}
 + \sum_{i < j} (a_i b_j - b_i a_j)\Be_i \Be_j \\
\end{aligned}
\end{equation}

This can be summarized nicely in terms of determinants:

\begin{equation}\label{eqn:gaBasics:introGaFirst:geocoord}
\Ba\Bb = \sum_{i} {a_i b_i} + \sum_{i < j} \DETuvij{a}{b}{i}{j} \Be_i \Be_j
\end{equation}

This shows, without requiring the ``triangle law'' expansion of
FIXME
%\eqnref{eqn:gaBasics:introGaFirst:standarddot}
, that the geometric product has a scalar component that we recognize as the Euclidean vector dot product.  It also shows that the remaining bit is a ``something else'' component.  This ``something else'' is called a bivector.  We do not yet know what this bivector is or what to do with it, but will come back to that.

Observe that an interchange of \(\Ba\) and \(\Bb\) leaves the scalar part of \eqnref{eqn:gaBasics:introGaFirst:geocoord} unaltered (ie: it is symmetric), whereas an interchange inverts the bivector (ie: it is the antisymmetric part).

\subsection{Antisymmetric part of the geometric product}

Having identified the symmetric sum of vector products with the dot product we can write the geometric product of two arbitrary vectors in terms of this and it is difference

\begin{equation}\label{eqn:gaBasics:843}
\begin{aligned}
\Ba \Bb
&= \inv{2}(\Ba \Bb + \Bb \Ba) + \inv{2}(\Ba \Bb - \Bb \Ba) \\
&= \Ba \cdot \Bb + f(\Ba, \Bb) \\
\end{aligned}
\end{equation}

Let us examine this second term, the bivector, a mapping of a pair of vectors into a different sort of object of yet unknown properties.

\begin{equation}\label{eqn:gaBasics:863}
\begin{aligned}
f(\Ba, k\Ba) = \inv{2}(\Ba k\Ba - k\Ba \Ba) = 0
\end{aligned}
\end{equation}

Property: Zero when the vectors are colinear.

\begin{equation}\label{eqn:gaBasics:883}
\begin{aligned}
f(\Ba, k\Ba + \Bb) = \inv{2}(\Ba (k\Ba + \Bb) - (k\Ba + m\Bb)\Ba) = f(\Ba, \Bb)
\end{aligned}
\end{equation}

Property: colinear contributions are rejected.

\begin{equation}\label{eqn:gaBasics:903}
\begin{aligned}
f(\alpha \Ba, \beta \Bb) = \inv{2}(\alpha \Ba \beta \Bb - \beta \Bb \alpha \Ba) = \alpha \beta f(\Ba, \Bb)
\end{aligned}
\end{equation}

Property: bilinearity.

\begin{equation}\label{eqn:gaBasics:923}
\begin{aligned}
f(\Bb, \Ba)
= \inv{2}(\Bb \Ba - \Ba\Bb)
= -\inv{2}(\Ba \Bb - \Bb\Ba)
= -f(\Ba, \Bb)
\end{aligned}
\end{equation}

Property: Interchange inverts.

Operationally, these are in fact the properties of what in the calculus of differential forms is called the wedge product (uncoincidentally, these are also all properties of the cross product as well.)

Because the properties are identical the notation from differential forms is stolen, and we write

\begin{equation}\label{eqn:gaBasics:introGaFirst:wedge}
\Ba \wedge \Bb = \inv{2}(\Ba \Bb - \Bb \Ba)
\end{equation}

And as mentioned, the object that this wedge product produces from two vectors is called a bivector.

Strictly speaking the wedge product of differential calculus is defined as an alternating, associative, multilinear form.  We have here bilinear, not multilinear and associativity is not meaningful until more than two factors are introduced, however when we get to the product of more than three vectors, we will find that the geometric vector product produces an entity with all of these properties.

Returning to the product of two vectors we can now write

\begin{equation}\label{eqn:gaBasics:introGaFirst:gaproddotwedge}
\Ba \Bb = \Ba \cdot \Bb + \Ba \wedge \Bb
\end{equation}

This is often used as the initial definition of the geometric product.

\subsection{Yes, but what is that wedge product thing}

Combination of the symmetric and antisymmetric decomposition in \eqnref{eqn:gaBasics:introGaFirst:gaproddotwedge} shows that the product of two vectors according to the axioms
has a scalar part and a bivector part.  What is this bivector part geometrically?

One can show that the equation of a plane can be written in terms of bivectors.  One can also show that
the area of the parallelogram spanned by two vectors can be expressed in terms of the ``magnitude'' of a bivector.  Both of these
show that a bivector characterizes a plane and can be thought of loosely as a ``plane vector''.

Neither the plane equation or the area result are hard to show, but we will get to those later.  A more direct way to get an intuitive feel for the geometric properties of the bivector can be obtained by first examining the square of a bivector.

By subtracting the projection of one vector \(\Ba\) from another \(\Bb\), one can form the rejection of \(\Ba\) from \(\Bb\):

\begin{equation}\label{eqn:gaBasics:943}
\begin{aligned}
\Bb' = \Bb - (\Bb \cdot \acap)\acap
\end{aligned}
\end{equation}

With respect to the dot product, this vector is orthogonal to \(\Ba\).  Since \(\Ba \wedge \acap = 0\), this allows us to write the wedge product of vectors \(\Ba\) and \(\Bb\) as the direct product of two orthogonal vectors

\begin{equation}\label{eqn:gaBasics:963}
\begin{aligned}
\Ba \wedge \Bb
&= \Ba \wedge (\Bb - (\Bb \cdot \acap)\acap)) \\
&= \Ba \wedge \Bb' \\
&= \Ba \Bb' \\
&= -\Bb' \Ba \\
\end{aligned}
\end{equation}

The square of the bivector can then be written

\begin{equation}\label{eqn:gaBasics:983}
\begin{aligned}
(\Ba \wedge \Bb)^2
&= (\Ba \Bb')(-\Bb'\Ba) \\
&= -\Ba^2 (\Bb')^2.
\end{aligned}
\end{equation}

Thus the square of a bivector is negative.  It is natural to define a bivector norm:

\begin{equation}\label{eqn:gaBasics:1003}
\begin{aligned}
\abs{\Ba \wedge \Bb} = \sqrt{-(\Ba \wedge \Bb)^2} = \sqrt{ (\Ba \wedge \Bb)(\Bb \wedge \Ba) }
\end{aligned}
\end{equation}

Dividing by this norm we have an entity that acts precisely like the imaginary number \(i\).

Looking back to \eqnref{eqn:gaBasics:introGaFirst:gaproddotwedge} one can now assign additional meaning to the two parts.  The first, the dot product, is a scalar (ie: a real number), and a second part, the wedge product, is a pure imaginary term.  Provided \(\Ba \wedge \Bb \ne 0\), we can write \(i = \frac{\Ba \wedge \Bb}{ \abs{\Ba \wedge \Bb} }\) and express the geometric product in complex number form:

\begin{equation}\label{eqn:gaBasics:1023}
\begin{aligned}
\Ba \Bb = \Ba \cdot \Bb + i \abs{\Ba \wedge \Bb}
\end{aligned}
\end{equation}

The complex number system is the algebra of the plane, and the geometric product of two vectors can be used to completely characterize the algebra of an arbitrarily oriented plane in a higher order vector space.

It actually will be very natural to define complex numbers in terms of the geometric product, and we will see later that the geometric product allows for the ad-hoc definition of ``complex number'' systems according to convenience in many ways.

We will also see that generalizations of complex numbers such as quaternion algebras also find their natural place as specific instances of geometric products.

Concepts familiar from complex numbers such as conjugation, inversion, exponentials as rotations, and even things like the residue theory of complex contour integration, will all have a natural geometric algebra analogue.

We will return to this, but first some more detailed initial examination of the wedge product properties is in order, as is a look at the product of greater than two vectors.



\subsection{Examples}


\subsection{3D vector Algebra equivalents}

\subsection{GA Identities}

FIXME: the stuff that needs to be revamped:
\subsection{Introductory concepts}
\subsubsection{Motivation}

As an exercise work out axiomatically some of the key vector identities of Geometric Algebra.

Want to at least derive the vector bivector dot product distribution identity

\begin{equation}\label{eqn:gaBasics:1043}
\begin{aligned}
a \cdot ( b \wedge c) = (a \cdot b) c - (a \cdot c) b
\end{aligned}
\end{equation}

At the same time attempt here to provide a naturally sequenced introduction to the algebra.

\subsubsection{The Axioms}

Two basic axioms are required, contraction and associative multiplication respectively

\begin{equation}\label{eqn:gaBasics:1063}
\begin{aligned}
a^2 &= \text{scalar} \\
a (b c) &= (a b) c = a b c
\end{aligned}
\end{equation}

Linearity and scalar multiplication should probably also be included for completeness, but even with those this is a surprisingly small set of rules.  The choice to impose these as the rules for vector multiplication will be seen to have a rich set of consequences once explored.  It will take a fair amount of work to extract all the consequences of this decision, and some of that will be done here.

\subsubsection{Contraction and the metric}

Defining \(a^2\) itself requires introduction of a metric, the specification of the multiplication rules for a particular basis for the vector space.  For Euclidean spaces, a requirement that

\begin{equation}\label{eqn:gaBasics:1083}
\begin{aligned}
a^2 = \Abs{a}^2
\end{aligned}
\end{equation}

is sufficient to implicitly define this metric.  However, for the Minkowski spaces of special relativity one wants the squares of time and spatial basis vectors to be opposing in sign.  Deferring the discussion of metric temporarily one can work with the axioms above to discover their implications, and in particular how these relate to the coordinate vector space constructions that are so familiar.

\subsubsection{Symmetric sum of vector products}

Squaring a vector sum provides the first interesting feature of the general vector product

\begin{equation}\label{sumSquared}
\begin{aligned}
(a + b)^2 %&= (a + b)(a + b) \\
&= a^2 + b^2 + a b + b a
\end{aligned}
\end{equation}

Observe that the LHS is a scalar by the contraction identity, and on the RHS we have scalars \(a^2\) and \(b^2\) by the same.  This implies that the symmetric sum of products

\begin{equation}\label{eqn:gaBasics:1103}
\begin{aligned}
a b + b a
\end{aligned}
\end{equation}

is also a scalar, independent of any choice of metric.  Symmetric sums of this form have a place in physics over the space of operators, often instantiated in matrix form.  There one writes this as the commutator and denotes it as

\begin{equation}\label{eqn:gaBasics:anticommutator}
\begin{aligned}
\symmetric{a}{b} \equiv a b + b a
\end{aligned}
\end{equation}

In an Euclidean space one can observe that equation \ref{sumSquared} has the same structure as the law of cosines so it should not be surprising that this symmetric sum is also related to the dot product.  For a Euclidean space where one the notion of perpendicularity can be expressed as

\begin{equation}\label{eqn:gaBasics:1123}
\begin{aligned}
\Abs{ a + b }^2 = \Abs{a}^2 + \Abs{b}^2
\end{aligned}
\end{equation}

we can then see that an implication of the vector product is the fact that perpendicular vectors have the property

\begin{equation}\label{eqn:gaBasics:1143}
\begin{aligned}
a b + ba = 0
\end{aligned}
\end{equation}

or

\begin{equation}\label{eqn:gaBasics:1163}
\begin{aligned}
b a = - a b
\end{aligned}
\end{equation}

This notion of perpendicularity will also be seen to make sense for non-Euclidean spaces.

Although it retracts from a purist Geometric Algebra approach where things can be done in a coordinate free fashion, the connection between the symmetric product and the standard vector dot product can be most easily shown by considering an expansion with respect to an orthonormal basis.

Lets write two vectors in an orthonormal basis as

\begin{equation}\label{eqn:gaBasics:1183}
\begin{aligned}
a &= \sum_\mu a^\mu e_\mu \\
b &= \sum_\mu b^\mu e_\mu
\end{aligned}
\end{equation}

Here the choice to utilize raised indices rather than lower for the coordinates is taken from physics where summation is typically implied when upper and lower indices are matched as above.

Forming the symmetric product we have

\begin{equation}\label{eqn:gaBasics:1203}
\begin{aligned}
a b + b a
&=
\sum_{\mu,\nu} a^\mu e_\mu b^\nu e_\nu + b^\mu e_\mu a^\nu e_\nu \\
&=
\sum_{\mu,\nu} a^\mu b^\nu \left( e_\mu e_\nu + e_\nu e_\mu \right) \\
&=
2 \sum_{\mu} a^\mu b^\mu {e_\mu}^2 + \sum_{\mu \ne \nu} a^\mu b^\nu \left( e_\mu e_\nu + e_\nu e_\mu \right) \\
\end{aligned}
\end{equation}

For an Euclidean space we have \({e_\mu}^2 = 1\), and \(e_\nu e_\mu = -e_\mu e_\nu\), so we are left with

\begin{equation}\label{eqn:gaBasics:1223}
\begin{aligned}
\sum_{\mu} a^\mu b^\mu = \inv{2} ( a b + b a)
\end{aligned}
\end{equation}

This shows that we can make an identification between the symmetric product, and the anticommutator of physics with the dot product, and then define

\begin{equation}\label{eqn:gaBasics:dotDefined}
\begin{aligned}
a \cdot b \equiv \inv{2} \symmetric{a}{b} = \inv{2} (a b + ba)
\end{aligned}
\end{equation}

\subsubsection{Antisymmetric product of two vectors (wedge product)}

Having identified or defined the symmetric product with the dot product we are now prepared to examine a general product of two vectors.  Employing a symmetric + antisymmetric decomposition we can write such a general product as

\begin{equation}\label{eqn:gaBasics:1243}
\begin{aligned}
a b = \mathLabelBox{\inv{2}(a b + b a)}{\(a \cdot b\)} + \mathLabelBox{ \inv{2} ( a b - b a ) }{\(a\) something \(b\)}
\end{aligned}
\end{equation}

What is this remaining vector operation between the two vectors

\begin{equation}\label{eqn:gaBasics:1263}
\begin{aligned}
a \something b = \inv{2} ( a b - b a )
\end{aligned}
\end{equation}

One can continue the comparison with the quantum mechanics, and like the anticommutator operator that expressed our symmetric sum in \eqnref{eqn:gaBasics:anticommutator} one can introduce a commutator operator

\begin{equation}\label{eqn:gaBasics:commutator}
\begin{aligned}
\antisymmetric{a}{b} \equiv a b - b a
\end{aligned}
\end{equation}

The commutator however, does not naturally extend to more than two vectors, so as with the scalar part of the vector product (the dot product part), it is desirable to make a different identification for this part of the vector product.

One observation that we can make is that this vector operation changes sign when the operations are reversed.  We have

\begin{equation}\label{eqn:gaBasics:1283}
\begin{aligned}
b \something a = \inv{2} ( b a - a b) = - a \something b
\end{aligned}
\end{equation}

Similarly, if \(a\) and \(b\) are colinear, say \(b = \alpha a\), this product is zero

\begin{equation}\label{eqn:gaBasics:1303}
\begin{aligned}
a \something (\alpha a)
&= \inv{2} ( a  (\alpha a) - (\alpha a) a ) \\
&= 0
\end{aligned}
\end{equation}

This complete antisymmetry, aside from a potential difference in sign, are precisely the properties of the wedge product used in the mathematics of differential forms.  In this differential geometry the wedge product of \(m\) one-forms (vectors in this context) can be defined as

\begin{equation}\label{eqn:gaBasics:wedge}
\begin{aligned}
a_1 \wedge a_2 \cdots \wedge a_m
&= \inv{m!} \sum a_{i_1} a_{i_2} \cdots a_{i_m} \sgn(\pi(i_1 i_2 \cdots i_m))
\end{aligned}
\end{equation}

Here \(\sgn(\pi(\cdots))\) is the sign of the permutation of the indices.  While we have not gotten yet to products of more than two vectors it is helpful to know that the wedge product will have a place in such a general product.   An equation like \eqnref{eqn:gaBasics:wedge} makes a lot more sense after writing it out in full for a few specific cases.  For two vectors \(a_1\) and \(a_2\) this is

\begin{equation}\label{eqn:gaBasics:wedgeTwo}
\begin{aligned}
a_1 \wedge a_2 = \inv{2}
\left( a_1 a_2 (1) + a_2 a_1 (-1) \right)
\end{aligned}
\end{equation}

and for three vectors this is

\begin{equation}\label{eqn:gaBasics:1323}
\begin{aligned}
a_1 \wedge a_2 \wedge a_3 = \inv{6}
(
&a_1 a_2 a_3 (1) + a_1 a_3 a_2 (-1) \\
+&a_2 a_1 a_3 (-1) + a_3 a_1 a_2 (1) \\
+&a_2 a_3 a_1 (1) + a_3 a_2 a_1 (-1) )
\end{aligned}
\end{equation}

We will see later that this completely antisymmetrized sum, the wedge product of differential forms will have an important place in this algebra, but like the dot product it is a specific construction of the more general vector product.  The choice to identify the antisymmetric sum with the wedge product is an action that amounts to a definition of the wedge product.  Explicitly, and complementing the dot product definition of \eqnref{eqn:gaBasics:dotDefined} for the dot product of two vectors, we say

\begin{equation}\label{eqn:gaBasics:wedgeDefined}
\begin{aligned}
a \wedge b \equiv \inv{2} \antisymmetric{a}{b} = \inv{2} ( a b - b a )
\end{aligned}
\end{equation}

Having made this definition, the symmetric and antisymmetric decomposition of two vectors leaves us with a peculiar looking hybrid construction:

\begin{equation}\label{eqn:gaBasics:dotPlusWedge}
\begin{aligned}
a b %&= \inv{2} (a b + b a) + \inv{2} ( a b - b a ) \\
&= a \cdot b + a \wedge b
\end{aligned}
\end{equation}

We had already seen that part of this vector product was not a vector, but was in fact a scalar.  We now see that the remainder is also not a vector but is instead something that resides in a different space.  In differential geometry this object is called a two form, or a simple element in \(\bigwedge^2\).  Various labels are available for this object are available in Geometric (or Clifford) algebra, one of which is a 2-blade.  2-vector or bivector is also used in some circumstances, but in dimensions greater than three there are reasons to reserve these labels for a slightly more general construction.

The definition of \eqnref{eqn:gaBasics:dotPlusWedge} is often used as the starting point in Geometric Algebra introductions.  While there is value to this approach I have personally found that the non-axiomatic approach becomes confusing if one attempts to sort out which of the many identities in the algebra are the fundamental ones.  That is why my preference is to treat this as a consequence rather than the starting point.

\subsubsection{Expansion of the wedge product of two vectors}

Many introductory geometric algebra treatments try very hard to avoid explicit coordinate treatment.  It is true that GA provides infrastructure for coordinate free treatment, however, this avoidance perhaps contributes to making the subject less accessible.  Since we are so used to coordinate geometry in vector and tensor algebra, let us take advantage of this comfort, and express the wedge product explicitly in coordinate form to help get some comfort for it.

Employing the definition of \eqnref{eqn:gaBasics:wedgeDefined}, and an orthonormal basis expansion in coordinates for two vectors \(a\), and \(b\), we have

\begin{equation}\label{eqn:gaBasics:1343}
\begin{aligned}
2 (a \wedge b)
&= ( a b - b a ) \\
&=
\sum_{\mu,\nu} a^\mu b^\nu e_\mu e_\nu
-\sum_{\alpha,\beta} a^\alpha b^\beta e_\alpha e_\beta \\
&=
\mathLabelBox{\sum_{\mu} a^\mu b^\mu - \sum_{\alpha} a^\alpha b^\alpha }{\(=0\)}
+ \sum_{\mu \ne \nu} a^\mu b^\nu e_\mu e_\nu
- \sum_{\alpha \ne \beta} a^\alpha b^\beta e_\alpha e_\beta \\
&=
\sum_{\mu < \nu} (a^\mu b^\nu e_\mu e_\nu + a^\nu b^\mu e_\nu e_\mu)
- \sum_{\alpha < \beta} (a^\alpha b^\beta e_\alpha e_\beta + a^\beta b^\alpha e_\beta e_\alpha )
\\
&=
2 \sum_{\mu < \nu} ( a^\mu b^\nu - a^\nu b^\mu ) e_\mu e_\nu
\end{aligned}
\end{equation}

So we have
\begin{equation}\label{eqn:gaBasics:1363}
\begin{aligned}
a \wedge b
&= \sum_{\mu < \nu} \uDETuvij{a}{b}{\mu}{\nu} e_\mu e_\nu
\end{aligned}
\end{equation}

The similarity to the \R{3} vector cross product is not accidental.  This similarity can be made explicit by observing the following

\begin{equation}\label{eqn:gaBasics:1383}
\begin{aligned}
e_1 e_2 &= e_1 e_2 (e_3 e_3) = (e_1 e_2 e_3) e_3 \\
e_2 e_3 &= e_2 e_3 (e_1 e_1) = (e_1 e_2 e_3) e_1 \\
e_1 e_3 &= e_1 e_3 (e_2 e_2) = -(e_1 e_2 e_3) e_2 \\
\end{aligned}
\end{equation}

This common factor, a product of three normal vectors, or grade three blade, is called the pseudoscalar for \R{3}.  We write \(i = e_1 e_2 e_3\), and can then express the \R{3} wedge product in terms of the cross product

\begin{equation}\label{eqn:gaBasics:1403}
\begin{aligned}
a \wedge b
&=
\uDETuvij{a}{b}{2}{3} e_2 e_3
+\uDETuvij{a}{b}{1}{3} e_1 e_3
+\uDETuvij{a}{b}{1}{2} e_1 e_2  \\
&=
(e_1 e_2 e_3) \left( \uDETuvij{a}{b}{2}{3} e_1
-\uDETuvij{a}{b}{1}{3} e_2
+\uDETuvij{a}{b}{1}{2} e_3 \right) \\
\end{aligned}
\end{equation}

This is

\begin{equation}\label{eqn:gaBasics:wedgeAsCross}
\begin{aligned}
a \wedge b &= i (a \cross b)
\end{aligned}
\end{equation}

With this identification we now also have a curious integrated relation where the dot and cross products are united into a single structure

\begin{equation}\label{eqn:gaBasics:scalarPlusIcross}
\begin{aligned}
a b = a \cdot b + i (a \cross b)
\end{aligned}
\end{equation}

\subsubsection{Vector product in exponential form}

One naturally expects there is an inherent connection between the dot and cross products, especially when expressed in terms of the angle between the vectors, as in

\begin{equation}\label{eqn:gaBasics:1423}
\begin{aligned}
a \cdot b &= \Abs{a}\Abs{b} \cos\theta_{a,b} \\
a \cross b &= \Abs{a}\Abs{b} \sin\theta_{a,b} \ncap_{a,b}
\end{aligned}
\end{equation}

However, without the structure of the geometric product the specifics of what connection is is not obvious.  In particular the use of \eqnref{eqn:gaBasics:scalarPlusIcross} and the angle relations, one can easily blunder upon the natural complex structure of the geometric product

\begin{equation}\label{eqn:gaBasics:1443}
\begin{aligned}
a b
&= a \cdot b + i (a \cross b) \\
&=
\Abs{a}\Abs{b} \left( \cos\theta_{a,b} + i\ncap_{a,b} \sin\theta_{a,b} \right) \\
\end{aligned}
\end{equation}

As we have seen pseudoscalar multiplication in \R{3} provides a mapping between a grade 2 blade and a vector, so this \(i\ncap\) product is a 2-blade.

In \R{3} we also have \(i \ncap = \ncap i\) (exercise for reader) and also \(i^2 = -1\) (again for the reader), so this 2-blade \(i\ncap\) has all the properties of the \(i\) of complex arithmetic.  We can, in fact, write

\begin{equation}\label{eqn:gaBasics:1463}
\begin{aligned}
a b
&= a \cdot b + i (a \cross b) \\
&=
\Abs{a}\Abs{b} \exp( i\ncap_{a,b} \theta_{a,b} )
\end{aligned}
\end{equation}

In particular, for unit vectors \(a\), \(b\) one is able to quaternion exponentials of this form to rotate from one vector to the other

\begin{equation}\label{eqn:gaBasics:1483}
\begin{aligned}
b &= a \exp( i\ncap_{a,b} \theta_{a,b} )
\end{aligned}
\end{equation}

This natural GA use of multivector exponentials to implement rotations is not restricted to \R{3} or even Euclidean space, and is one of the most powerful features of the algebra.

\subsubsection{Pseudoscalar}

In general the pseudoscalar for \R{N} is a product of \(N\) normal vectors and multiplication by such an object maps m-blades to (N-m) blades.

For \R{2} the unit pseudoscalar has a negative square

\begin{equation}\label{eqn:gaBasics:1503}
\begin{aligned}
(e_1 e_2) (e_1 e_2)
&=
- (e_2 e_1) (e_1 e_2) \\
&=
- e_2 (e_1 e_1) e_2 \\
&=
- e_2 e_2 \\
&=
-1
\end{aligned}
\end{equation}

and we have seen an example of such a planar pseudoscalar in the subspace of the span of two vectors above (where \(\ncap i\) was a pseudoscalar for that subspace).  In general the sign of the square of the pseudoscalar depends on both the dimension and the metric of the space, so the ``complex'' exponentials that rotate one vector into another may represent hyperbolic rotations.

For example we have for a four dimensional space the pseudoscalar square is

\begin{equation}\label{eqn:gaBasics:1523}
\begin{aligned}
i^2 &=
(e_0 e_1 e_2 e_3) (e_0 e_1 e_2 e_3) \\
&=
- e_0 e_0 e_1 e_2 e_3 e_1 e_2 e_3 \\
&=
- e_0 e_0 e_1 e_2 e_3 e_1 e_2 e_3 \\
&=
- e_0 e_0 e_1 e_1 e_2 e_3 e_2 e_3 \\
&=
e_0 e_0 e_1 e_1 e_2 e_2 e_3 e_3 \\
\end{aligned}
\end{equation}

For a Euclidean space where each of the \({e_k}^2 = 1\), we have \(i^2 = 1\), but for a Minkowski space where one would have for \(k\ne0\), \({e_0}^2 {e_k}^2 = -1\), we have \(i^2 = -1\)

Such a mixed signature metric will allow for implementation of Lorentz transformations as exponentials (hyperbolic) rotations in a fashion very much like the quaternionic spatial rotations for Euclidean spaces.

It is also worth pointing out that the pseudoscalar multiplication naturally provides a mapping operator into a dual space, as we have seen in the cross product example, mapping vectors to bivectors, or bivectors to vectors.  Pseudoscalar multiplication in fact provides an implementation of the Hodge duality operation of differential geometry.

In higher than three dimensions, such as four, this duality operation can in fact map 2-blades to orthogonal 2-blades (orthogonal in the sense of having no common factors).  Take for example the typical example of a non-simple element from differential geometry

\begin{equation}\label{eqn:gaBasics:1543}
\begin{aligned}
\omega = e_1 \wedge e_2 + e_3 \wedge e_4
\end{aligned}
\end{equation}

The two blades that compose this sum have no common factors and thus cannot be formed as the wedge product of two vectors.  These two blades are orthogonal in a sense that can be made more exact later.   As this time we just wish to make the observation that the pseudoscalar provides a natural duality operation between these two subspaces of \(\bigwedge^2\).  Take for example

\begin{equation}\label{eqn:gaBasics:1563}
\begin{aligned}
i e_1 \wedge e_2
&=
 e_1 e_2 e_3 e_4 e_1 e_2  \\
&=
- e_1 e_1 e_2 e_3 e_4 e_2  \\
&=
- e_1 e_1 e_2 e_2 e_3 e_4 \\
&\propto
e_3 e_4 \\
\end{aligned}
\end{equation}

\subsubsection{Higher order products}

FIXME.

\subsection{Comparison of many traditional vector and GA identities}

\subsubsection{Three dimensional vector relationships vs N dimensional equivalents}

Here are some comparisons between standard \({\mathbb R}^3\) vector relations and their corresponding wedge and geometric product equivalents.  All the wedge and geometric product equivalents here are good for more than three dimensions, and some also for two.  In two dimensions the cross product is undefined even if what it describes (like torque) is a perfectly well defined in a plane without introducing an arbitrary normal vector outside of the space.

Many of these relationships only require the introduction of the wedge product to generalize, but since that may not be familiar to somebody with only a traditional background in vector algebra and calculus, some examples are given.

\subsubsection{wedge and cross products are antisymmetric}
\begin{equation}\label{eqn:gaBasics:1583}
\begin{aligned}
\Bv \times \Bu = - (\Bu \times \Bv)
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:1603}
\begin{aligned}
\Bv \wedge \Bu = - (\Bu \wedge \Bv)
\end{aligned}
\end{equation}

\subsubsection{wedge and cross products are zero when identical}
\begin{equation}\label{eqn:gaBasics:1623}
\begin{aligned}
\Bu \times \Bu = 0
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:1643}
\begin{aligned}
\Bu \wedge \Bu = 0
\end{aligned}
\end{equation}

\subsubsection{wedge and cross products are linear}

These are both linear in the first variable
\begin{equation}\label{eqn:gaBasics:1663}
\begin{aligned}
(\Bv + \Bw) \times \Bw = \Bu \times \Bw + \Bv \times \Bw
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:1683}
\begin{aligned}
(\Bv + \Bw) \wedge \Bw = \Bu \wedge \Bw + \Bv \wedge \Bw
\end{aligned}
\end{equation}

and are linear in the second variable
\begin{equation}\label{eqn:gaBasics:1703}
\begin{aligned}
\Bu \times (\Bv + \Bw)= \Bu \times \Bv + \Bu \times \Bw
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:1723}
\begin{aligned}
\Bu \wedge (\Bv + \Bw)= \Bu \wedge \Bv + \Bu \wedge \Bw
\end{aligned}
\end{equation}

\subsubsection{In general, cross product is not associative, but the wedge product is}

\begin{equation}\label{eqn:gaBasics:1743}
\begin{aligned}
(\Bu \times \Bv) \times \Bw \neq \Bu \times (\Bv \times \Bw)
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:1763}
\begin{aligned}
(\Bu \wedge \Bv) \wedge \Bw = \Bu \wedge (\Bv \wedge \Bw)
\end{aligned}
\end{equation}

\subsubsection{Wedge and cross product relationship to a plane}

\(\Bu \times \Bv\) is perpendicular to plane containing \(\Bu\) and \(\Bv\).
\(\Bu \wedge \Bv\) is an oriented representation of the plane containing \(\Bu\) and \(\Bv\).

\subsubsection{norm of a vector}

The norm (length) of a vector is defined in terms of the dot product

\begin{equation}\label{eqn:gaBasics:1783}
\begin{aligned}
 {\Vert \Bu \Vert}^2 = \Bu \cdot \Bu
\end{aligned}
\end{equation}

Using the geometric product this is also true, but this can be also be expressed more compactly as

\begin{equation}\label{eqn:gaBasics:1803}
\begin{aligned}
{\Vert \Bu \Vert}^2 = {\Bu}^2
\end{aligned}
\end{equation}

This follows from the definition of the geometric product and the fact that a vector wedge product with itself is zero

\begin{equation}\label{eqn:gaBasics:1823}
\begin{aligned}
 \Bu \, \Bu = \Bu \cdot \Bu + \Bu \wedge \Bu = \Bu \cdot \Bu
\end{aligned}
\end{equation}

\subsubsection{Lagrange identity}

In three dimensions the product of two vector lengths can be expressed in terms of the dot and cross products

\begin{equation}\label{eqn:gaBasics:1843}
\begin{aligned}
{\Vert \Bu  \Vert}^2 {\Vert \Bv  \Vert}^2
=
({\Bu  \cdot \Bv })^2 + {\Vert \Bu  \times \Bv  \Vert}^2
\end{aligned}
\end{equation}

The corresponding generalization expressed using the geometric product is

\begin{equation}\label{eqn:gaBasics:1863}
\begin{aligned}
{\Vert \Bu  \Vert}^2 {\Vert \Bv  \Vert}^2
= ({\Bu  \cdot \Bv })^2 - (\Bu  \wedge \Bv )^2
\end{aligned}
\end{equation}

This follows from by expanding the geometric product of a pair of vectors with its reverse

\begin{equation}\label{eqn:gaBasics:1883}
\begin{aligned}
(\Bu  \Bv )(\Bv  \Bu )
= ({\Bu  \cdot \Bv } + {\Bu  \wedge \Bv }) ({\Bu  \cdot \Bv } - {\Bu  \wedge \Bv })
\end{aligned}
\end{equation}

\subsubsection{determinant expansion of cross and wedge products}

\begin{equation}\label{eqn:gaBasics:1903}
\begin{aligned}
\Bu \times \Bv = \sum_{i<j}{ \begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}  {\Be}_i \times {\Be}_j }
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:1923}
\begin{aligned}
\Bu \wedge \Bv = \sum_{i<j}{ \begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}  {\Be}_i \wedge {\Be}_j }
\end{aligned}
\end{equation}

Without justification or historical context, traditional linear algebra texts will often define the determinant as the first step of an elaborate sequence of definitions and theorems leading up to the solution of linear systems, Cramer's rule and matrix inversion.

An alternative treatment is to axiomatically introduce the wedge product, and then demonstrate that this can be used directly to solve linear systems.  This is shown below, and does not require sophisticated math skills to understand.

It is then possible to define determinants as nothing more than the coefficients of the wedge product in terms of "unit k-vectors" (\({\Be}_i \wedge {\Be}_j\) terms) expansions as above.

A one by one determinant is the coefficient of \(\Be _1\) for an \(\mathbb R^1\) 1-vector.

A two-by-two determinant is the coefficient of \(\Be _1 \wedge \Be _2\) for an \(\mathbb R^2\) bivector

A three-by-three determinant is the coefficient of \(\Be _1 \wedge \Be _2 \wedge \Be _3\) for an \(\mathbb R^3\) trivector

When linear system solution is introduced via the wedge product, Cramer's rule follows as a side effect, and there is no need to lead up to the end results with definitions of minors, matrices, matrix inevitability, adjoints, cofactors, Laplace expansions, theorems on determinant multiplication and row column exchanges, and so forth.

\subsubsection{Equation of a plane}

For the plane of all points \({\Br}\) through the plane passing through three independent points \({\Br}_0\), \({\Br}_1\), and \({\Br}_2\), the normal form of the equation is

\begin{equation}\label{eqn:gaBasics:1943}
\begin{aligned}
(({\Br}_2 - {\Br}_0) \times ({\Br}_1 - {\Br}_0)) \cdot ({\Br} - {\Br}_0) = 0
\end{aligned}
\end{equation}

The equivalent wedge product equation is
\begin{equation}\label{eqn:gaBasics:1963}
\begin{aligned}
({\Br}_2 - {\Br}_0) \wedge ({\Br}_1 - {\Br}_0) \wedge ({\Br} - {\Br}_0) = 0
\end{aligned}
\end{equation}

\subsubsection{Projective and rejective components of a vector}

For three dimensions the projective and rejective components of a vector with respect to an arbitrary non-zero unit vector, can be expressed in terms of the dot and cross product

\begin{equation}\label{eqn:gaBasics:1983}
\begin{aligned}
\Bv = (\Bv \cdot \ucap)\ucap + \ucap \times (\Bv \times \ucap)
\end{aligned}
\end{equation}

For the general case the same result can be written in terms of the dot and wedge product and the geometric product of that and the unit vector

\begin{equation}\label{eqn:gaBasics:2003}
\begin{aligned}
\Bv = (\Bv \cdot \ucap)\ucap + (\Bv \wedge \ucap) \ucap
\end{aligned}
\end{equation}

It is also worthwhile to point out that this result can also be expressed using right or left vector division as defined by the geometric product

\begin{equation}\label{eqn:gaBasics:2023}
\begin{aligned}
\Bv = (\Bv \cdot \Bu)\frac{1}{\Bu} + (\Bv \wedge \Bu) \frac{1}{\Bu}
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:2043}
\begin{aligned}
\Bv = \frac{1}{\Bu}(\Bu \cdot \Bv) + \frac{1}{\Bu}(\Bu \wedge \Bv)
\end{aligned}
\end{equation}

\subsubsection{Area (squared) of a parallelogram is norm of cross product}

\begin{equation}\label{eqn:gaBasics:2063}
\begin{aligned}
A^2 = {\Vert \Bu \times \Bv \Vert}^2 = \sum_{i<j}{\begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}}^2
\end{aligned}
\end{equation}

and is the negated square of a wedge product
\begin{equation}\label{eqn:gaBasics:2083}
\begin{aligned}
A^2 = -(\Bu \wedge \Bv)^2 = \sum_{i<j}{\begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}}^2
\end{aligned}
\end{equation}

Note that this squared bivector is a geometric product.

\subsubsection{Angle between two vectors}

\begin{equation}\label{eqn:gaBasics:2103}
\begin{aligned}
({\sin \theta})^2 = \frac{{\Vert \Bu \times \Bv \Vert}^2}{{\Vert \Bu \Vert}^2 {\Vert \Bv \Vert}^2}
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:2123}
\begin{aligned}
({\sin \theta})^2 = -\frac{(\Bu \wedge \Bv)^2}{{ \Bu }^2 { \Bv }^2}
\end{aligned}
\end{equation}

\subsubsection{Volume of the parallelepiped formed by three vectors}

\begin{equation}\label{eqn:gaBasics:2143}
\begin{aligned}
V^2 = {\Vert (\Bu \times \Bv) \cdot \Bw \Vert}^2
= {
\begin{vmatrix}
u_1 & u_2 & u_3 \\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3 \\
\end{vmatrix}
}^2
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:gaBasics:2163}
\begin{aligned}
V^2 = -(\Bu \wedge \Bv \wedge \Bw)^2
= -\left(\sum_{i<j<k}
\begin{vmatrix}
u_i & u_j & u_k \\
v_i & v_j & v_k \\
w_i & w_j & w_k \\
\end{vmatrix}
\ecap_i \wedge \ecap_j \wedge \ecap_k
\right)^2
= \sum_{i<j<k}
{
\begin{vmatrix}
u_i & u_j & u_k \\
v_i & v_j & v_k \\
w_i & w_j & w_k \\
\end{vmatrix}
}^2
\end{aligned}
\end{equation}


\subsubsection{Some properties and examples}

Some fundamental geometric algebra manipulations will be provided below, showing how this vector product can be used in calculation of projections, area, and rotations.  How some of these tie together and correlate concepts from other branches of mathematics, such as complex numbers, will also be shown.

In some cases these examples provide details used above in the cross product and geometric product comparisons.

\subsubsection{Inversion of a vector}

One of the powerful properties of the Geometric product is that it provides the capability to express the inverse of a non-zero vector.  This is expressed by:

\begin{equation}\label{eqn:gaBasics:2183}
\begin{aligned}
{\Ba}^{-1} = \frac{\Ba}{\Ba \Ba} = \frac{\Ba}{{\Vert \Ba \Vert}^2}.
\end{aligned}
\end{equation}

\subsubsection{dot and wedge products defined in terms of the geometric product}

Given a definition of the geometric product in terms of the dot and wedge products, adding and subtracting \(\Ba  \Bb \) and \(\Bb  \Ba \) demonstrates that the dot and wedge product of two vectors can also be defined in terms of the geometric product

\subsubsection{The dot product}

\begin{equation}\label{eqn:gaBasics:2203}
\begin{aligned}
\Ba \cdot\Bb  = \frac{1}{2}(\Ba \Bb  + \Bb \Ba )
\end{aligned}
\end{equation}

This is the symmetric component of the geometric product.  When two vectors are colinear the geometric and dot products of those vectors are equal.

As a motivation for the dot product it is normal to show that this quantity occurs in the solution of the length of a general triangle where the third side is the vector sum of the first and second sides \(\Bc  = \Ba  + \Bb \).

\begin{equation}\label{eqn:gaBasics:2223}
\begin{aligned}
{\Vert \Bc  \Vert}^2 = \sum_{i}(a_i + b_i)^2 = {\Vert \Ba  \Vert}^2 + {\Vert \Bb  \Vert}^2 + 2 \sum_{i}a_i b_i
\end{aligned}
\end{equation}

The last sum is then given the name the dot product and other properties of this quantity are then shown (projection, angle between vectors, ...).

This can also be expressed using the geometric product

\begin{equation}\label{eqn:gaBasics:2243}
\begin{aligned}
\Bc ^2 = (\Ba  + \Bb )(\Ba  + \Bb ) = \Ba ^2 + \Bb ^2 + (\Ba \Bb  + \Bb \Ba )
\end{aligned}
\end{equation}

By comparison, the following equality exists

\begin{equation}\label{eqn:gaBasics:2263}
\begin{aligned}
\sum_{i}a_i b_i = \frac{1}{2}(\Ba \Bb  + \Bb \Ba )
\end{aligned}
\end{equation}

Without requiring expansion by components one can define the dot product exclusively in terms of the geometric product due to its properties of contraction, distribution and associativity.  This is arguably a more natural way to define the geometric product.  Addition of two similar terms is not immediately required, especially since one of those terms is the wedge product which may also be unfamiliar.

\subsubsection{The wedge product}

\begin{equation}\label{eqn:gaBasics:2283}
\begin{aligned}
\Ba \wedge\Bb  = \frac{1}{2}(\Ba \Bb  - \Bb \Ba )
\end{aligned}
\end{equation}

This is the antisymmetric component of the geometric product.  When two vectors are orthogonal the geometric and wedge products of those vectors are equal.

Switching the order of the vectors negates this antisymmetric geometric product component, and contraction property shows that this is zero if the vectors are equal.  These are the defining properties of the wedge product.

\subsubsection{Note on symmetric and antisymmetric dot and wedge product formulas}

A generalization of the dot product that allows computation of the component of a vector "in the direction" of a plane (bivector), or other k-vectors can be found below.  Since the signs change depending on the grades of the terms being multiplied, care is required with the formulas above to ensure that they are only used for a pair of vectors.

\subsubsection{Reversing multiplication order.  Dot and wedge products compared to the real and imaginary parts of a complex number}

Reversing the order of multiplication of two vectors, has the effect of the inverting the sign of just the wedge product term of the product.

It is not a coincidence that this is a similar operation to the conjugate operation of complex numbers.

The reverse of a product is written in the following fashion

\begin{equation}\label{eqn:gaBasics:2303}
\begin{aligned}
{\Bb  \Ba } = ({\Ba  \Bb })^\dagger
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:2323}
\begin{aligned}
{\Bc  \Bb  \Ba } = ({\Ba  \Bb  \Bc })^\dagger
\end{aligned}
\end{equation}

Expressed this way the dot and wedge products are

\begin{equation}\label{eqn:gaBasics:2343}
\begin{aligned}
\Ba \cdot\Bb  = \frac{1}{2}(\Ba \Bb  + ({\Ba  \Bb })^\dagger)
\end{aligned}
\end{equation}

This is the symmetric component of the geometric product.  When two vectors are colinear the geometric and dot products of those vectors are equal.

\begin{equation}\label{eqn:gaBasics:2363}
\begin{aligned}
\Ba \wedge\Bb  = \frac{1}{2}(\Ba \Bb  - ({\Ba  \Bb })^\dagger)
\end{aligned}
\end{equation}

These symmetric and antisymmetric pairs, the dot and wedge products extract the scalar and bivector components of a geometric product in the same fashion as the real and imaginary components of a complex number are also extracted by its symmetric and antisymmetric components

\begin{equation}\label{eqn:gaBasics:2383}
\begin{aligned}
\mathop{Re}(z) = \frac{1}{2}(z + \overbar{z})
\end{aligned}
\end{equation}
\begin{equation}\label{eqn:gaBasics:2403}
\begin{aligned}
\mathop{Im}(z) = \frac{1}{2}(z - \overbar{z})
\end{aligned}
\end{equation}

This extraction of components also applies to higher order geometric product terms.  For example

\begin{equation}\label{eqn:gaBasics:2423}
\begin{aligned}
\Ba \wedge\Bb \wedge \Bc
= \frac{1}{2}(\Ba \Bb \Bc  - ({\Ba  \Bb } \Bc )^\dagger)
= \frac{1}{2}(\Bb \Bc \Ba  - ({\Bb  \Bc } \Ba )^\dagger)
= \frac{1}{2}(\Bc \Ba \Bb  - ({\Bc  \Ba } \Bb )^\dagger)
\end{aligned}
\end{equation}

\subsubsection{Orthogonal decomposition of a vector}

Using the Gram-Schmidt process a single vector can be decomposed into two components with respect to a reference vector, namely the projection onto a unit vector in a reference direction, and the difference between the vector and that projection.

With, \( \ucap = \Bu / {\Vert \Bu \Vert}\), the projection of \(\Bv\) onto \( \ucap\) is

\begin{equation}\label{eqn:gaBasics:2443}
\begin{aligned}
 \mathrm{Proj}_{\ucap}\,\Bv  = \ucap (\ucap \cdot \Bv)
\end{aligned}
\end{equation}

Orthogonal to that vector is the difference, designated the rejection,

\begin{equation}\label{eqn:gaBasics:2463}
\begin{aligned}
 \Bv - \ucap (\ucap \cdot \Bv) = \frac{1}{{\Vert \Bu \Vert}^2} ( {\Vert \Bu \Vert}^2 \Bv - \Bu (\Bu \cdot \Bv))
\end{aligned}
\end{equation}

The rejection can be expressed as a single geometric algebraic product in a few different ways

\begin{equation}\label{eqn:gaBasics:2483}
\begin{aligned}
 \frac{ \Bu }{{\Bu}^2} ( \Bu \Bv - \Bu \cdot \Bv)
= \frac{1}{\Bu} ( \Bu \wedge \Bv )
= \ucap ( \ucap \wedge \Bv )
= ( \Bv \wedge \ucap ) \ucap
\end{aligned}
\end{equation}

The similarity in form between between the projection and the rejection is notable.  The sum of these recovers the original vector

\begin{equation}\label{eqn:gaBasics:2503}
\begin{aligned}%\label{eqn:gaBasics:gaWiki:orthoD}
 \Bv = \ucap (\ucap \cdot \Bv) + \ucap ( \ucap \wedge \Bv )
\end{aligned}
\end{equation}

Here the projection is in its customary vector form.  An alternate formulation is possible that puts the projection in a form that differs from the usual vector formulation

\begin{equation}\label{eqn:gaBasics:2523}
\begin{aligned}
 \Bv
= \frac{1}{\Bu} (\Bu \cdot \Bv) + \frac{1}{\Bu} ( \Bu \wedge \Bv )
= (\Bv \cdot \Bu) \frac{1}{\Bu}  + ( \Bv \wedge \Bu ) \frac{1}{\Bu}
\end{aligned}
\end{equation}

\subsubsection{A quicker way to the end result}

Working backwards from the end result, it can be observed that this orthogonal decomposition result can in fact follow more directly from the definition of the geometric product itself.

\begin{equation}\label{eqn:gaBasics:2543}
\begin{aligned}
\Bv = \ucap \ucap \Bv
= \ucap (\ucap \cdot \Bv + \ucap \wedge \Bv )
\end{aligned}
\end{equation}

With this approach, the original geometrical consideration is not necessarily obvious, but it is a much quicker way to get at the same algebraic result.

However, the hint that one can work backwards, coupled with the knowledge that the wedge product can be used to solve sets of linear equations, the problem of orthogonal decomposition can be posed directly,

FIXME: convert to reference.
This linear equation solution method requires only the wedge product, and an example of that without GA can be found in the \href{http://www.grassmannalgebra.info/grassmannalgebra/book/bookpdf/TheExteriorProduct.pdf}{Grassman Algebra book chapter on the Exterior Product.}

Let \(\Bv = a \Bu + \Bx\), where \(\Bu \cdot \Bx = 0\).  To discard the portions of \(\Bv\) that are colinear with \(\Bu\), take the wedge product

\begin{equation}\label{eqn:gaBasics:2563}
\begin{aligned}
\Bu \wedge \Bv = \Bu \wedge (a \Bu + \Bx) = \Bu \wedge \Bx
\end{aligned}
\end{equation}

Here the geometric product can be employed

\begin{equation}\label{eqn:gaBasics:2583}
\begin{aligned}
\Bu \wedge \Bv = \Bu \wedge \Bx = \Bu \Bx - \Bu \cdot \Bx = \Bu \Bx
\end{aligned}
\end{equation}

Because the geometric product is invertible, this can be solved for x

\begin{equation}\label{eqn:gaBasics:2603}
\begin{aligned}
\Bx = \frac{1}{\Bu}(\Bu \wedge \Bv)
\end{aligned}
\end{equation}

The same techniques can be applied to similar problems, such as calculation of the component of a vector in a plane and perpendicular to the plane.

\subsubsection{Area of parallelogram spanned by two vectors}

FIXME: parallelogramArea
%\begin{figure}[htp]
%\centering
%\includegraphics[totalheight=0.4\textheight]{parallelogramArea}
%\caption{parallelogramArea}\label{fig:parallelogramArea}
%\end{figure}

As depicted in \cref{fig:parallelogramArea}, one can see that the area of a parallelogram spanned by two vectors is computed from the base times height.  In the figure \(\Bu\) was picked as the base, with length \(\Norm{\Bu}\).  Designating the second vector \(\Bv\), we want the component of \(\Bv\) perpendicular to \(\ucap\) for the height.  An orthogonal decomposition of \(\Bv\) into directions parallel and perpendicular to \(\ucap\) can be performed in two ways.

\begin{equation}\label{eqn:gaBasics:2623}
\begin{aligned}
\Bv &= \Bv \ucap \ucap = (\Bv \cdot \ucap) \ucap + (\Bv \wedge \ucap) \ucap \\
    &= \ucap \ucap \Bv = \ucap (\ucap \cdot \Bv) + \ucap (\ucap \wedge \Bv)
\end{aligned}
\end{equation}

The height is the length of the perpendicular component expressed using the wedge as either \(\ucap (\ucap \wedge \Bv)\) or \((\Bv \wedge \ucap) \ucap\).

Multiplying base times height we have the parallelogram area

\begin{equation}\label{eqn:gaBasics:2643}
\begin{aligned}
A(\Bu,\Bv)
&= \Vert \Bu \Vert \Vert \ucap ( \ucap \wedge \Bv ) \Vert \\
&= \Vert \ucap ( \Bu \wedge \Bv ) \Vert \\
\end{aligned}
\end{equation}

Since the squared length of an Euclidean vector is the geometric square of that vector, we can compute the squared area of this parallelogram by squaring this single scaled vector

\begin{equation}\label{eqn:gaBasics:2663}
\begin{aligned}
A^2 &= (\ucap ( \Bu \wedge \Bv ) )^2
\end{aligned}
\end{equation}

Utilizing both encodings of the perpendicular to \(\ucap\) component of \(\Bv\) computed above we have for the squared area

\begin{equation}\label{eqn:gaBasics:2683}
\begin{aligned}
A^2
&= (\ucap( \Bu \wedge {\Bv} ) )^2 \\
&= (( \Bv \wedge {\Bu} ) \ucap) (\ucap ( {\Bu} \wedge \Bv )) \\
&= ( \Bv \wedge \Bu ) ( \Bu \wedge \Bv ) \\
\end{aligned}
\end{equation}

Since \(\Bu \wedge \Bv = -\Bv \wedge \Bu\), we have finally

\begin{equation}\label{eqn:gaBasics:2703}
\begin{aligned}
A^2 = -( \Bu \wedge \Bv )^2
\end{aligned}
\end{equation}

There are a few things of note here.  One is that the parallelogram area can easily be expressed in terms of the square of a bivector.  Another is that the square of a bivector has the same property as a purely imaginary number, a negative square.

It can also be noted that a vector lying completely within a plane anticommutes with the bivector for that plane.  More generally components of vectors that lie within a plane commute with the bivector for that plane while the perpendicular components of that vector commute.  These commutation or anticommutation properties depend both on the vector and the grade of the object that one attempts to commute it with (these properties lie behind the generalized definitions of the dot and wedge product to be seen later).

FIXME: relate to the cross product.  Also note the fact that this is all dimensional, and just just two or three (the last with the cross product).
\subsubsection{Expansion of a bivector and a vector rejection in terms of the standard basis}

If a vector is factored directly into projective and rejective terms using the geometric product \(\Bv = \frac{1}{\Bu}( \Bu \cdot \Bv + \Bu \wedge \Bv)\), then it is not necessarily obvious that the rejection term, a product of vector and bivector is even a vector.  Expansion of the vector bivector product in terms of the standard basis vectors has the following form

Let
\begin{equation}\label{eqn:gaBasics:2723}
\begin{aligned}
\Br
= \frac{1}{\Bu} ( \Bu \wedge \Bv )
= \frac{\Bu}{\Bu^2} ( \Bu \wedge \Bv )
= \frac{1}{{\Vert \Bu \Vert}^2} \Bu ( \Bu \wedge \Bv )
\end{aligned}
\end{equation}

It can be shown that
\begin{equation}\label{eqn:gaBasics:2743}
\begin{aligned}
\Br = \frac{1}{{\Vert{\Bu}\Vert}^2} \sum_{i<j}\begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}
\begin{vmatrix}u_i & u_j\\ {\Be}_i & {\Be}_j\end{vmatrix}
\end{aligned}
\end{equation}

(a result that can be shown more easily straight from \(\Br = \Bv - \ucap (\ucap \cdot \Bv)\)).

The rejective term is perpendicular to \(\Bu\), since
$\begin{vmatrix}
u_i & u_j\\ u_i & u_j
\end{vmatrix}
 = 0$
implies \(\Br \cdot \Bu = \Bzero\).

The magnitude of \(\Br\), is

\begin{equation}\label{eqn:gaBasics:2763}
\begin{aligned}
{\Vert \Br \Vert}^2 = \Br \cdot \Bv = \frac{1}{{\Vert{\Bu}\Vert}^2} \sum_{i<j}\begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}^2
\end{aligned}
\end{equation}.

So, the quantity

\begin{equation}\label{eqn:gaBasics:2783}
\begin{aligned}
{\Vert \Br \Vert}^2 {\Vert{\Bu}\Vert}^2 = \sum_{i<j}\begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}^2
\end{aligned}
\end{equation}

is the squared area of the parallelogram formed by \(\Bu\) and \(\Bv\).

It is also noteworthy that the bivector can be expressed as

\begin{equation}\label{eqn:gaBasics:2803}
\begin{aligned}
\Bu \wedge \Bv = \sum_{i<j}{ \begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}  {\Be}_i \wedge {\Be}_j }
\end{aligned}
\end{equation}.

Thus is it natural, if one considers each term \({\Be}_i \wedge {\Be}_j\) as a basis vector of the bivector space, to define the (squared) "length" of that bivector as the (squared) area.

Going back to the geometric product expression for the length of the rejection \(\frac{1}{\Bu} ( \Bu \wedge \Bv )\) we see that the length of the quotient, a vector, is in this case is the "length" of the bivector divided by the length of the divisor.

This may not be a general result for the length of the product of two \(k\)-vectors, however it is a result that may help build some intuition about the significance of the algebraic operations.  Namely,

When a vector is divided out of the plane (parallelogram span) formed from it and another vector, what remains is the perpendicular component of the remaining vector, and its length is the planar area divided by the length of the vector that was divided out.

\subsubsection{Projection and rejection of a vector onto and perpendicular to a plane}

Like vector projection and rejection, higher dimensional analogs of that calculation are also possible using the geometric product.

As an example, one can calculate the component of a vector perpendicular to a plane and the projection of that vector onto the plane.

Let \(\Bw = a \Bu + b \Bv + \Bx\), where \(\Bu \cdot \Bx = \Bv \cdot \Bx = 0\).  As above, to discard the portions of \(\Bw\) that are colinear with \(\Bu\) or \(\Bu\), take the wedge product

\begin{equation}\label{eqn:gaBasics:2823}
\begin{aligned}
\Bw \wedge \Bu \wedge \Bv = (a \Bu + b \Bv + \Bx) \wedge \Bu \wedge \Bv = \Bx \wedge \Bu \wedge \Bv
\end{aligned}
\end{equation}

Having done this calculation with a vector projection, one can guess that this quantity equals \(\Bx (\Bu \wedge \Bv)\).  One can also guess there is a vector and bivector dot product like quantity such that the allows the calculation of the component of a vector that is in the "direction of a plane".  Both of these guesses are correct, and the validating these facts is worthwhile.  However, skipping ahead slightly, this to be proved fact allows for a nice closed form solution of the vector component outside of the plane:

\begin{equation}\label{eqn:gaBasics:2843}
\begin{aligned}
\Bx
= (\Bw \wedge \Bu \wedge \Bv)\frac{1}{\Bu \wedge \Bv}
= \frac{1}{\Bu \wedge \Bv}(\Bu \wedge \Bv  \wedge \Bw)
\end{aligned}
\end{equation}

Notice the similarities between this planar rejection result a the vector rejection result.  To calculation the component of a vector outside of a plane we take the volume spanned by three vectors (trivector) and "divide out" the plane.

Independent of any use of the geometric product it can be shown that this rejection in terms of the standard basis is

\begin{equation}\label{eqn:gaBasics:2863}
\begin{aligned}
\Bx = \frac{1}{(A_{u,v})^2} \sum_{i<j<k}
\begin{vmatrix}w_i & w_j & w_k \\u_i & u_j & u_k \\v_i & v_j & v_k \\\end{vmatrix}
\begin{vmatrix}u_i & u_j & u_k \\v_i & v_j & v_k \\ {\Be}_i & {\Be}_j & {\Be}_k \\ \end{vmatrix}
\end{aligned}
\end{equation}

Where

\begin{equation}\label{eqn:gaBasics:2883}
\begin{aligned}
(A_{u,v})^2
= \sum_{i<j} \begin{vmatrix}u_i & u_j\\v_i & v_j\end{vmatrix}
= -(\Bu \wedge \Bv)^2
\end{aligned}
\end{equation}

is the squared area of the parallelogram formed by \(\Bu\), and \(\Bv\).

The (squared) magnitude of \(\Bx\) is

\begin{equation}\label{eqn:gaBasics:2903}
\begin{aligned}
{\Vert \Bx \Vert}^2 =
\Bx \cdot \Bw =
\frac{1}{(A_{u,v})^2} \sum_{i<j<k}
{\begin{vmatrix}w_i & w_j & w_k \\u_i & u_j & u_k \\v_i & v_j & v_k \\\end{vmatrix}}^2
\end{aligned}
\end{equation}

Thus, the (squared) volume of the parallelepiped (base area times perpendicular height) is

\begin{equation}\label{eqn:gaBasics:2923}
\begin{aligned}
\sum_{i<j<k}
{\begin{vmatrix}w_i & w_j & w_k \\u_i & u_j & u_k \\v_i & v_j & v_k \\\end{vmatrix}}^2
\end{aligned}
\end{equation}

Note the similarity in form to the w,u,v trivector itself

\begin{equation}\label{eqn:gaBasics:2943}
\begin{aligned}
\sum_{i<j<k}
{\begin{vmatrix}w_i & w_j & w_k \\u_i & u_j & u_k \\v_i & v_j & v_k \\\end{vmatrix}} {\Be}_i \wedge {\Be}_j \wedge {\Be}_k
\end{aligned}
\end{equation}

which, if you take the set of \({\Be}_i \wedge {\Be}_j \wedge {\Be}_k\) as a basis for the trivector space, suggests this is the natural way to define the length of a trivector.  Loosely speaking the length of a vector is a length, length of a bivector is area, and the length of a trivector is volume.

\subsubsection{Product of a vector and bivector.  Defining the "dot product" of a plane and a vector}

In order to justify the normal to a plane result above, a general examination of the product of a vector and bivector is required.  Namely,

\begin{equation}\label{eqn:gaBasics:2963}
\begin{aligned}
\Bw (\Bu \wedge \Bv)
= \sum_{i,j<k}w_i {\Be}_i {\begin{vmatrix}u_j & u_k \\v_j & v_k \\\end{vmatrix}} {\Be}_j \wedge {\Be}_k
\end{aligned}
\end{equation}

This has two parts, the vector part where \(i=j\) or \(i=k\), and the trivector parts where no indices equal.  After some index summation trickery, and grouping terms and so forth, this is


\begin{equation}\label{eqn:gaBasics:2983}
\begin{aligned}
\Bw (\Bu \wedge \Bv) =
\sum_{i<j}(w_i {\Be}_j
- w_j {\Be}_i )
{\begin{vmatrix}u_i & u_j \\v_i & v_j \\\end{vmatrix}}
+
\sum_{i<j<k}
{\begin{vmatrix}w_i & w_j & w_k \\ u_i & u_j & u_k \\v_i & v_j & v_k \\\end{vmatrix}}
{\Be}_i \wedge {\Be}_j \wedge {\Be}_k
\end{aligned}
\end{equation}

The trivector term is \(\Bw \wedge \Bu \wedge \Bv\).  Expansion of \((\Bu \wedge \Bv) \Bw\) yields the same trivector term.  This is the completely symmetric part, and the vector term is negated.
Like the geometric product of two vectors, this geometric product can be grouped into symmetric and antisymmetric parts, one of which is a pure k-vector.  In analogy the antisymmetric part of this product can be called a generalized dot product, and is roughly speaking the dot product of a "plane" (bivector), and a vector.

The properties of this generalized dot product remain to be explored, but first here is a summary of the notation

\begin{equation}\label{eqn:gaBasics:3003}
\begin{aligned}
\Bw (\Bu \wedge \Bv) = \Bw \cdot (\Bu \wedge \Bv) + \Bw \wedge \Bu \wedge \Bv
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:gaBasics:3023}
\begin{aligned}
(\Bu \wedge \Bv) \Bw = - \Bw \cdot (\Bu \wedge \Bv) + \Bw \wedge \Bu \wedge \Bv
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:gaBasics:3043}
\begin{aligned}
\Bw \wedge \Bu \wedge \Bv = \frac{1}{2}(\Bw (\Bu \wedge \Bv) + (\Bu \wedge \Bv) \Bw)
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:gaBasics:3063}
\begin{aligned}
\Bw \cdot (\Bu \wedge \Bv) = \frac{1}{2}(\Bw (\Bu \wedge \Bv) - (\Bu \wedge \Bv) \Bw)
\end{aligned}
\end{equation}

Let \(\Bw = \Bx + \By\), where \(\Bx = a \Bu + b \Bv\), and \(\By \cdot \Bu = \By \cdot \Bv = \Bzero\).  Expressing \(\Bw\) and the \(\Bu \wedge \Bv\), products in terms of these components is

\begin{equation}\label{eqn:gaBasics:3083}
\begin{aligned}
\Bw (\Bu \wedge \Bv) = \Bx (\Bu \wedge \Bv) + \By (\Bu \wedge \Bv)
=
\Bx \cdot (\Bu \wedge \Bv) + \By \cdot (\Bu \wedge \Bv) + \By \wedge \Bu \wedge \Bv
\end{aligned}
\end{equation}

With the conditions and definitions above, and some manipulation, it can be shown that the term \(\By \cdot (\Bu \wedge \Bv) = \Bzero\), which then justifies the previous solution of the normal to a plane problem.  Since the vector term of the vector bivector product the name dot product is zero when the vector is perpendicular to the plane (bivector), and this vector, bivector "dot product" selects only the components that are in the plane, so in analogy to the vector-vector dot product this name itself is justified by more than the fact this is the non-wedge product term of the geometric vector-bivector product.

\subsubsection{Complex numbers}
There is a one to one correspondence between the geometric product of two \(\mathbb{R}^2\) vectors and the field of complex numbers.

Writing, a vector in terms of its components, and left multiplying by the unit vector \({\Be}_1\) yields

\begin{equation}\label{eqn:gaBasics:3103}
\begin{aligned}
 Z = {\Be}_1 \BP = {\Be}_1 ( x {\Be}_1 + y {\Be}_2)
= x (1) + y ({\Be}_1 {\Be}_2)
= x (1) + y ({\Be}_1 \wedge {\Be}_2)
\end{aligned}
\end{equation}

The unit scalar and unit bivector pair \(1, {\Be}_1 \wedge {\Be}_2\) can be considered an alternate basis for a two dimensional vector space.  This alternate vector representation is closed with respect to the geometric product

\begin{equation}\label{eqn:gaBasics:3123}
\begin{aligned}
 Z_1 Z_2
&= {\Be}_1 ( x_1 {\Be}_1 + y_1 {\Be}_2) {\Be}_1 ( x_2 {\Be}_1 + y_2 {\Be}_2) \\
&= ( x_1 + y_1 {\Be}_1 {\Be}_2) ( x_2 + y_2 {\Be}_1 {\Be}_2) \\
&= x_1 x_2 + y_1 y_2 ({\Be}_1 {\Be}_2) {\Be}_1 {\Be}_2) \\
+ (x_1 y_2 + x_2 y_1) {\Be}_1 {\Be}_2 \\
\end{aligned}
\end{equation}

This closure can be observed after calculation of the square of the unit bivector above, a quantity

\begin{equation}\label{eqn:gaBasics:3143}
\begin{aligned}
({\Be}_1 \wedge {\Be}_2)^2 = {\Be}_1 {\Be}_2 {\Be}_1 {\Be}_2 = - {\Be}_1 {\Be}_1 {\Be}_2 {\Be}_2 = -1
\end{aligned}
\end{equation}

that has the characteristics of the complex number \(i^2 = -1\).

This fact allows the simplification of the product above to

\begin{equation}\label{eqn:gaBasics:3163}
\begin{aligned}
Z_1 Z_2
= (x_1 x_2 - y_1 y_2) + (x_1 y_2 + x_2 y_1) ({\Be}_1 \wedge {\Be}_2)
\end{aligned}
\end{equation}

Thus what is traditionally the defining, and arguably arbitrary seeming, rule of complex number multiplication, is found to follow naturally from the higher order structure of the geometric product, once that is applied to a two dimensional vector space.

It is also informative to examine how the length of a vector can be represented in terms of a complex number.  Taking the square of the length

\begin{equation}\label{eqn:gaBasics:3183}
\begin{aligned}
\BP \cdot \BP &= ( x {\Be}_1 + y {\Be}_2) \cdot ( x {\Be}_1 + y {\Be}_2) \\
&= ({\Be}_1 Z) {\Be}_1 Z \\
&= (( x  - y {\Be}_1 {\Be}_2) {\Be}_1) {\Be}_1 Z \\
&= ( x  - y ({\Be}_1 \wedge {\Be}_2)) Z \\
\end{aligned}
\end{equation}

This right multiplication of a vector with \({\Be}_1\), is named the conjugate

\begin{equation}\label{eqn:gaBasics:3203}
\begin{aligned}
\overline{Z} = x  - y ({\Be}_1 \wedge {\Be}_2)
\end{aligned}
\end{equation}

And with that definition, the length of the original vector can be expressed as

\begin{equation}\label{eqn:gaBasics:3223}
\begin{aligned}
\BP \cdot \BP = \overline{Z}Z
\end{aligned}
\end{equation}

This is also a natural definition of the length of a complex number, given the fact that the complex numbers can be considered an isomorphism with the two dimensional Euclidean vector space.

\subsubsection{Rotation in an arbitrarily oriented plane}

A point \(\BP\), of radius \(\Br\), located at an angle \(\theta\) from the vector \(\ucap\) in the direction from \(\Bu\) to \(\Bv\), can be expressed as

\begin{equation}\label{eqn:gaBasics:3243}
\begin{aligned}
\BP = r( \ucap \cos{\theta} +
\frac{\ucap (\ucap \wedge \Bv)}{\Vert \ucap (\ucap \wedge \Bv) \Vert}  \sin{\theta})
=
r \ucap
( \cos{\theta} +
\frac{(\Bu \wedge \Bv)}{\Vert \ucap (\Bu \wedge \Bv) \Vert} \sin{\theta})
\end{aligned}
\end{equation}

Writing \( {\BI}_{\Bu ,\Bv } = \frac{\Bu \wedge \Bv}{\Vert \ucap (\Bu \wedge \Bv) \Vert}\), the square of this bivector has the property \({\BI _{\Bu ,\Bv }}^2 = -1\) of the imaginary unit complex number.

This allows the point to be specified as a complex exponential

\begin{equation}\label{eqn:gaBasics:3263}
\begin{aligned}
= \ucap r ( \cos\theta + \BI _{\Bu ,\Bv } \sin\theta )
= \ucap r \exp( \BI _{\Bu ,\Bv } \theta )
\end{aligned}
\end{equation}

Complex numbers could be expressed in terms of the \(\mathbb R^2\)unit bivector \({\Be}_1 \wedge {\Be}_2\).  However this isomorphism really only requires a pair of linearly independent vectors in a plane (of arbitrary dimension).

\subsubsection{Quaternions}

Similar to complex numbers the geometric product of two \(\mathbb{R}^3\) vectors can be used to define quaternions.  Pre and Post multiplication with \({\Be}_1{\Be}_2{\Be}_3\) can be used to express a vector in terms of the quaternion unit numbers \(i, j, k\), as well as describe all the properties of those numbers.

\subsubsection{Cross product as outer product}

%The cross product of traditional vector algebra (on \(\mathbb{R}^3\)) find its place in geometric algebra \(\calG_3\)

Cross product can be written as a scaled outer product

\begin{equation}\label{eqn:gaBasics:3283}
\begin{aligned}
\Ba \times\Bb  = -i(\Ba \wedge\Bb )
\end{aligned}
\end{equation}

\begin{equation}\label{eqn:gaBasics:3303}
\begin{aligned}
i^2 &= ({\Be}_1{\Be}_2{\Be}_3)^2 \\
&= {\Be}_1{\Be}_2{\Be}_3{\Be}_1{\Be}_2{\Be}_3 \\
&= -{\Be}_1{\Be}_2{\Be}_1{\Be}_3{\Be}_2{\Be}_3 \\
&= {\Be}_1{\Be}_1{\Be}_2{\Be}_3{\Be}_2{\Be}_3 \\
&= -{\Be}_3{\Be}_2{\Be}_2{\Be}_3 \\
&= -1
\end{aligned}
\end{equation}

The equivalence of the \(\mathbb{R}^3\) cross product and the wedge product expression above can be confirmed by direct multiplication of \(-i = -{\Be}_1{\Be}_2{\Be}_3\) with a determinant expansion of the wedge product

\begin{equation}\label{eqn:gaBasics:3323}
\begin{aligned}
\Bu \wedge \Bv = \sum_{1<=i<j<=3}(u_i v_j - v_i u_j) {\Be}_i \wedge {\Be}_j
= \sum_{1<=i<j<=3}(u_i v_j - v_i u_j) {\Be}_i {\Be}_j
\end{aligned}
\end{equation}

\EndArticle
