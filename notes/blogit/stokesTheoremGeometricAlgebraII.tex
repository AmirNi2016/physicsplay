%
% Copyright © 2013 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
\input{../blogpost.tex}
\renewcommand{\basename}{stokesTheoremGeometricAlgebra}
\renewcommand{\dirname}{notes/gabook/}
\newcommand{\keywords}{Stokes theorem, Geometric algebra, Clifford algebra, gradient, divergence, wedge product}

\input{../peeter_prologue_print2.tex}

% ointctr...
\usepackage{txfonts}

\beginArtNoToc

\generatetitle{Stokes theorem in Geometric algebra}
%\chapter{Stokes theorem in Geometric algebra}
\label{chap:stokesTheoremGeometricAlgebra}

The generalization of Stokes theorem to higher dimesional spaces, expressed in the formalism of geometric algebra takes the form

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:120}
\int_V d^k \Bx \cdot (\boldpartial \wedge F) = \int_{\partial V} d^{k-1} \Bx \cdot F.
\end{equation}

Here $F \in \bigwedge^{s}, s < k$ is a blade, $d^k \Bx$ is a $m$ volume element on a manifold, $\boldpartial$ is the projection of the gradient onto the tangent space of the manifold, and $\partial V$ indicates integration over the boundary of $V$.

It takes some work to give all this more concrete meaning.  That will be attempted here.

\section{Basic notation}

A finite vector space, not neccessarily Euclidean, with basis $\{\Be_1, \Be_2, \cdots\}$ will be assumed to be the generator of the geometric algebra.  A dual or reciprocal basis $\{\Be^1, \Be^2, \cdots\}$ for this basis can be calculated, defined by the property

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:20}
\Be_i \cdot \Be^j = {\delta_i}^j.
\end{equation}

Implicit summation over repeated indexes will be employed unless otherwise noted.  For example, the components of a vector $\Bx$ with respect to the standard or reciprocal bases, are

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:40}
\Bx = \Be_i x^i = \Be^j x_j.
\end{equation}

The coordinates of the vector follow by taking dot products

\begin{subequations}
\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:60}
\Bx \cdot \Be^j = \lr{ \Be_i x^i } \cdot \Be^j = x^i {\delta_i}^j = x^j
\end{equation}
\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:80}
\Bx \cdot \Be_j = \lr{ \Be^i x_i } \cdot \Be_j = x_i {\delta^i}_j = x_j
\end{equation}
\end{subequations}

Similarly, a bivector $F$ in coordinate representation has the form

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1180}
F = \inv{2} \Be^i \wedge \Be^j F_{ij} = \inv{2} \Be_i \wedge \Be_j F^{ij},
\end{equation}

where
\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1200}
\begin{aligned}
F_{ij} &= \lr{ F \cdot \Be_j } \cdot \Be_i \\
F^{ij} &= \lr{ F \cdot \Be^j } \cdot \Be^i.
\end{aligned}
\end{equation}

With the directional derivative defined by

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1260}
\Ba \cdot \spacegrad f(\Bx) = \lim_{t \rightarrow 0} \frac{f(\Bx + t \Ba) - f(\Bx)}{t},
\end{equation}

the gradient $\spacegrad$, in terms of the standard and dual bases is

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:100}
\spacegrad \equiv \Be^i \PD{x^i}{}.
%\equiv \Be^i \partial_i,
\end{equation}

To select from a multivector $A$ the grade $k$ portion, say $A_k$ we write

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1220}
A_k = \gpgrade{A}{k}.
\end{equation}

The scalar portion of a multivector $A$ will be written as 

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1240}
\gpgrade{A}{0} \equiv \gpgradezero{A}.
\end{equation}

The grade selection operators can be used to define the outer and inner products.  For blades $U$, and $V$ of grade $r$ and $s$ respectively, these are

\begin{subequations}
\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:300}
\gpgrade{ U V }{\Abs{r + s}} \equiv U \wedge V
\end{dmath}
\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:780}
\gpgrade{ U V }{\Abs{r - s}} \equiv U \cdot V.
\end{dmath}
\end{subequations}

Written out explicitly for odd grade blades $A$ (vector, trivector, ...), and vector $\Ba$ the dot and wedge products are respectively

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:800}
\begin{aligned}
\Ba \wedge A &= \inv{2} (\Ba A - A \Ba) \\
\Ba \cdot A &= \inv{2} (\Ba A + A \Ba).
\end{aligned}
\end{equation}

Similarly for even grade blades these are

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:820}
\begin{aligned}
\Ba \wedge A &= \inv{2} (\Ba A + A \Ba) \\
\Ba \cdot A &= \inv{2} (\Ba A - A \Ba).
\end{aligned}
\end{equation}

It will be useful to employ the cyclic scalar reordering identity for the scalar selection operator

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:920}
\gpgradezero{\Ba \Bb \Bc} 
= \gpgradezero{\Bb \Bc \Ba}
= \gpgradezero{\Bc \Ba \Bb}.
\end{equation}

\section{Curvilinear coordinates}

A manifold will be loosely defined herein as a parameterized surface.  For example, a 2D manifold can be considered a surface in an $n$ dimensional vector space, parameterized by two variables

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1280}
\Bx = \Bx(a,b) = \Bx(u^1, u^2).
\end{equation}

Note that the indices here do not represent exponentiation.  We can construct a basis for the manifold as

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1300}
\Bx_i = \PD{u^i}{\Bx}.
\end{equation}

On the manifold we can calculate a reciprocal basis $\{\Bx^i\}$, defined by requiring, at each point on the surface

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1320}
\Bx^i \cdot \Bx_j = {\delta^i}_j.
\end{equation}

Associated implicitly with this basis is a curvilinear coordinate representation defined by the projection operation

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1340}
\Bx = x^i \Bx_i,
\end{equation}

(sums over mixed indexes are implied).   These coordinates can be calculated by taking dot products with the reciprocal frame vectors

\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:1360}
\Bx \cdot \Bx^i 
= x^j \Bx_j \cdot \Bx^i 
= x^j {\delta_j}^i 
= x^i.
\end{dmath}

When the complete vector space is spanned by $\{\Bx_i\}$ the gradient has the curvilinear representation

\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:1380}
\spacegrad = \Bx^i \PD{u^i}{}.
\end{dmath}

When this basis does not span the space, this sum is the projection of the gradiient onto the tangent space at the point of evaluation, and is called the vector derivative

\begin{equation}\label{eqn:stokesTheoremGeometricAlgebraII:1400}
\boldpartial = \Bx^i \PD{u^i}{}.
\end{equation}

See \citep{aMacdonaldVAGC} for a more complete discussion of the gradient and vector derivatives in curvilinear coordinates.

\section{Green's theorem}

\section{Curl theorem}

\section{Appendix}

We'd like to be able to distribute the inner product of two blades $A_s, B_r$ with grades $0 < r < s$

\begin{lemma}\label{thm:stokesTheoremGeometricAlgebraII:1420}
\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:1440}
A_s \cdot \lr{ b \wedge B_r } = \lr{ A_s \cdot b } \cdot B_r.
\end{dmath}
\end{lemma}

The proof is mechanical, but straightforward.  Start by expanding the wedge and dot products within a grade selection operator

\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:1460}
A_s \cdot \lr{ b \wedge B_r }
=
\gpgrade{A_s (b \wedge B_r)}{s - (r + 1)}
=
\inv{2} \gpgrade{A_s \lr{b B_r + (-1)^{r} B_r b} }{s - (r + 1)}
\end{dmath}

Solving for $B_r b$ in 

\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:1480}
2 b \cdot B_r = b B_r - (-1)^{r} B_r b,
\end{dmath}

we have

\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:1500}
A_s \cdot \lr{ b \wedge B_r }
=
\inv{2} \gpgrade{ A_s b B_r + A_s \lr{ b B_r - 2 b \cdot B_r } }{s - (r + 1)}
=
\gpgrade{ A_s b B_r }{s - (r + 1)}
- 
\cancel{\gpgrade{ A_s \lr{ b \cdot B_r } }{s - (r + 1)}}.
\end{dmath}

The last term above is zero since we are selecting the $s - r - 1$ grade element of a multivector with grades $s - r + 1$ and $s + r - 1$, which has no terms for $r > 0$.  Now we can expand the $A_s b$ multivector product, for

\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:1520}
A_s \cdot \lr{ b \wedge B_r }
=
\gpgrade{ \lr{ A_s \cdot b + A_s \wedge b} B_r }{s - (r + 1)}.
\end{dmath}

The latter multivector (with the wedge product factor) above has grades $s + 1 - r$ and $s + 1 + r$, so this selection operator finds nothing.  This leaves

\begin{dmath}\label{eqn:stokesTheoremGeometricAlgebraII:1540}
A_s \cdot \lr{ b \wedge B_r }
=
\gpgrade{ 
\lr{ A_s \cdot b } \cdot B_r
+ \lr{ A_s \cdot b } \wedge B_r 
}{s - (r + 1)}.
\end{dmath}

The first dot products term has grade $s - 1 - r$ and is selected, whereas the wedge term has grade $s - 1 + r \ne s - r - 1$ (for $r > 0$).  This completes the proof.

\EndArticle
