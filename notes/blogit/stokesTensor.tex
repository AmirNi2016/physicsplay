\input{../peeter_prologue_print.tex}
%\input{../peeter_prologue_widescreen.tex}

\chapter{Stokes Theorem in tensor form.}
\label{chap:stokesTensor}
%\useCCL
\blogpage{http://sites.google.com/site/peeterjoot/math2011/stokesTensor.pdf}
\date{Feb 20, 2011}
\revisionInfo{stokesTensor.tex}

\beginArtWithToc
%\beginArtNoToc

\section{Motivation.}

I've worked through Stokes theorem proofs a couple times now.  One of the first times, I was trying to formulate this in a Geometric Algebra context, and had to resort to a tensor decomposition, before ending back in the Geometric Algebra description.  Later I figured out how to do it entirely with a Geometric Algebra description.

It's my expectation that if one started with a tensor description, the proof entirely in tensor form would not be difficult.  This is what I'd like to try this time.  To start off, I'll temporarily use the Geometric Algebra curl expression so I know what my tensor equation starting point will be, but once that starting point is found, we can work entirely in coordinate representation.  For somebody who already knows that this is the starting point, all of this initial motivation can be skipped.

\section{Translating the exterior derivative to a coordinate representation.}

Our starting point is a curl, dotted with a volume element of the same grade, so that the result is a scalar

\begin{equation}\label{eqn:stokesTensor:n}
\int d^n x \cdot (\grad \wedge A).
\end{equation}

Here $A$ is a blade of grade $n-1$, and we wedge this with the gradient for the space

\begin{equation}\label{eqn:stokesTensor:n}
\grad \equiv e^i \partial_i = e_i \partial^i,
\end{equation}

where we with with a basis (not neccessarily orthonormal) $\{e_i\}$, and the reciprocal frame for that basis $\{e^i\}$ defined by the relation

\begin{equation}\label{eqn:stokesTensor:n}
e^i \cdot e_j = {\delta^i}_j.
\end{equation}

Our coordinates in these basis sets are

\begin{align}\label{eqn:stokesTensor:n}
x \cdot e^i &\equiv x^i \\
x \cdot e_i &\equiv x_i
\end{align}

so that

\begin{equation}\label{eqn:stokesTensor:n}
x = x^i e_i = x_i e^i.
\end{equation}

The operator coordinates of the gradient are defined in the usual fashion
\begin{align}\label{eqn:stokesTensor:n}
\partial_i &\equiv \PD{x^i}{} \\
\partial^i &\equiv \PD{x_i}{}
\end{align}

The volume element for the subspace that we are integrating over we will define in terms of an arbitrary parameterization

\begin{equation}\label{eqn:stokesTensor:n}
x = x(\alpha_1, \alpha_2, \cdots, \alpha_n)
\end{equation}

The subspace can be considered spanned by the differential elements in each of the respective curves where all but the $i$th parameter are held constant.

\begin{equation}\label{eqn:stokesTensor:n}
dx_{\alpha_i}
= d\alpha_i \PD{\alpha_i}{x}
= d\alpha_i \PD{\alpha_i}{x^j} e_j.
\end{equation}

We assume that the integral is being performed in a subspace for which none of these differential elements in that region are linearly dependent (i.e. our Jacobean determinant must be non-zero).

The magnitude of the wedge product of all such differential elements provides the volume of the parallelogram, or parallopiped (or higher dimensional analogue), and is

\begin{equation}\label{eqn:stokesTensor:n}
d^n x
=
d\alpha_1
d\alpha_2
\cdots
d\alpha_n
\PD{\alpha_n}{x} \wedge
\cdots
\wedge
\PD{\alpha_2}{x}
\wedge
\PD{\alpha_1}{x}.
\end{equation}

The volume element is a oriented quantity, and may be adjusted with an arbitrary sign (or equivalently an arbitrary permutation of the differential elements in the wedge product), and we'll see that it is convienient for the translation to tensor form, to express these in reversed order.

Let's write

\begin{equation}\label{eqn:stokesTensor:n}
d^n \alpha = d\alpha_1 d\alpha_2 \cdots d\alpha_n,
\end{equation}

so that our volume element in coordinate form is

\begin{equation}\label{eqn:stokesTensor:n}
d^n x = d^n \alpha
\PD{\alpha_1}{x^i}
\PD{\alpha_2}{x^j}
\cdots
\PD{\alpha_{n-1}}{x^k}
\PD{\alpha_n}{x^l}
( e_l \wedge e_k \wedge \cdots \wedge e_j \wedge e_i ).
\end{equation}

Our curl will also also be a grade $n$ blade.  We write for the $n-1$ grade blade

\begin{equation}\label{eqn:stokesTensor:n}
A = A_{b c \cdots d} (e^b \wedge e^c \wedge \cdots e^d),
\end{equation}

where $A_{b c \cdots d}$ is antisymmetric (i.e. $A = a_1 \wedge a_2 \wedge \cdots a_{n-1}$ for a some set of vectors $a_i, i \in 1 .. n-1$).

With our gradient in coordinate form
\begin{equation}\label{eqn:stokesTensor:n}
\grad = e^a \partial_a,
\end{equation}

the curl is then
\begin{equation}\label{eqn:stokesTensor:n}
\grad \wedge A = \partial_a A_{b c \cdots d} (e^a \wedge e^b \wedge e^c \wedge \cdots e^d).
\end{equation}

The differential form for our integral can now be computed by expanding out the dot product.  We want

\begin{equation}\label{eqn:stokesTensor:n}
( e_l \wedge e_k \wedge \cdots \wedge e_j \wedge e_i )
\cdot
(e^a \wedge e^b \wedge e^c \wedge \cdots e^d)
=
((((( e_l \wedge e_k \wedge \cdots \wedge e_j \wedge e_i ) \cdot e^a ) \cdot e^b ) \cdot e^c ) \cdot \cdots ) \cdot e^d.
\end{equation}

Evaluation of the interior dot products introduces the intrinsic antisymmetry required for Stokes theorem.  For example, with

\begin{align*}
( e_n \wedge e_{n-1} \wedge \cdots \wedge e_2 \wedge e_1 ) \cdot e^a
&=
( e_n \wedge e_{n-1} \wedge \cdots \wedge e_3 \wedge e_2 ) (e_1 \cdot e^a) \\
&-( e_n \wedge e_{n-1} \wedge \cdots \wedge e_3 \wedge e_1 ) (e_2 \cdot e^a) \\
&+( e_n \wedge e_{n-1} \wedge \cdots \wedge e_2 \wedge e_1 ) (e_3 \cdot e^a) \\
&\cdots \\
&(-1)^{n-1}
( e_{n-1} \wedge e_{n-2} \wedge \cdots \wedge e_2 \wedge e_1 ) (e_n \cdot e^a)
\end{align*}

Since $e_i \cdot e^a = {\delta_i}^a$ our end result is a completely antisymmetric set of permutations of all the deltas

\begin{equation}\label{eqn:stokesTensor:n}
( e_l \wedge e_k \wedge \cdots \wedge e_j \wedge e_i )
\cdot
(e^a \wedge e^b \wedge e^c \wedge \cdots e^d)
=
{\delta^{[a}}_i
{\delta^b}_j
\cdots
{\delta^{d]}}_l,
\end{equation}

and the curl integral takes it's coordinate form

\begin{equation}\label{eqn:stokesTensor:n}
\int d^n x \cdot ( \grad \wedge A ) =
\int
d^n \alpha
\PD{\alpha_1}{x^i}
\PD{\alpha_2}{x^j}
\cdots
\PD{\alpha_{n-1}}{x^k}
\PD{\alpha_n}{x^l}
\partial_a A_{b c \cdots d}
{\delta^{[a}}_i
{\delta^b}_j
\cdots
{\delta^{d]}}_l.
\end{equation}

One final contraction of the paired indexes gives us our Stokes integral in its coordinate representation

\begin{equation}\label{eqn:stokesTensor:n}
\int d^n x \cdot ( \grad \wedge A ) =
\int
d^n \alpha
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^b}
\cdots
\PD{\alpha_{n-1}}{x^c}
\PD{\alpha_n}{x^{d]}}
\partial_a A_{b c \cdots d}
\end{equation}

We now have a starting point that is free of any of the abstraction of Geometric Algebra or differential forms.  We can identify the products of partials here as components of a scalar hypervolume element (possibly signed depending on the orientation of the parameterization)

\begin{equation}\label{eqn:stokesTensor:n}
d\alpha_1
d\alpha_2
\cdots
d\alpha_n
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^b}
\cdots
\PD{\alpha_{n-1}}{x^c}
\PD{\alpha_n}{x^{d]}}
\end{equation}

This is also a specific computation recipe for these hypervolume components, something that may not be obvious when we allow for general metrics for the space.  We are also allowing for non-orthonormal coordinate representations, and arbitrary parameterizations of the subspace that we are integrating over (our integral need not have the same dimension as the underlying vector space).

Observe that when the number of parameters equals the dimension of the space, we can write out the antisymmetrical term utilizing the determinant of the Jacobian matrix

\begin{equation}\label{eqn:stokesTensor:n}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^b}
\cdots
\PD{\alpha_{n-1}}{x^c}
\PD{\alpha_n}{x^{d]}}
= \epsilon^{a b \cdots d} \Abs{ \frac{\partial(x^1, x^2, \cdots x^n)}{\partial(\alpha_1, \alpha_2, \cdots \alpha_n)} }
\end{equation}

\section{The Stokes work starts here.}

The task is to relate our integral to the boundary of this volume, coming up with an explicit recipe for the description of that bounding surface, and determining the exact form of the reduced rank integral.  This job is essentially to reduce the ranks of the tensors that are being contracted in our Stokes integral.  With the derivative applied to our rank $n-1$ antisymmetric tensor $A_{b c \cdots d}$, we can apply the chain rule and examine the permutations so that this can be rewritten as a contraction of $A$ itself with a set of rank $n-1$ surface area elements.

\begin{equation}\label{eqn:stokesTensor:n}
%\inv{n!}
\int
d^n \alpha
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^b}
\cdots
\PD{\alpha_{n-1}}{x^c}
\PD{\alpha_n}{x^{d]}}
\partial_a A_{b c \cdots d} = ?
\end{equation}

%Note that since we are working in tensor form completely and are summing over all set of indexes, it will be convient to include a factorial adjustment in the integral.
Now, while the setup here has been completely general, this task is motivated by study of special relativity, where there is a requirement to work in a four dimensional space.  Because of that explicit goal, I'm not going to attempt to formulate this in a completely abstract fashion.  That task is really one of introducing sufficiently general notation.  Instead, I'm going to proceed with a simpleton approach, and do this explicitly, and repeatedly for each of the rank 1, rank 2, and rank 3 tensor cases.  It will be clear how this all generalizes by doing so, should one wish to work in still higher dimensional spaces.

\subsection{The rank 1 tensor case.}

The equation we are working with for this vector case is

\begin{equation}\label{eqn:stokesTensor:n}
\int d^2 x \cdot (\grad \wedge A) =
\int
d{\alpha_1} d{\alpha_2}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
\partial_a A_{b}(\alpha_1, \alpha_2)
\end{equation}

Expanding out the antisymetrized partials we have

\begin{align*}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
&=
\PD{\alpha_1}{x^{a}}
\PD{\alpha_2}{x^{b}}
-
\PD{\alpha_1}{x^{b}}
\PD{\alpha_2}{x^{a}},
\end{align*}

with which we can reduce the integral to

\begin{align*}
\int d^2 x \cdot (\grad \wedge A)
&=
\int
\left( d{\alpha_1}
\PD{\alpha_1}{x^{a}}
\PD{x^a}{A_{b}} \right)
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\left( d{\alpha_2}
\PD{\alpha_2}{x^{a}}
\PD{x^a}{A_{b}} \right)
\PD{\alpha_1}{x^{b}} d{\alpha_1} \\
&=
\int
\left( d\alpha_1 \PD{\alpha_1}{A_b} \right)
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\left( d\alpha_2 \PD{\alpha_2}{A_b} \right)
\PD{\alpha_1}{x^{b}} d{\alpha_1} \\
\end{align*}

Without any real loss of generality we can designate the integration bounds over the unit parameterization space square $\alpha_i \in [0,1]$, allowing this integral to be expressed as

\begin{equation}\label{eqn:stokesTensor:n}
\int
d{\alpha_1} d{\alpha_2}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
\partial_a A_{b}(\alpha_1, \alpha_2)
=
\int
\left( A_b(1, \alpha_2) - A_b(0, \alpha_2) \right)
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\left( A_b(\alpha_1, 1) - A_b(\alpha_1, 0) \right)
\PD{\alpha_1}{x^{b}} d{\alpha_1}.
\end{equation}

It's also fairly common to see $\evalbar{A}{\partial \alpha_i}$ used to designate evaluation of this first integral on the boundary, and using this we write

\begin{equation}\label{eqn:stokesTensor:n}
\boxed{
\int
d{\alpha_1} d{\alpha_2}
\PD{\alpha_1}{x^{[a}}
\PD{\alpha_2}{x^{b]}}
\partial_a A_{b}(\alpha_1, \alpha_2)
=
\int
\evalbar{A_b}{\partial \alpha_1}
\PD{\alpha_2}{x^{b}} d{\alpha_2}
-
\evalbar{A_b}{\partial \alpha_2}
\PD{\alpha_1}{x^{b}} d{\alpha_1}.
}
\end{equation}

\subsubsection{Sanity check: \R{2} case in rectangular coordinates.}

For $x^1 = x, x^2 = y$, and $\alpha_1 = x, \alpha_2 = y$, we have for the LHS

\begin{align*}
\int_{x=x_0}^{x_1}
\int_{y=y_0}^{y_1}
dx dy
\left(
\PD{\alpha_1}{x^{1}}
\PD{\alpha_2}{x^{2}}
-\PD{\alpha_1}{x^{2}}
\PD{\alpha_2}{x^{1}}
\right)
\partial_1 A_{2}
+
\left(
\PD{\alpha_1}{x^{2}}
\PD{\alpha_2}{x^{1}}
-\PD{\alpha_1}{x^{1}}
\PD{\alpha_2}{x^{2}}
\right)
\partial_2 A_{1}
&=
\int_{x=x_0}^{x_1}
\int_{y=y_0}^{y_1}
dx dy
\left( \PD{x}{A_y} - \PD{y}{A_x} \right)
\end{align*}

Our RHS expands to

\begin{align*}
&\int_{y=y_0}^{y_1} dy
\left(
\left( A_1(x_1, y) - A_1(x_0, y) \right)
\PD{y}{x^{1}}
+
\left( A_2(x_1, y) - A_2(x_0, y) \right)
\PD{y}{x^{2}}
\right) \\
&\qquad-
\int_{x=x_0}^{x_1} dx
\left(
\left( A_1(x, y_1) - A_1(x, y_0) \right)
\PD{x}{x^{1}}
+
\left( A_2(x, y_1) - A_2(x, y_0) \right)
\PD{x}{x^{2}}
\right) \\
&=
\int_{y=y_0}^{y_1} dy
\left( A_y(x_1, y) - A_y(x_0, y) \right)
-
\int_{x=x_0}^{x_1} dx
\left( A_x(x, y_1) - A_x(x, y_0) \right)
\end{align*}

We have

\begin{equation}\label{eqn:stokesTensor:n}
\int_{x=x_0}^{x_1}
\int_{y=y_0}^{y_1}
dx dy
\left( \PD{x}{A_y} - \PD{y}{A_x} \right)
=
\int_{y=y_0}^{y_1} dy
\left( A_y(x_1, y) - A_y(x_0, y) \right)
-
\int_{x=x_0}^{x_1} dx
\left( A_x(x, y_1) - A_x(x, y_0) \right)
\end{equation}

The RHS is just a positively oriented line integral around the rectangle of integration

\begin{equation}\label{eqn:stokesTensor:n}
\int
A_x(x, y_0) \xcap \cdot ( \xcap dx )
+ A_y(x_1, y) \ycap \cdot ( \ycap dy )
+ A_x(x, y_1) \xcap \cdot ( -\xcap dx )
+ A_y(x_0, y) \ycap \cdot ( -\ycap dy )
= \oint \BA \cdot d\Br.
\end{equation}

This special case is also recognizable as Green's theorem, evident with the substuition $A_x = P$, $A_y = Q$, which gives us

\begin{equation}\label{eqn:stokesTensor:n}
\int_A dx dy \left( \PD{x}{Q} - \PD{y}{P} \right)
=
\oint_C P dx + Q dy.
\end{equation}

Strictly speaking, Green's theorem is more general, since it applies to integration regions more general than rectangles, but that generalization can be arrived at easily enough, once the region is broken down into adjoining elementary regions.

\subsubsection{Sanity check: \R{3} case in rectangular coordinates.}

\subsection{The rank 2 tensor case.}
\subsection{The rank 3 tensor case.}

\EndArticle
%\EndNoBibArticle
