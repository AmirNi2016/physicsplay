\input{../peeter_prologue_print.tex}
%\input{../peeter_prologue_widescreen.tex}

\chapter{Dirac Notation Ponderings.}
\label{chap:desaiDiracNotes}
%\useCCL
\blogpage{http://sites.google.com/site/peeterjoot/math2010/desaiDiracNotes.pdf}
\date{June 23, 2010}
\revisionInfo{desaiDiracNotes.tex}

%\beginArtWithToc
\beginArtNoToc

\section{Motivation.}

I've got the textbook \cite{desai2009quantum} now for the QM I course I'll be taking in the fall, and have started some light perusing.  Starting things off is the Dirac bra ket notation.  Some aspects of that notation, or the explanation in the text, are not quite obvious to me so here I try to make sense of things.

\section{Dirac Adjoint notes.}

There are a pair of relations given to define the Dirac Adjoint.  These are 1.26 and 1.27 respectively:

\begin{align*}
\left( A \ket{\alpha} \right)^\conj &= \bra{\alpha} A^\dagger \\
{\bra{\beta} A \ket{\alpha}}^\conj &= \bra{\alpha} A^\dagger \ket{\beta}
\end{align*}

Is there some redundancy to these definitions.  Namely is 1.27 a consequence of 1.26?

Since the ket was defined as the conjugate of the bra, we can probably rewrite 1.26 as

\begin{align*}
\bra{\alpha} A^\conj &= \bra{\alpha} A^\dagger 
\end{align*}

Left ``multiplication'', by the ket $\ket{\beta}$ gives

\begin{align*}
(\bra{\alpha} A^\conj) \ket{\beta} &= (\bra{\alpha} A^\dagger) \ket{\beta} \\
{\bra{\beta} (A \ket{\alpha})}^\conj &= (\bra{\alpha} A^\dagger) \ket{\beta} \\
\end{align*}

I've added and retained parenthesis to retain the operational direction.  Is that operational direction not important?  For example, given an operator like $p = -i \hbar \partial_x$, it makes a big difference whether the operator operates to the left or to the right.  In the text, this last relation is equation 1.27 once the parens are dropped, so it does appear that 1.27 is a consequence of 1.26.  This also then seems to imply that in a bra operator ket sandwich, the operator implicitly operates on the ket (to the right), while an adjoint operator implicitly operates on the bra (to the left).

Let's compare this to the simpler and more pedestrian notation found in an old fashioned book like Bohm's \cite{bohm1989qt}.  His expectation values explicitly use an integral definition, and his adjoint definition is very explicit about order of operations.  Namely

\begin{align}\label{eqn:desaiDiracNotes:1}
\int \phi^\conj (A \psi) 
&\equiv \int \psi (A^\dagger \phi^\conj) 
\end{align}

Starting with a concrete definition like this seems a bit easier.  Suppose we also define the bra ket sandwich based on the integral as follows

\begin{align*}
\bra{\phi} A \ket{\psi} 
&\equiv \bra{\phi} (A \ket{\psi}) \\
&\equiv \int \phi^\conj (A \psi) \\
\end{align*}

Now, we can rewrite \ref{eqn:desaiDiracNotes:1}, as 

\begin{align*}
\int \phi^\conj (A \psi)   &\equiv \int \psi (A^\dagger \phi^\conj) \\
&\implies \\
\bra{\phi} (A \ket{\psi})  &= \bra{\psi^\conj} ( A^\dagger \ket{\phi^\conj} ) \\
&\implies \\
\left(\bra{\phi} (A \ket{\psi}) \right)^\conj  &= ( \bra{\phi} A^\dagger ) \ket{\psi}
\end{align*}

When starting off with the integral we see the notational requirement for non-adjoint operators to operate implicitly to the right, and the adjoint operators to operate implicitly to the left.  With that notation requirement we can drop the parens and recover 1.27.

A couple clarification goals are now complete.  The first is seeing how equation 1.26 in the text implies 1.27.  We also have reconciled the Dirac notation with the familiar integral inner product notation, and seen two different ways that clarify the implicit operator directionality in the bra operator ket sandwiches.

\section{Chapter 1 problems.}

\subsection{P1.}

With
\begin{align*}
\ket{\alpha_1} &\equiv 
\begin{bmatrix}
1 \\
0
\end{bmatrix} \\
\ket{\alpha_2} &\equiv 
\begin{bmatrix}
0 \\
1
\end{bmatrix} \\
\bra{\alpha_1} &\equiv 
\begin{bmatrix}
1 & 0
\end{bmatrix} \\
\bra{\alpha_2} &\equiv 
\begin{bmatrix}
0 & 1
\end{bmatrix}
\end{align*}

\subsubsection{P1.i Orthonormal.}

Straight multiplication is sufficient to show this and we get

\begin{align*}
\braket{\alpha_1}{\alpha_1} &= 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
\begin{bmatrix}
1 \\
0
\end{bmatrix} =
\begin{bmatrix}
1
\end{bmatrix} \\
\braket{\alpha_2}{\alpha_2} &= 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
\begin{bmatrix}
0 \\
1
\end{bmatrix} =
\begin{bmatrix}
1
\end{bmatrix} \\
\braket{\alpha_1}{\alpha_2} &= 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
\begin{bmatrix}
0 \\
1
\end{bmatrix} =
\begin{bmatrix}
0
\end{bmatrix} \\
\braket{\alpha_2}{\alpha_1} &= 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
\begin{bmatrix}
1 \\
0
\end{bmatrix} =
\begin{bmatrix}
0
\end{bmatrix} \\
\end{align*}

\subsubsection{P1.ii Linear combinations for state vectors}

\begin{align*}
\begin{bmatrix}
a \\
b
\end{bmatrix} &= 
a \ket{\alpha_1}
+b \ket{\alpha_2}
\end{align*}

\subsubsection{P1.iii Outer products.}

We have

\begin{align*}
\ket{\alpha_1}\bra{\alpha_2} &=
\begin{bmatrix}
1 \\
0
\end{bmatrix} 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
=
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix} \\
\ket{\alpha_2}\bra{\alpha_1} &=
\begin{bmatrix}
0 \\
1
\end{bmatrix} 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
=
\begin{bmatrix}
0 & 0 \\
1 & 0
\end{bmatrix} \\
\ket{\alpha_1}\bra{\alpha_1} &=
\begin{bmatrix}
1 \\
0
\end{bmatrix} 
\begin{bmatrix}
1 & 0
\end{bmatrix} 
=
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix} \\
\ket{\alpha_2}\bra{\alpha_2} &=
\begin{bmatrix}
0 \\
1
\end{bmatrix} 
\begin{bmatrix}
0 & 1
\end{bmatrix} 
=
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} \\
\end{align*}

\subsubsection{P1.iv Completeness relation.}

From the above outer products, summation over just the diagonal terms we have

\begin{align*}
\ket{\alpha_1}\bra{\alpha_1} + \ket{\alpha_2}\bra{\alpha_2} &=
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix} +
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix} 
= I
\end{align*}

\subsubsection{P1.v Arbitrary matrix as sum of outer products.}

By inspection

\begin{align*}
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} 
&=
a \ket{\alpha_1}\bra{\alpha_1} 
+b \ket{\alpha_1}\bra{\alpha_2} 
+c \ket{\alpha_2}\bra{\alpha_1} 
+d \ket{\alpha_2}\bra{\alpha_2}
\end{align*}

\subsubsection{P1.vi Spin matrix}

Given

\begin{align*}
A \ket{\alpha_1} &= + \ket{\alpha_1} \\
A \ket{\alpha_2} &= - \ket{\alpha_1}
\end{align*}

Our matrix elements are
\begin{align*}
\bra{\alpha_1} A \ket{\alpha_1} &= 1 \\
\bra{\alpha_2} A \ket{\alpha_1} &= 0 \\
\bra{\alpha_1} A \ket{\alpha_2} &= 0 \\
\bra{\alpha_2} A \ket{\alpha_2} &= -1
\end{align*}

Thus the matrix representation of the operator $A$ with respect to basis $\{\alpha_1, \alpha_2\}$ is

\begin{align*}
\{A\} =
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix} 
\end{align*}

\subsection{P2. Derivative of inverse operator}

We take derivatives of the identity operator, giving

\begin{align*}
0 
&= \frac{dI}{d\lambda} \\
&= \frac{d (A A^{-1})}{d\lambda} \\
&= \frac{d A }{d\lambda} A^{-1} + A \frac{d A^{-1}}{d\lambda} \\
\end{align*}

left multiplication by $A^{-1}$ and rearranging we have

\begin{align*}
\frac{d A^{-1}}{d\lambda} 
&= -A^{-1} \frac{d A }{d\lambda} A^{-1} 
\end{align*}

as desired.

\subsection{P3.}
\subsection{P4.}
\subsection{P5.}

\EndArticle
