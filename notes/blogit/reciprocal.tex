%
% Copyright © 2016 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%{
\input{../blogpost.tex}
\renewcommand{\basename}{reciprocal}
%\renewcommand{\dirname}{notes/phy1520/}
\renewcommand{\dirname}{notes/ece1228-electromagnetic-theory/}
%\newcommand{\dateintitle}{}
%\newcommand{\keywords}{}

\input{../peeter_prologue_print2.tex}

\usepackage{peeters_layout_exercise}
\usepackage{peeters_braket}
\usepackage{peeters_figures}
\usepackage{siunitx}
%\usepackage{mhchem} % \ce{}
%\usepackage{macros_bm} % \bcM
%\usepackage{macros_qed} % \qedmarker
%\usepackage{txfonts} % \ointclockwise

\beginArtNoToc

\generatetitle{Reciprocal frame vectors}
%\chapter{reciprocal frame vectors}
%\label{chap:reciprocal}

\makedefinition{Reciprocal frame}{dfn:reciprocal:frame}{
Given a basis for a subspace \( \setlr{ \Bx_1, \Bx_2, \cdots \Bx_n } \), the reciprocal frame is defined as the set of vectors \( \setlr{ \Bx^1, \Bx^2, \cdots \Bx^n } \) satisfying

\begin{dmath*}
\Bx_i \cdot \Bx^j = {\delta_i}^j.
\end{dmath*}

The vector \( \Bx^j \) is not the jth power of \( \Bx \), but is a superscript index, the conventional way of denoting a reciprocal frame vector.
} % definition

The concept of a reciprocal frame generalizes the notion of normal to non-orthonormal bases.

\paragraph{Motivation:} Reciprocal frames are required to express the GA form of Stokes theorem and for more general GA integration theory, including integration with respect to both curvilinear coordinates and higher dimensions.  While the focus in these notes are two and three dimensional problems, often in Cartesian coordinates, it is desirable to formulate integration theory in a way that is compatible with the four dimensional non-Euclidean vector space that describes the intrinsic relativistic nature of electrodynamics.

One of the utilities of a reciprocal frame is that it allows for the computation of the coordinates of a vector with respect to a non-orthonormal frame.  Let a vector be given in terms of coordinates \( a^i \), where \( i \) is an index not a power

\begin{dmath}\label{eqn:reciprocal:20}
\Ba = \sum_i a^i \Bx_i.
\end{dmath}

Dotting with the reciprocal frame vectors then trivally extracts these coordinates

\begin{dmath}\label{eqn:reciprocal:40}
\Ba \cdot \Bx^i 
= \lr{\sum_j a^j \Bx_j} \cdot \Bx^i
= \sum_j a^j {\delta_j}^i
= a^i.
\end{dmath}

Similarly, dotting with the frame basis vectors provides the coordinates with respect to the reciprocal frame.  Let those coordinates be \( a_i \), so that

\begin{dmath}\label{eqn:reciprocal:60}
\Ba = \sum_i a_i \Bx^i.
\end{dmath}

Dotting with the basis vectors gives

\begin{dmath}\label{eqn:reciprocal:80}
\Ba \cdot \Bx^i 
= \lr{\sum_j a_j \Bx^j} \cdot \Bx_i
= \sum_j a_j {\delta^j}_i
= a_i.
\end{dmath}

An example of a 2D oblique Euclidean basis and a corresponding reciprocal basis is plotted in \cref{fig:obliqueReciprocal:obliqueReciprocalFig2}.

\imageFigure{../../figures/GAelectrodynamics/obliqueReciprocalFig2}{Oblique basis}{fig:obliqueReciprocal:obliqueReciprocalFig2}{0.3}

The question to answer is how to compute the reciprocal frame vectors.  This is not intrinsically a GA problem, and can be solved with standand linear algebra theory.  For example, given a 2D basis \( \setlr{ \Bx_1, \Bx_2 } \), the reciprocal basis can be assumed to have a coordinate representation in the original basis

\begin{dmath}\label{eqn:reciprocal:100}
\begin{aligned}
\Bx^1 &= a \Bx_1 + b \Bx_2 \\
\Bx^2 &= c \Bx_1 + d \Bx_2.
\end{aligned}
\end{dmath}

Imposing the constraints of \cref{dfn:reciprocal:frame} leads to a pair of 2x2 linear systems that are easily solved to find
\begin{dmath}\label{eqn:reciprocal:120}
\begin{aligned}
\Bx^1 &= \inv{ \Bx_1^2 \Bx_2^2 - \lr{ \Bx_1 \cdot \Bx_2}^2 } \lr{ \Bx_2^2 \Bx_1 - \lr{ \Bx_1 \cdot \Bx_2 } \Bx_2 } \\
\Bx^2 &= \inv{ \Bx_1^2 \Bx_2^2 - \lr{ \Bx_1 \cdot \Bx_2}^2 } \lr{ \Bx_1^2 \Bx_2 - \lr{ \Bx_1 \cdot \Bx_2 } \Bx_1 } \\
\end{aligned}
\end{dmath}

The reader may notice that for \R{3} the denominator is related to the norm of the cross product \( \Bx_1 \cross \Bx_2 \).  More generally, this can be expressed as the square of the bivector \( \Bx_1 \wedge \Bx_2 \)

\begin{dmath}\label{eqn:reciprocal:140}
-\lr{\Bx_1 \wedge \Bx_2 }^2
=
-\lr{\Bx_1 \wedge \Bx_2 } \cdot \lr{\Bx_1 \wedge \Bx_2 }
=
-\lr{ \lr{\Bx_1 \wedge \Bx_2 } \cdot \Bx_1 } \cdot \Bx_2
=
\Bx_1^2 \Bx_2^2 - \lr{\Bx_1 \cdot \Bx_2}^2.
\end{dmath}

Additionally, the numerators are each dot products of \( \Bx_1, \Bx_2 \) with that same bivector

\begin{dmath}\label{eqn:reciprocal:160}
\begin{aligned}
\Bx^1 &= \frac{\Bx_2 \cdot \lr{ \Bx_1 \wedge \Bx_2 } }{ \lr{ \Bx_1 \wedge \Bx_2}^2 } \\
\Bx^2 &= \frac{\Bx_1 \cdot \lr{ \Bx_2 \wedge \Bx_1 } }{ \lr{ \Bx_1 \wedge \Bx_2}^2 },
\end{aligned}
\end{dmath}

or
%\begin{dmath}\label{eqn:reciprocal:180}
\boxedEquation{eqn:reciprocal:180}{
\begin{aligned}
\Bx^1 &= \Bx_2 \cdot \inv{ \Bx_1 \wedge \Bx_2 } \\
\Bx^2 &= \Bx_1 \cdot \inv{ \Bx_2 \wedge \Bx_1 }.
\end{aligned}
}
%\end{dmath}

\makeproblem{Reciprocal frame for two dimensional subspace.}{problem:reciprocal:2dsubspaceRecip}{
Prove \cref{eqn:reciprocal:120}.
} % problem

\makeanswer{problem:reciprocal:2dsubspaceRecip}{

Assuming the representation of \cref{eqn:reciprocal:100}, the dot products are

\begin{dmath}\label{eqn:reciprocal:200}
\begin{aligned}
1 &= \Bx_1 \cdot \Bx^1 = a \Bx_1^2 + b \Bx_1 \cdot \Bx_2 \\
0 &= \Bx_2 \cdot \Bx^1 = a \Bx_2 \cdot \Bx_1 + b \Bx_2^2 \\
0 &= \Bx_1 \cdot \Bx^2 = c \Bx_1^2 + d \Bx_1 \cdot \Bx_2 \\
1 &= \Bx_2 \cdot \Bx^2 = c \Bx_2 \cdot \Bx_1 + d \Bx_2^2
\end{aligned}
\end{dmath}

This can be written out as a pair of matrix equations

\begin{dmath}\label{eqn:reciprocal:220}
\begin{aligned}
\begin{bmatrix}
1 \\ 
0
\end{bmatrix}
&=
\begin{bmatrix}
\Bx_1^2 & \Bx_1 \cdot \Bx_2 \\
\Bx_2 \cdot \Bx_1 & \Bx_2^2 \\
\end{bmatrix}
\begin{bmatrix}
a \\
b
\end{bmatrix} \\
\begin{bmatrix}
0 \\ 
1
\end{bmatrix}
&=
\begin{bmatrix}
\Bx_1^2 & \Bx_1 \cdot \Bx_2 \\
\Bx_2 \cdot \Bx_1 & \Bx_2^2 \\
\end{bmatrix}
\begin{bmatrix}
c \\
d
\end{bmatrix}.
\end{aligned}
\end{dmath}

The matrix inverse is
\begin{dmath}\label{eqn:reciprocal:240}
{
\begin{bmatrix}
\Bx_1^2 & \Bx_1 \cdot \Bx_2 \\
\Bx_2 \cdot \Bx_1 & \Bx_2^2 \\
\end{bmatrix}
}^-1
=
\inv{ \Bx_1^2 \Bx_2^2 - \lr{\Bx_1 \cdot \Bx_2}^2 }
\begin{bmatrix}
\Bx_2^2 & -\Bx_1 \cdot \Bx_2 \\
-\Bx_2 \cdot \Bx_1 & \Bx_1^2 \\
\end{bmatrix}
\end{dmath}

Multiplying by the \( (1,0), and (0,1) \) vectors picks out the respective columns, and gives \cref{eqn:reciprocal:120}.
} % answer

%}
%\EndArticle
\EndNoBibArticle
