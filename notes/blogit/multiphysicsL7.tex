%
% Copyright © 2014 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
% for template copy, run:
%
% ~/bin/ct multiphysicsL1  multiphysicsLectureN tl1
%
\input{../blogpost.tex}
\renewcommand{\basename}{multiphysicsL7}
\renewcommand{\dirname}{notes/ece1254/}
\newcommand{\keywords}{Condensed matter physics, ECE1254H}
\input{../peeter_prologue_print2.tex}

%\usepackage{kbordermatrix}
\usepackage{algorithmic}

\beginArtNoToc
\generatetitle{ECE1254H Modeling of Multiphysics Systems.  Lecture 7: Sparse factorization and iterative methods.  Taught by Prof.\ Piero Triverio}
%\chapter{Sparse factorization and iterative methods}
\label{chap:multiphysicsL7}

\section{Disclaimer}

Peeter's lecture notes from class.  These may be incoherent and rough.

\section{Fill ins}

The problem of fill ins in LU computations arise in locations where rows and columns cross over zero positions.

Rows and columns can be permuted to deal with these.  Here is an ad-hoc permutation of rows and columns that will result in less fill ins.

\begin{equation}\label{eqn:multiphysicsL7:180}
\begin{aligned}
&
\begin{bmatrix}
a & b & c & 0 \\
d & e & 0 & 0 \\
0 & f & g & 0 \\
0 & h & 0 & i \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
b_4
\end{bmatrix} \\
\implies &
\begin{bmatrix}
a & c & 0 & b \\
d & 0 & 0 & e \\
0 & g & 0 & f \\
0 & 0 & i & h \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_4 \\
x_3 \\
x_2 \\
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
b_4 \\
\end{bmatrix} \\
\implies &
\begin{bmatrix}
0 & a & c & b \\
0 & d & 0 & e \\
0 & 0 & g & f \\
i & 0 & 0 & h \\
\end{bmatrix}
\begin{bmatrix}
x_3 \\
x_4 \\
x_1 \\
x_2 \\
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
b_4 \\
\end{bmatrix} \\
\implies &
\begin{bmatrix}
i & 0 & 0 & h \\
0 & a & c & b \\
0 & d & 0 & e \\
0 & 0 & g & f \\
\end{bmatrix}
\begin{bmatrix}
x_3 \\
x_4 \\
x_1 \\
x_2 \\
\end{bmatrix}
=
\begin{bmatrix}
b_4 \\
b_1 \\
b_2 \\
b_3 \\
\end{bmatrix} \\
\implies &
\begin{bmatrix}
i & 0 & 0 & h \\
0 & c & a & b \\
0 & 0 & d & e \\
0 & g & 0 & f \\
\end{bmatrix}
\begin{bmatrix}
x_3 \\
x_1 \\
x_4 \\
x_2 \\
\end{bmatrix}
=
\begin{bmatrix}
b_4 \\
b_1 \\
b_2 \\
b_3 \\
\end{bmatrix} \\
\end{aligned}
\end{equation}

\sectionAndIndex{Markowitz product}

To facilitate such permutations the Markowitz product that estimates the amount of fill in required.

\makedefinition{Markowitz product}{dfn:multiphysicsL7:20}{
\begin{equation*}
\begin{aligned}
		  \text{Markowitz product} =
		  &\lr{\text{Non zeros in unfactored part of Row -1}} \times \\
&\lr{\text{Non zeros in unfactored part of Col -1}}
\end{aligned}
\end{equation*}
}

In \citep{markowitz1957elimination} it is stated ``A still simpler alternative, which seems adequate generally, is to choose the pivot which minimizes the number of coefficients modified at each step (excluding those which are eliminated at the particular step).  This is equivalent to choosing the non-zero element with minimum \( (\rho_i - 1 )(\sigma_j -1) \).''  

Note that this product is applied only to \( i j \) positions that are non-zero, something not explicitly mentioned in the slides, nor in other locations like \citep{pivotingSparsity}.

\makeexample{Markowitz product}{example:multiphysicsL7:200}{

%In the slides the Markowitz product is computed for each of the diagonal elements, and the example looks like the product is really the product of the number of non-zero values in that row and column with the diagonal values excepted.  That seems to differ from the definition, and also differs from the description in 

For this matrix 
\begin{equation}\label{eqn:multiphysicsL7:220}
\begin{bmatrix}
a & b & c & 0 \\
d & e & 0 & 0 \\
0 & f & g & 0 \\
0 & h & 0 & i \\
\end{bmatrix},
\end{equation}

the Markowitz products are

\begin{equation}\label{eqn:multiphysicsL7:280}
\begin{bmatrix}
1 & 6 & 2 &   \\
1 & 3 &   &   \\
  & 3 & 1 &   \\
  & 3 &   & 0 \\
\end{bmatrix}.
\end{equation}
}

\sectionAndIndex{Markowitz reordering}

The Markowitz Reordering procedure (copied directly from the slides) is

\begin{itemize}
\item 
For i = 1 to n
\item 
Find diagonal \( j >= i \) with min Markowitz product
\item 
Swap rows \( j \leftrightarrow i \) and columns \( j \leftrightarrow i \)
\item 
Factor the new row \( i \) and update Markowitz products
\end{itemize}

\makeexample{Markowitz reordering}{example:multiphysicsL7:280}{

Looking at the Markowitz products \cref{eqn:multiphysicsL7:280} a swap of rows and columns \( 1, 4 \) gives the modified matrix

\begin{equation}\label{eqn:multiphysicsL7:300}
\begin{bmatrix}
i & 0 & h & 0 \\
0 & d & e & 0 \\
0 & 0 & f & g \\
0 & a & b & c \\
\end{bmatrix}
\end{equation}

In this case, this reordering has completely avoided any requirement to do any actual Gaussian operations for this first stage reduction.

Presuming that the Markowitz products for the remaining 3x3 submatrix are only computed from that submatrix, the new products are
\begin{equation}\label{eqn:multiphysicsL7:320}
\begin{bmatrix}
&   &   &   \\
& 1 & 2 &   \\
&   & 2 & 1 \\
& 2 & 4 & 2 \\
\end{bmatrix}.
\end{equation}

We have a minimal product in the pivot position, which happens to already lie on the diagonal.  Note that it is not necessarily the best for numerical stability.  It appears the off diagonal Markowitz products are not really of interest since the reordering algorithm swaps both rows and columns.
}

\section{Graph representation}

It is possible to interpret the Markowitz products on the diagonal as connectivity of a graph that represents the interconnections of the nodes.  Consider the circuit of \cref{fig:lecture7:lecture7Fig2} as an example

\imageFigure{../../figures/ece1254/lecture7Fig2}{Simple circuit}{fig:lecture7:lecture7Fig2}{0.3}

The system equations for this circuit is of the form
\begin{equation}\label{eqn:multiphysicsL7:340}
\begin{bmatrix}
x & x & x & 0 & 1 \\
x & x & x & 0 & 0 \\
x & x & x & x & 0 \\
0 & 0 & x & x & -1 \\
-1 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\begin{bmatrix}
V_1 \\
V_2 \\
V_3 \\
V_4 \\
i \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
x \\
\end{bmatrix}.
\end{equation}

The Markowitz products along the diagonal are
\begin{equation}\label{eqn:multiphysicsL7:360}
\begin{aligned}
M_{11} &= 9 \\
M_{22} &= 4 \\
M_{33} &= 9 \\
M_{44} &= 4 \\
M_{55} &= 4 \\
\end{aligned}
\end{equation}

Compare these to the number of interconnections of the graph \cref{fig:lecture7:lecture7Fig3} of the nodes in this circuit.  We see that these are the squares of the number of the node interconnects in each case.

\imageFigure{../../figures/ece1254/lecture7Fig3}{Graph representation}{fig:lecture7:lecture7Fig3}{0.3}

Here a 5th node was introduced for the current \( i \) between nodes \( 4 \) and \( 1 \).  Observe that the Markowitz product of this node was counted as the number of non-zero values excluding the \( 5,5 \) matrix position.  However, that doesn't matter too much since a Markowitz swap of row/column 1 with row/column 5 would put a zero in the \( 1,1 \) position of the matrix, which is not desirable.  We have to restrict the permutations of zero diagonal positions to pivots for numerical stability, or use a more advanced zero fill avoidance algorithm.

The minimum diagonal Markowitz products are in positions 2 or 4, with respective Markowitz reorderings of the form

\begin{equation}\label{eqn:multiphysicsL7:380}
\begin{bmatrix}
x & x & x & 0 & 0 \\
x & x & x & 0 & 1 \\
x & x & x & x & 0 \\
0 & 0 & x & x & -1 \\
0 & -1 & 0 & 1 & 0 \\
\end{bmatrix}
\begin{bmatrix}
V_2 \\
V_1 \\
V_3 \\
V_4 \\
i \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
x \\
\end{bmatrix},
\end{equation}

and
\begin{equation}\label{eqn:multiphysicsL7:400}
\begin{bmatrix}
x &  0 & 0 & x & -1 \\
0 &  x & x & x & 1 \\
0 &  x & x & x & 0 \\
x &  x & x & x & 0 \\
1 & -1 & 0 & 0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
V_4 \\
V_1 \\
V_2 \\
V_3 \\
i \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
x \\
\end{bmatrix}.
\end{equation}

The original system had 7 zeros that could potentially be filled in the remaining \( 4 \times 4 \) submatrix.  After a first round of Gaussian elimination, our system matrices have the respective forms

\begin{subequations}
\begin{equation}\label{eqn:multiphysicsL7:420}
\begin{bmatrix}
x & x & x & 0 & 0 \\
0 & x & x & 0 & 1 \\
0 & x & x & x & 0 \\
0 & 0 & x & x & -1 \\
0 & -1 & 0 & 1 & 0 \\
\end{bmatrix}
\end{equation}
\begin{equation}\label{eqn:multiphysicsL7:440}
\begin{bmatrix}
x &  0 & 0 & x & -1 \\
0 &  x & x & x & 1 \\
0 &  x & x & x & 0 \\
0 &  x & x & x & 0 \\
0 & -1 & 0 & x & x \\
\end{bmatrix}
\end{equation}
\end{subequations}

The remaining \( 4 \times 4 \) submatrices have interconnect graphs sketched in \cref{fig:lecture7:lecture7Fig4}.

\imageFigure{../../figures/ece1254/lecture7Fig4}{Graphs after one round of Gaussian elimination}{fig:lecture7:lecture7Fig4}{0.3}

From a graph point of view, we want to delete the most connected nodes.  This can be driven by the Markowitz products along the diagonal or directly with graph methods.

\section{Summary of factorization costs}

\paragraph{LU (dense)}

\begin{itemize}
   \item cost: \( O(n^3) \)
   \item cost depends only on size
\end{itemize}

\paragraph{LU (sparse)}

\begin{itemize}
   \item cost: Diagonal and tridiagonal are \( O(n) \), but we can have up to \( O(n^3) \) depending on sparsity and the method of dealing with the sparsity.
   \item cost depends on size and sparsity
\end{itemize}

Computation can be affordable up to a few million elements.

\paragraph{Iterative methods}

Can be cheap if done right.  Convergence requires careful preconditioning.

\section{Iterative methods}

Suppose that we have an initial guess \( \Bx_0 \).   Iterative methods are generally of the form

\begin{algorithmic}
\REPEAT
\STATE \(\Br = \Bb - M \Bx_i\)
\UNTIL{\(\Norm{\Br} < \epsilon \).}
\end{algorithmic}

The difference \( \Br \) is called the residual.  For as long as it is bigger than desired, continue improving the estimate \( \Bx_i \).

The matrix vector product \( M \Bx_i \), if dense, is of \( O(n^2) \).  Suppose, for example, that we can perform the iteration in ten iterations.  If the matrix is dense, we can have \( 10 \, O(n^2) \) performance.  If sparse, this can be worse than just direct computation.

\section{Gradient method}

This is a method for iterative solution of the equation \( M \Bx = \Bb \).

This requires symmetric positive definite matrix \( M = M^\T \), with \( M > 0 \).

We introduce an \textAndIndex{energy function}

\begin{equation}\label{eqn:multiphysicsL7:60}
\Psi(\By) \equiv \inv{2} \By^\T M \By - \By^\T \Bb
\end{equation}

For a two variable system this is illustrated in \cref{fig:lecture7:lecture7Fig1}.

\imageFigure{../../figures/ece1254/lecture7Fig1}{Positive definite energy function}{fig:lecture7:lecture7Fig1}{0.3}

\maketheorem{Energy function minimum}{thm:multiphysicsL7:460}{
The energy function \cref{eqn:multiphysicsL7:60} has a minimum at

\begin{equation}\label{eqn:multiphysicsL7:80}
\By = M^{-1} \Bb = \Bx.
\end{equation}

To prove this, consider the coordinate representation

\begin{equation}\label{eqn:multiphysicsL7:480}
\Psi = \inv{2} y_a M_{ab} y_b - y_b b_b,
\end{equation}

for which the derivatives are
\begin{dmath}\label{eqn:multiphysicsL7:500}
\PD{y_i}{\Psi} = 
\inv{2} M_{ib} y_b 
+
\inv{2} y_a M_{ai} 
- b_i
=
\lr{ M \By - \Bb }_i.
\end{dmath}

The last operation above was possible because \( M = M^\T \).  Setting all of these equal to zero, and rewriting this as a matrix relation we have

\begin{equation}\label{eqn:multiphysicsL7:520}
M \By = \Bb,
\end{equation}

as asserted.
}

This is called the gradient method because the gradient moves us along the path of steepest descent towards the minimum if it exists.

The method is

\begin{equation}\label{eqn:multiphysicsL7:100}
\Bx^{(k+1)} = \Bx^{(k)} + 
\mathLabelBox
{ \alpha_k }{step size}
\mathLabelBox
[
   labelstyle={below of=m\themathLableNode, below of=m\themathLableNode}
]
{
\Bd^{(k)}
}{direction},
\end{equation}

where the direction is

\begin{dmath}\label{eqn:multiphysicsL7:120}
\Bd^{(k)} = - \spacegrad \Phi = \Bb - M \Bx^k = r^{(k)}.
\end{dmath}

\paragraph{Optimal step size}

Note that for the minimization of \( \Phi \lr{ \Bx^{(k+1)} } \), we note

\begin{dmath}\label{eqn:multiphysicsL7:140}
\Phi \lr{ \Bx^{(k+1)} }
= \Phi\lr{ \Bx^{(k)} + \alpha_k \Bd^{(k)} }
= 
\inv{2} 
\lr{ \Bx^{(k)} + \alpha_k \Bd^{(k)} }^\T
M
\lr{ \Bx^{(k)} + \alpha_k \Bd^{(k)} }
-
\lr{ \Bx^{(k)} + \alpha_k \Bd^{(k)} }^\T \Bb
\end{dmath}

If we take the derivative of both sides with respect to \( \alpha_k \) to find the minimum, we have

\begin{equation}\label{eqn:multiphysicsL7:540}
0 = 
\inv{2} 
\lr{ \Bd^{(k)} }^\T
M
\Bx^{(k)}
+
\inv{2} 
\lr{ \Bx^{(k)} }^\T
M
\Bd^{(k)}
+
\alpha_k \lr{ \Bd^{(k)} }^\T
M
\Bd^{(k)} 
-
\lr{ \Bd^{(k)} }^\T \Bb.
\end{equation}

Because \( M \) is symmetric, this is

\begin{equation}\label{eqn:multiphysicsL7:560}
\alpha_k \lr{ \Bd^{(k)} }^\T
M
\Bd^{(k)} 
=
\lr{ \Bd^{(k)} }^\T \lr{ \Bb - M \Bx^{(k)}}
=
\lr{ \Bd^{(k)} }^\T r^{(k)},
\end{equation}

or

\begin{equation}\label{eqn:multiphysicsL7:160}
\alpha_k 
= \frac{
\lr{\Br^{(k)}}^\T 
\Br^{(k)}
}{
\lr{\Br^{(k)}}^\T 
M
\Br^{(k)}
}
\end{equation}

We will see that this method is not optimal when we pick one direction and keep going down that path.

\section{Definitions and theorems}

\makedefinition{Positive (negative) definite}{dfn:multiphysicsL7:40}{
   A matrix \( M \) is positive (negative) definite, denoted \( M > 0 (<0) \) if \( \By^\T M \By > 0 (<0), \quad \forall \By \).

   If a matrix is neither positive, nor negative definite, it is called indefinite.
}

\maketheorem{Positive (negative) definite}{thm:multiphysicsL7:60}{
   A symmetric matrix \( M > 0 (<0)\) iff \( \lambda_i > 0 (<0)\) for all eigenvalues \( \lambda_i \), or is indefinite iff its eigenvalues \( \lambda_i \) are of mixed sign.
}

\EndArticle
%\EndNoBibArticle
