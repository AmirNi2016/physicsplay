%
% Copyright Â© 2016 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
\makeproblem{}{problem:multiplication:bivectorDot}{
Show that

\boxedEquation{eqn:bivectorDot:20}{
(\Ba \wedge \Bb) \cdot (\Bc \wedge \Bd)
=
((\Ba \wedge \Bb) \cdot (\Bc) \cdot \Bd,
}

and hence
\boxedEquation{eqn:bivectorDot:40}{
(\Ba \wedge \Bb) \cdot (\Bc \wedge \Bd)
=
(\Bb \cdot \Bc) (\Ba \cdot \Bd)
-(\Ba \cdot \Bc)( \Bb \cdot \Bd).
}
} % problem

\makeanswer{problem:multiplication:bivectorDot}{

\begin{dmath}\label{eqn:bivectorDot:60}
(\Ba \wedge \Bb) \cdot (\Bc \wedge \Bd)
=
\gpgradezero{
(\Ba \wedge \Bb) (\Bc \wedge \Bd)
}
=
\gpgradezero{
(\Ba \wedge \Bb) (\Bc \Bd - \cancel{\Bc \cdot \Bd})
}
=
\gpgradezero{
\lr{
(\Ba \wedge \Bb) \cdot \Bc
+ \cancel{(\Ba \wedge \Bb) \wedge \Bc }
}
\Bd
}
=
\gpgradezero{
((\Ba \wedge \Bb) \cdot \Bc ) \cdot \Bd
+
\cancel{((\Ba \wedge \Bb) \cdot \Bc ) \wedge \Bd}
}
=
((\Ba \wedge \Bb) \cdot \Bc ) \cdot \Bd.
\end{dmath}

Above, any product that could not possibly contribute a scalar grade has been cancelled.  The remains are now straightforward to expand

\begin{dmath}\label{eqn:bivectorDot:80}
((\Ba \wedge \Bb) \cdot \Bc ) \cdot \Bd
=
(
\Ba (\Bb \cdot \Bc)
-
\Bb (\Ba \cdot \Bc)
)
\cdot \Bd
=
(\Ba \cdot \Bd) (\Bb \cdot \Bc)
-
(\Bb \cdot \Bd) (\Ba \cdot \Bc).
\end{dmath}

A strictly GA derivation is not neccesarily the easiest or most compact way to derive a given result.  For example, this result can be proved by making a duality transformation, and then doing a standard tensor expansion

\begin{dmath}\label{eqn:bivectorDot:100}
(\Ba \wedge \Bb) \cdot (\Bc \wedge \Bd)
=
\gpgradezero{
-I (\Ba \cross \Bb) (-I) (\Bc \cross \Bd)
}
=
- (\Ba \cross \Bb) \cdot (\Bc \cross \Bd)
=
-(\epsilon_{ijk} \Be_i a_j b_k) \cdot (\epsilon_{r s t} \Be_r c_s d_t)
=
-\epsilon_{ijk} a_j b_k \epsilon_{i s t} c_s d_t
=
-\delta_{jk}^{[st]}
a_j b_k c_s d_t
=
-a_s b_t c_s d_t
+
a_t b_s c_s d_t
=
-(\Ba \cdot \Bc)(\Bb \cdot \Bd)
+
(\Ba \cdot \Bd)(\Bb \cdot \Bc).
\end{dmath}

Of course there is a learning curve to use of the tensor toolbox too, so this may not seem like an obvious alternate approach.
} % answer
